{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Paper Prowler","text":"<p>The Paper Prowler is A versatile tool that aggregates and organizes web content like RSS feeds, arXiv papers to streamline your research and information management.</p>"},{"location":"#pipeline","title":"Pipeline","text":"<p>The Paper Prowler pipeline is a series of steps that are executed  to fetch, process, and store the data. The pipeline relies on configuration files to specify the steps and their parameters and writes and updates a Haystack <code>InMemoryDocumentStore</code> with the processed data.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Paper Prowler</li> <li>Papers<ul> <li>Overview</li> <li>synthetic data generation llms<ul> <li>Synthetic Test Collections for Retrieval Evaluation</li> <li>Synthetic Test Collections for Retrieval Evaluation</li> <li>ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs</li> <li>Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations</li> <li>Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data</li> <li>Differentially Private Synthetic Data via Foundation Model APIs 2: Text</li> <li>Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal</li> <li>Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications</li> <li>S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models</li> <li>MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents</li> <li>Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks</li> <li>Synthetic Test Collections for Retrieval Evaluation</li> <li>ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs</li> <li>Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations</li> <li>Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data</li> <li>Differentially Private Synthetic Data via Foundation Model APIs 2: Text</li> <li>Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal</li> <li>Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications</li> <li>S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models</li> <li>MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents</li> <li>Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks</li> </ul> </li> <li>LLMs as judges<ul> <li>Humans or LLMs as the Judge? A Study on Judgement Biases</li> <li>Can LLM be a Personalized Judge?</li> <li>On the Limitations of Fine-tuned Judge Models for LLM Evaluation</li> <li>JudgeLM: Fine-tuned Large Language Models are Scalable Judges</li> <li>Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment</li> <li>Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models</li> <li>Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models</li> <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</li> <li>Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs</li> <li>Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!</li> </ul> </li> <li>data quality for LLMs<ul> <li>MEGAnno+: A Human-LLM Collaborative Annotation System</li> <li>A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation</li> <li>Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning</li> <li>Label-free Node Classification on Graphs with Large Language Models (LLMS)</li> <li>OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data</li> <li>METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</li> <li>Towards Training A Chinese Large Language Model for Anesthesiology</li> <li>FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models</li> <li>Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements</li> <li>Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals</li> </ul> </li> <li>human feedback LLMs<ul> <li>LLM A: Human in the Loop Large Language Models Enabled A Search for Robotics</li> <li>Verbosity Bias in Preference Labeling by Large Language Models</li> <li>Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF</li> <li>Aligning Large Language Models through Synthetic Feedback</li> <li>The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values</li> <li>LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback</li> <li>A Survey on Human Preference Learning for Large Language Models</li> <li>LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models</li> <li>Prototypical Reward Network for Data-Efficient RLHF</li> <li>IterAlign: Iterative Constitutional Alignment of Large Language Models</li> </ul> </li> <li>LLM benchmarking and evaluation<ul> <li>MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition</li> <li>ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models</li> <li>OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety</li> <li>tinyBenchmarks: evaluating LLMs with fewer examples</li> <li>How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation</li> <li>Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate</li> <li>A User-Centric Benchmark for Evaluating Large Language Models</li> <li>PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain</li> <li>State of What Art? A Call for Multi-Prompt LLM Evaluation</li> <li>LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond</li> </ul> </li> </ul> </li> </ul>"},{"location":"sections/papers/","title":"Overview","text":"synthetic data generation llmsLLMs as judgesdata quality for LLMshuman feedback LLMsLLM benchmarking and evaluation title abstract Synthetic Test Collections for Retrieval Evaluation Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. Synthetic Test Collections for Retrieval Evaluation Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost. Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation. Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations. For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention. By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs. However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks. In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy. The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy. Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs. The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data. Differentially Private Synthetic Data via Foundation Model APIs 2: Text Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe. Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains. Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable. S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning. However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration. In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios. The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs. MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models. Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk Synthetic Test Collections for Retrieval Evaluation Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost. Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation. Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations. For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention. By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs. However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks. In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy. The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy. Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs. The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data. Differentially Private Synthetic Data via Foundation Model APIs 2: Text Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe. Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains. Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable. S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning. However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration. In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios. The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs. MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models. Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk title abstract Humans or LLMs as the Judge? A Study on Judgement Biases Adopting human and large language models (LLM) as judges (\\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating Fallacy Oversight Bias, Authority Bias and Beauty Bias on LLM and human judges. We curate a dataset referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of developing robust evaluation systems. Can LLM be a Personalized Judge? Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization. On the Limitations of Fine-tuned Judge Models for LLM Evaluation Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT-4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this study, we conduct an empirical study of judge models. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness, aspect-specific evaluation, and scalability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations. Finally, we propose an effective indicator to measure the reliability of fine-tuned judges, with the aim of maximizing their utility in LLM evaluation. JudgeLM: Fine-tuned Large Language Models are Scalable Judges Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat. Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5. This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios. Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive. Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as \"re-judge inconsistency\" in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs' capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge. Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of LLM outputs. The code is publicly available at https://github.com/xz-liu/GraphEval. Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better! Leveraging Large Language Models (LLMs) as judges for evaluating the performance of LLMs has recently garnered attention. Nonetheless, this type of approach concurrently introduces potential biases from LLMs, raising concerns about the reliability of the evaluation results. To mitigate this issue, we propose and study two versions of many-shot in-context prompts, Reinforced and Unsupervised ICL, for helping GPT-4o-as-a-Judge in single answer grading. Based on the designed prompts, we investigate the impact of scaling the number of in-context examples on the agreement and quality of the evaluation. Furthermore, we first reveal the symbol bias in GPT-4o-as-a-Judge for pairwise comparison and then propose a simple yet effective approach to mitigate it. Experimental results show that advanced long-context LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot regime. Meanwhile, the experimental results further verify the effectiveness of the symbol bias mitigation approach. title abstract MEGAnno+: A Human-LLM Collaborative Annotation System Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans. A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. In addition, we propose a Crowd-LLM hybrid label aggregation method and verify the performance. We find that adding LLM labels from good LLMs to existing crowdsourcing datasets can enhance the quality of the aggregated labels of the datasets, which is also higher than the quality of LLM labels themselves. Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs. Label-free Node Classification on Graphs with Large Language Models (LLMS) In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with a cost less than 1 dollar. OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4. METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of their approaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques. This approach facilitates the systematic testing of LLM qualities by defining Metamorphic Relations (MRs), which serve as modularized evaluation metrics. The METAL framework can automatically generate hundreds of MRs from templates that cover various QAs and tasks. In addition, we introduced novel metrics that integrate the ASR method into the semantic qualities of text to assess the effectiveness of MRs accurately. Through the experiments conducted with three prominent LLMs, we have confirmed that the METAL framework effectively evaluates essential QAs on primary LLM tasks and reveals the quality risks in LLMs. Moreover, the newly proposed metrics can guide the optimal MRs for testing each task and suggest the most effective method for generating MRs. Towards Training A Chinese Large Language Model for Anesthesiology Medical large language models (LLMs) have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of LLMs in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one LLM to assess the quality of the generated data from another LLM and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by fine-tuning LLMs using the general medicine data and subsequently improving the fine-tuned LLMs using data specifically from Anesthesiology. The general medical data supplement the medical expertise in Anesthesiology and enhance the effectiveness of Hypnos' generation. 3) We introduce a standardized benchmark for evaluating medical LLM in Anesthesiology. Our benchmark includes both publicly available instances from the Internet and privately obtained cases from the Hospital. Hypnos outperforms other medical LLMs in anesthesiology in metrics, GPT-4, and human evaluation on the benchmark dataset. FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research of FedLLM and enable a broad range of industrial applications. Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning (DEBATUNE) pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATUNE, we curate the largest dataset of debate topics so far, which covers 710 controversial topics and corresponding arguments for each topic. Evaluations by the GPT-4 judge with a novel controversy controllability metric show that LLMs' capability of generating diverse perspectives is significantly improved by DEBATUNE. Moreover, such controllability can be generalized to unseen topics, generating high-quality statements supporting controversial arguments. Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals This paper explores the evolving relationship between clinician trust in LLMs, the transformation of data sources from predominantly human-generated to AI-generated content, and the subsequent impact on the precision of LLMs and clinician competence. One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in healthcare deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. A key takeaway from our investigation is the critical role of user expertise and the necessity for a discerning approach to trusting and validating LLM outputs. The paper highlights how expert users, particularly clinicians, can leverage LLMs to enhance productivity by offloading routine tasks while maintaining a critical oversight to identify and correct potential inaccuracies in AI-generated content. This balance of trust and skepticism is vital for ensuring that LLMs augment rather than undermine the quality of patient care. Moreover, we delve into the potential risks associated with LLMs' self-referential learning loops and the deskilling of healthcare professionals. The risk of LLMs operating within an echo chamber, where AI-generated content feeds into the learning algorithms, threatens the diversity and quality of the data pool, potentially entrenching biases and reducing the efficacy of LLMs. title abstract LLM A: Human in the Loop Large Language Models Enabled A Search for Robotics This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A, aims to leverage the commonsense of LLMs, and the utility-optimal A is proposed to facilitate few-shot near-optimal path planning. Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to LLMs on intermediate planning results. This makes the whole path planning process a `white box' and human feedback guides LLM A to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning. In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques. Comparative analysis against A and RL shows that LLM A is more efficient in terms of search space and achieves an on-a-par path with A and a better path than RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks. Verbosity Bias in Preference Labeling by Large Language Models In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias. Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF Reinforcement Learning from Human Feedback (RLHF) is popular in large language models (LLMs), whereas traditional Reinforcement Learning (RL) often falls short. Current autonomous driving methods typically utilize either human feedback in machine learning, including RL, or LLMs. Most feedback guides the car agent's learning process (e.g., controlling the car). RLHF is usually applied in the fine-tuning step, requiring direct human \"preferences,\" which are not commonly used in optimizing autonomous driving models. In this research, we innovatively combine RLHF and LLMs to enhance autonomous driving safety. Training a model with human guidance from scratch is inefficient. Our framework starts with a pre-trained autonomous car agent model and implements multiple human-controlled agents, such as cars and pedestrians, to simulate real-life road environments. The autonomous car model is not directly controlled by humans. We integrate both physical and physiological feedback to fine-tune the model, optimizing this process using LLMs. This multi-agent interactive environment ensures safe, realistic interactions before real-world application. Finally, we will validate our model using data gathered from real-life testbeds located in New Jersey and New York City. Aligning Large Language Models through Synthetic Feedback Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories.First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges. LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages. A Survey on Human Preference Learning for Large Language Models The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs. LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion of human-annotated data in the annotated datasets affects the fine-tuning performance. Prototypical Reward Network for Data-Efficient RLHF The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences. Extensive experiments on various datasets demonstrate that Proto-RM significantly improves the performance of reward models and LLMs in human feedback tasks, achieving comparable and usually better results than traditional methods, while requiring significantly less data. in data-limited scenarios. This research offers a promising direction for enhancing the efficiency of reward models and optimizing the fine-tuning of language models under restricted feedback conditions. IterAlign: Iterative Constitutional Alignment of Large Language Models With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\\%$ in harmlessness. title abstract MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' performance objectively and scientifically; 2) LLMs only get a small percentage of the right reasoning chain, e.g. GPT-4 only gets 36.3\\% right reasoning chain. We believe this new Multi-hop QA evaluation benchmark and novel evaluation methods will facilitate the development of trustworthy LLM evaluation on the MHQA task. ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models The unprecedented performance of large language models (LLMs) requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the ZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks. Especially, we also propose a new benchmark that focuses on knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration: We use 3 different yet complementary evaluation methods to comprehensively evaluate LLMs, which can ensure the authority and accuracy of the evaluation results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering benchmark that fully assesses LLMs in Chinese, while also providing equally robust evaluation abilities in English. (4) Avoiding potential data leakage: To avoid data leakage, we construct evaluation data specifically for 37 tasks. We evaluate 10 current mainstream LLMs and conduct an in-depth discussion and analysis of their results. The ZhuJiu benchmark and open-participation leaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we also provide a demo video at https://youtu.be/qypkJ89L1Ic. OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety The rapid development of Chinese large language models (LLMs) poses big challenges for efficient LLM evaluation. While current initiatives have introduced new benchmarks or evaluation platforms for assessing Chinese LLMs, many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues. To address this gap, we introduce OpenEval, an evaluation testbed that benchmarks Chinese LLMs across capability, alignment and safety. For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning. For alignment assessment, OpenEval contains 7 datasets that examines the bias, offensiveness and illegalness in the outputs yielded by Chinese LLMs. To evaluate safety, especially anticipated risks (e.g., power-seeking, self-awareness) of advanced LLMs, we include 6 datasets. In addition to these benchmarks, we have implemented a phased public evaluation and benchmark update strategy to ensure that OpenEval is in line with the development of Chinese LLMs or even able to provide cutting-edge benchmark datasets to guide the development of Chinese LLMs. In our first public evaluation, we have tested a range of Chinese LLMs, spanning from 7B to 72B parameters, including both open-source and proprietary models. Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety. tinyBenchmarks: evaluating LLMs with fewer examples The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results. How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation In recent years, AI has demonstrated remarkable capabilities in simulating human behaviors, particularly those implemented with large language models (LLMs). However, due to the lack of systematic evaluation of LLMs' simulated behaviors, the believability of LLMs among humans remains ambiguous, i.e., it is unclear which behaviors of LLMs are convincingly human-like and which need further improvements. In this work, we design SimulateBench to evaluate the believability of LLMs when simulating human behaviors. In specific, we evaluate the believability of LLMs based on two critical dimensions: 1) consistency: the extent to which LLMs can behave consistently with the given information of a human to simulate; and 2) robustness: the ability of LLMs' simulated behaviors to remain robust when faced with perturbations. SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs' simulated behaviors. Based on SimulateBench, we evaluate the performances of 10 widely used LLMs when simulating characters. The experimental results reveal that current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations in certain factors. Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \\url{https://github.com/GAIR-NLP/scaleeval}. A User-Centric Benchmark for Evaluating Large Language Models Large Language Models (LLMs) are essential tools to collaborate with users on different tasks. Evaluating their performance to serve users' needs in real-world scenarios is important. While many benchmarks have been created, they mainly focus on specific predefined model abilities. Few have covered the intended utilization of LLMs by real users. To address this oversight, we propose benchmarking LLMs from a user perspective in both dataset construction and evaluation designs. We first collect 1846 real-world use cases with 15 LLMs from a user study with 712 participants from 23 countries. These self-reported cases form the User Reported Scenarios(URS) dataset with a categorization of 7 user intents. Secondly, on this authentic multi-cultural dataset, we benchmark 10 LLM services on their efficacy in satisfying user needs. Thirdly, we show that our benchmark scores align well with user-reported experience in LLM interactions across diverse intents, both of which emphasize the overlook of subjective scenarios. In conclusion, our study proposes to benchmark LLMs from a user-centric perspective, aiming to facilitate evaluations that better reflect real user needs. The benchmark dataset and code are available at https://github.com/Alice1998/URS. PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain Biomedical language understanding benchmarks are the driving forces for artificial intelligence applications with large language model (LLM) back-ends. However, most current benchmarks: (a) are limited to English which makes it challenging to replicate many of the successes in English for other languages, or (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs apply these knowledge to perform on a wide range of bio-medical tasks, or (c) have become a publicly available corpus and are leaked to LLMs during pre-training. To facilitate the research in medical LLMs, we re-build the Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a large scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable test-bed and an online platform for evaluating Chinese LLMs' multi-task capabilities on a wide range bio-medical tasks including medical entity recognition, medical text classification, medical natural language inference, medical dialogue understanding and medical content/dialogue generation. To establish evaluation on these tasks, we have experimented and report the results with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning techniques. State of What Art? A Call for Multi-Prompt LLM Evaluation Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs. LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8\\% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur."},{"location":"sections/papers/-1806687785486849885/1ef23815fc6dbd4b967b14592e3987b6070ee34ec4401bca56f057b290db1918/","title":"FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models","text":"<p>Arxiv Link - 2023-10-16 04:17:13 </p>"},{"location":"sections/papers/-1806687785486849885/1ef23815fc6dbd4b967b14592e3987b6070ee34ec4401bca56f057b290db1918/#abstract","title":"Abstract","text":"<p>Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research of FedLLM and enable a broad range of industrial applications. </p>"},{"location":"sections/papers/-1806687785486849885/1ef23815fc6dbd4b967b14592e3987b6070ee34ec4401bca56f057b290db1918/#socials","title":"Socials","text":"LinkedIn X \ud83c\udf1f Exciting News in the World of Large Language Models (LLMs)! \ud83c\udf1fLarge Language Models (LLMs) like ChatGPT, LLaMA, GLM, and PaLM have been making waves with their exceptional performances. However, two key challenges have hindered their widespread adoption: resource-intensive training and the need for vast amounts of high-quality data.Introducing FATE-LLM - an industrial-grade federated learning framework designed to tackle these challenges head-on. FATE-LLM enables federated learning for large language models, promotes efficient training through parameter-efficient fine-tuning methods, safeguards intellectual property, and ensures data privacy during training and inference.Excited to learn more? Dive into the details and explore the code of FATE-LLM at: http://arxiv.org/abs/2310.10049v1Let's revolutionize the world of LLMs together! \ud83d\ude80#LLMs #FederatedLearning #AI #NLP #TechInnovation \ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! Introducing FATE-LLM, an industrial-grade federated learning framework designed to tackle challenges faced by LLMs in real-world applications. Learn more about FATE-LLM and its features at: http://arxiv.org/abs/2310.10049v1 #AI #NLP #LLMs #FederatedLearning #TechResearch"},{"location":"sections/papers/-1806687785486849885/1ef23815fc6dbd4b967b14592e3987b6070ee34ec4401bca56f057b290db1918/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/3dd33a07282cc7d5994269d2e955a480df34a788ab2de927a326f72d36b24534/","title":"A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation","text":"<p>Arxiv Link - 2024-01-18 07:23:51 </p>"},{"location":"sections/papers/-1806687785486849885/3dd33a07282cc7d5994269d2e955a480df34a788ab2de927a326f72d36b24534/#abstract","title":"Abstract","text":"<p>Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. In addition, we propose a Crowd-LLM hybrid label aggregation method and verify the performance. We find that adding LLM labels from good LLMs to existing crowdsourcing datasets can enhance the quality of the aggregated labels of the datasets, which is also higher than the quality of LLM labels themselves. </p>"},{"location":"sections/papers/-1806687785486849885/3dd33a07282cc7d5994269d2e955a480df34a788ab2de927a326f72d36b24534/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the world of AI and NLP! Can Large Language Models (LLMs) outperform crowdsourcing on data annotation tasks? \ud83e\udd14\ud83d\udccaRecent studies have delved into this question by comparing the performance of individual crowd workers and LLM workers on specific NLP tasks. However, there's a new perspective to explore! \ud83c\udf1fCheck out this research paper that investigates utilizing existing crowdsourcing datasets for evaluating annotation quality from a different angle. \ud83d\udcdd The findings reveal that incorporating LLM labels can enhance the quality of aggregated labels, surpassing the quality of LLM labels alone. \ud83d\udcc8Dive deeper into the study and explore the proposed Crowd-LLM hybrid label aggregation method for optimizing data annotation tasks. \ud83e\udde0Read more about this intriguing research here: http://arxiv.org/abs/2401.09760v1#AI #NLP #LLMs #DataAnnotation #Crowdsourcing #Research #TechInnovation \ud83d\ude80 Exciting findings in the world of AI and NLP! Can Large Language Models outperform crowdsourcing for data annotation tasks? This study delves into the comparison between individual crowd workers and LLM workers, along with proposing a Crowd-LLM hybrid label aggregation method. Discover the results here: http://arxiv.org/abs/2401.09760v1 #AI #NLP #LLMs #Crowdsourcing #TechResearch \ud83e\udd16\ud83d\udcca"},{"location":"sections/papers/-1806687785486849885/3dd33a07282cc7d5994269d2e955a480df34a788ab2de927a326f72d36b24534/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/7c032378e1ee4331b0969bed9248056ef0d6182e1d0341694da9622a4b9c9cfc/","title":"METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities","text":"<p>Arxiv Link - 2023-12-11 01:29:19 </p>"},{"location":"sections/papers/-1806687785486849885/7c032378e1ee4331b0969bed9248056ef0d6182e1d0341694da9622a4b9c9cfc/#abstract","title":"Abstract","text":"<p>Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of their approaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques. This approach facilitates the systematic testing of LLM qualities by defining Metamorphic Relations (MRs), which serve as modularized evaluation metrics. The METAL framework can automatically generate hundreds of MRs from templates that cover various QAs and tasks. In addition, we introduced novel metrics that integrate the ASR method into the semantic qualities of text to assess the effectiveness of MRs accurately. Through the experiments conducted with three prominent LLMs, we have confirmed that the METAL framework effectively evaluates essential QAs on primary LLM tasks and reveals the quality risks in LLMs. Moreover, the newly proposed metrics can guide the optimal MRs for testing each task and suggest the most effective method for generating MRs. </p>"},{"location":"sections/papers/-1806687785486849885/7c032378e1ee4331b0969bed9248056ef0d6182e1d0341694da9622a4b9c9cfc/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of Large-Language Models (LLMs)! Recent studies have introduced the METAL framework, a groundbreaking approach that utilizes Metamorphic Testing techniques to systematically evaluate the quality attributes of LLMs. By defining Metamorphic Relations (MRs) as modularized evaluation metrics, METAL can generate hundreds of MRs covering various qualities and tasks in LLMs.The experiments conducted with three major LLMs have demonstrated that the METAL framework effectively identifies essential Quality Attributes, helping to uncover potential risks in LLM outputs. Additionally, novel metrics have been introduced to enhance the assessment accuracy by integrating semantic qualities of text.Curious to learn more about this innovative approach and its implications for the future of LLM testing? Check out the full study here: http://arxiv.org/abs/2312.06056v1#LLM #MetamorphicTesting #QualityAttributes #TechInnovation #AI #NLP #ResearchStudy #TechNews \ud83d\ude80 Exciting new research alert! Discover how the METAL framework enhances the evaluation of Large-Language Models (LLMs) in diverse applications by utilizing Metamorphic Testing techniques. Find out more at: http://arxiv.org/abs/2312.06056v1 #AI #NLP #LLMs #METALframework #TechResearch"},{"location":"sections/papers/-1806687785486849885/7c032378e1ee4331b0969bed9248056ef0d6182e1d0341694da9622a4b9c9cfc/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/7d9bf2c964dd29dfe457920873ea2d27df399bb3877534d85d09b4c626e4077d/","title":"OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data","text":"<p>Arxiv Link - 2024-02-20 11:01:39 </p>"},{"location":"sections/papers/-1806687785486849885/7d9bf2c964dd29dfe457920873ea2d27df399bb3877534d85d09b4c626e4077d/#abstract","title":"Abstract","text":"<p>This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4. </p>"},{"location":"sections/papers/-1806687785486849885/7d9bf2c964dd29dfe457920873ea2d27df399bb3877534d85d09b4c626e4077d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in AI Research \ud83d\ude80Thrilled to share a groundbreaking paper on hallucination detection of Large Language Models (LLMs) that secured the second prize in the model-agnostic track of the SemEval-2024 Task 6! This unified system not only excelled in the model-aware track but also showcased impressive results in detecting hallucinations with LLMs across different text-generation tasks, all without the need for labeled training data.The team leveraged prompt engineering and few-shot learning to evaluate various LLMs on validation data, ultimately identifying those with superior performance to generate high-quality weakly supervised training data. By fine-tuning different LLMs using this data, the study revealed that even smaller LLMs can achieve remarkable levels of performance in hallucination detection, rivaling larger LLMs and prompt-based approaches employing GPT-4.Curious to dive deeper into this cutting-edge research? Check out the full paper here: http://arxiv.org/abs/2402.12913v1#AI #LLMs #NLP #SemEval2024 #TechResearch #InnovationInAI \ud83d\ude80 Exciting news in the field of LLMs! This paper presents a unified system for hallucination detection using LLMs, achieving remarkable results in the SemEval-2024 Task 6. Learn more about their innovative approach and findings at: http://arxiv.org/abs/2402.12913v1 #AI #NLP #LLMs #SemEval2024 \ud83e\udde0\ud83d\udd0d\ud83d\udcda"},{"location":"sections/papers/-1806687785486849885/7d9bf2c964dd29dfe457920873ea2d27df399bb3877534d85d09b4c626e4077d/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/850614379780f9fbe0e259b9556ab0e774c40107ea95bb2bf1084074fc832c84/","title":"Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning","text":"<p>Arxiv Link - 2024-06-07 20:23:21 </p>"},{"location":"sections/papers/-1806687785486849885/850614379780f9fbe0e259b9556ab0e774c40107ea95bb2bf1084074fc832c84/#abstract","title":"Abstract","text":"<p>Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs. </p>"},{"location":"sections/papers/-1806687785486849885/850614379780f9fbe0e259b9556ab0e774c40107ea95bb2bf1084074fc832c84/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in large language models! \ud83c\udf1fImproving instruction tuning for LLMs is crucial for enhancing their performance, but it heavily relies on the quality of training data. Check out this groundbreaking paper introducing Selective Reflection-Tuning, a novel paradigm that leverages teacher-student collaboration to automatically refine instruction-tuning data, resulting in high-quality and student-compatible instruction-response pairs. This innovative approach boosts LLMs' performance and efficiency without the need for additional data collection.Read more about this cutting-edge research and its impact on Alpaca and WizardLM data at: http://arxiv.org/abs/2402.10110v2#AI #NLP #LLMs #DataQuality #Research #Innovation \ud83d\ude80 Exciting research alert! Learn about Selective Reflection-Tuning, a novel paradigm enhancing instruction tuning for large language models (LLMs). This method improves data quality and student model compatibility, boosting LLM performance. Check out the paper here: http://arxiv.org/abs/2402.10110v2 #AI #NLP #LLMs #Research #TechInnovation"},{"location":"sections/papers/-1806687785486849885/850614379780f9fbe0e259b9556ab0e774c40107ea95bb2bf1084074fc832c84/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/916aef1d06cb486b5fbb74ea705493b75ef2447c67aed99b5a56a245ee6d3469/","title":"Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals","text":"<p>Arxiv Link - 2024-04-01 05:03:45 </p>"},{"location":"sections/papers/-1806687785486849885/916aef1d06cb486b5fbb74ea705493b75ef2447c67aed99b5a56a245ee6d3469/#abstract","title":"Abstract","text":"<p>This paper explores the evolving relationship between clinician trust in LLMs, the transformation of data sources from predominantly human-generated to AI-generated content, and the subsequent impact on the precision of LLMs and clinician competence. One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in healthcare deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. A key takeaway from our investigation is the critical role of user expertise and the necessity for a discerning approach to trusting and validating LLM outputs. The paper highlights how expert users, particularly clinicians, can leverage LLMs to enhance productivity by offloading routine tasks while maintaining a critical oversight to identify and correct potential inaccuracies in AI-generated content. This balance of trust and skepticism is vital for ensuring that LLMs augment rather than undermine the quality of patient care. Moreover, we delve into the potential risks associated with LLMs' self-referential learning loops and the deskilling of healthcare professionals. The risk of LLMs operating within an echo chamber, where AI-generated content feeds into the learning algorithms, threatens the diversity and quality of the data pool, potentially entrenching biases and reducing the efficacy of LLMs. </p>"},{"location":"sections/papers/-1806687785486849885/916aef1d06cb486b5fbb74ea705493b75ef2447c67aed99b5a56a245ee6d3469/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Just published! Dive into the complex dynamics between clinician trust in LLMs, evolving data sources, and the impact on precision and competence in healthcare. Explore the potential risks of feedback loops and deskilling, emphasizing the crucial role of proactive dialogue and user expertise. Stay informed with the full paper here: http://arxiv.org/abs/2403.14691v2 #AI #LLMs #Healthcare #TechInnovation \ud83d\udcc8\ud83d\udd0d \ud83d\udd0d New research alert! Explore the impact of clinician trust in LLMs on data sources and clinician competence in healthcare. Uncover the risks of feedback loops and deskilling in AI integration. Proactive dialogue is key. Access the full study here: http://arxiv.org/abs/2403.14691v2 #AI #LLMs #healthtech #research"},{"location":"sections/papers/-1806687785486849885/916aef1d06cb486b5fbb74ea705493b75ef2447c67aed99b5a56a245ee6d3469/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/9259a7c56f4b45036b2ff0b6146afce940635521e648ae2aacb0d2655173e762/","title":"Label-free Node Classification on Graphs with Large Language Models (LLMS)","text":"<p>Arxiv Link - 2024-02-24 06:44:45 </p>"},{"location":"sections/papers/-1806687785486849885/9259a7c56f4b45036b2ff0b6146afce940635521e648ae2aacb0d2655173e762/#abstract","title":"Abstract","text":"<p>In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with a cost less than 1 dollar. </p>"},{"location":"sections/papers/-1806687785486849885/9259a7c56f4b45036b2ff0b6146afce940635521e648ae2aacb0d2655173e762/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in AI research! A new approach, LLM-GNN, combines Large Language Models and Graph Neural Networks for label-free node classification on graphs. By leveraging LLMs to annotate a small portion of nodes and training GNNs on this data, LLM-GNN achieves impressive accuracy of 74.9% on a large dataset while keeping costs under $1. Learn more about this innovative pipeline here: http://arxiv.org/abs/2310.04668v3 #AI #LLM #GNN #NodeClassification #TechInnovation \ud83c\udf1f \ud83d\ude80 Exciting developments in AI research! Introducing LLM-GNN, a novel pipeline for label-free node classification on graphs. By combining Large Language Models and Graph Neural Networks, LLM-GNN achieves 74.9% accuracy on a large dataset with minimal cost. Learn more at: http://arxiv.org/abs/2310.04668v3 #AI #LLM #GNN #NodeClassification"},{"location":"sections/papers/-1806687785486849885/9259a7c56f4b45036b2ff0b6146afce940635521e648ae2aacb0d2655173e762/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/e086c20943b945085fe4dc6e8194d198338bb80cb3d0cc13d67125075c3af1a2/","title":"Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements","text":"<p>Arxiv Link - 2024-06-07 20:19:09 </p>"},{"location":"sections/papers/-1806687785486849885/e086c20943b945085fe4dc6e8194d198338bb80cb3d0cc13d67125075c3af1a2/#abstract","title":"Abstract","text":"<p>Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning (DEBATUNE) pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATUNE, we curate the largest dataset of debate topics so far, which covers 710 controversial topics and corresponding arguments for each topic. Evaluations by the GPT-4 judge with a novel controversy controllability metric show that LLMs' capability of generating diverse perspectives is significantly improved by DEBATUNE. Moreover, such controllability can be generalized to unseen topics, generating high-quality statements supporting controversial arguments. </p>"},{"location":"sections/papers/-1806687785486849885/e086c20943b945085fe4dc6e8194d198338bb80cb3d0cc13d67125075c3af1a2/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in AI and NLP Research! \ud83c\udf1fEnhancing the controllability of Language Models (LLMs) to generate statements supporting diverse and controversial perspectives is crucial for fostering inclusivity. A recent study introduces a novel approach, DEBATUNE, which leverages multi-round debates between LLMs with opposing stances to improve the quality and relevance of generated content.Discover how DEBATUNE refines LLMs' ability to produce statements aligned with user-defined arguments by checking out the research paper here: http://arxiv.org/abs/2402.10614v2#AI #NLP #Inclusivity #Research #DEBATUNE #LLMs #TechInnovationLet's continue pushing the boundaries of AI for a more inclusive future! \ud83c\udf10\ud83d\udca1 \ud83c\udf1f Exciting research alert! Improving LLMs' controllability in generating diverse and controversial statements is key for inclusivity. Learn how multi-round debates enhance statement quality and how DEBATUNE refines LLMs in this new paper: http://arxiv.org/abs/2402.10614v2 #AI #NLP #LLMs #InclusivityResearch \ud83e\udd16\ud83d\udcda\ud83d\udd0d"},{"location":"sections/papers/-1806687785486849885/e086c20943b945085fe4dc6e8194d198338bb80cb3d0cc13d67125075c3af1a2/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/e59a93b980c1587952fd591081edb63485c9d8a67a6813d2640ac44585382908/","title":"Towards Training A Chinese Large Language Model for Anesthesiology","text":"<p>Arxiv Link - 2024-03-05 07:53:49 </p>"},{"location":"sections/papers/-1806687785486849885/e59a93b980c1587952fd591081edb63485c9d8a67a6813d2640ac44585382908/#abstract","title":"Abstract","text":"<p>Medical large language models (LLMs) have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of LLMs in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one LLM to assess the quality of the generated data from another LLM and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by fine-tuning LLMs using the general medicine data and subsequently improving the fine-tuned LLMs using data specifically from Anesthesiology. The general medical data supplement the medical expertise in Anesthesiology and enhance the effectiveness of Hypnos' generation. 3) We introduce a standardized benchmark for evaluating medical LLM in Anesthesiology. Our benchmark includes both publicly available instances from the Internet and privately obtained cases from the Hospital. Hypnos outperforms other medical LLMs in anesthesiology in metrics, GPT-4, and human evaluation on the benchmark dataset. </p>"},{"location":"sections/papers/-1806687785486849885/e59a93b980c1587952fd591081edb63485c9d8a67a6813d2640ac44585382908/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of AI and healthcare! Introducing Hypnos, a groundbreaking Chinese Anesthesia model that pushes the boundaries of Large Language Models (LLMs) like never before. \ud83e\udde0 Hypnos addresses the gap in specialized medical fields by leveraging a unique approach. It enhances data quality by implementing a cross-filtering strategy, utilizes a general-to-specific training method, and introduces a standardized benchmark for evaluating medical LLMs in Anesthesiology.\ud83d\udcc8 The results speak for themselves - Hypnos outperforms other medical LLMs in anesthesiology in various metrics, including GPT-4 and human evaluation. \ud83d\udd17 Read more about Hypnos and its innovative contributions in the field of AI and healthcare in the research paper here: http://arxiv.org/abs/2403.02742v1#AI #HealthTech #LLMs #Anesthesiology #Innovation #HealthcareAI \ud83d\ude80 Exciting news in the world of AI and healthcare! Introducing Hypnos, a Chinese Anesthesia model that outperforms other medical LLMs in anesthesiology. Learn about its innovative strategies and impressive results here: http://arxiv.org/abs/2403.02742v1 #AI #HealthTech #LLMs #Anesthesiology \ud83c\udfe5\ud83d\udca1"},{"location":"sections/papers/-1806687785486849885/e59a93b980c1587952fd591081edb63485c9d8a67a6813d2640ac44585382908/#pdf","title":"PDF","text":""},{"location":"sections/papers/-1806687785486849885/f7f262cce54cf8b35012c5328fd7f800707f80925f207c2ea4fbf59f76120000/","title":"MEGAnno+: A Human-LLM Collaborative Annotation System","text":"<p>Arxiv Link - 2024-02-28 04:58:07 </p>"},{"location":"sections/papers/-1806687785486849885/f7f262cce54cf8b35012c5328fd7f800707f80925f207c2ea4fbf59f76120000/#abstract","title":"Abstract","text":"<p>Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans. </p>"},{"location":"sections/papers/-1806687785486849885/f7f262cce54cf8b35012c5328fd7f800707f80925f207c2ea4fbf59f76120000/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in AI and NLP! \ud83d\ude80As large language models (LLMs) continue to revolutionize the field of Natural Language Processing (NLP), a critical discussion emerges on the importance of combining human expertise with machine efficiency. While LLMs excel in rapid and cost-effective data labeling, their limitations in grasping intricate social, cultural, or domain-specific nuances may result in inaccuracies.Introducing MEGAnno+ - a cutting-edge human-LLM collaborative annotation system designed to enhance the accuracy and quality of labeled data. By leveraging the strengths of both humans and LLMs, MEGAnno+ offers efficient LLM agent and annotation management, robust annotation processes, and human verification to ensure precise labeling.Discover how collaborative efforts between humans and LLMs can elevate the standard of data annotation and drive impactful advancements in AI and NLP. Dive into the details of MEGAnno+ here: Read more#AI #NLP #Collaboration #DataAnnotation #MEGAnno+ #TechInnovation \ud83c\udf1f \ud83d\ude80 Exciting research alert! Large language models (#LLMs) are speeding up data labeling, but may lack nuanced understanding. Check out MEGAnno+, a collaborative human-LLM annotation system for reliable labels. Dive into the details here: http://arxiv.org/abs/2402.18050v1 #NLP #AI"},{"location":"sections/papers/-1806687785486849885/f7f262cce54cf8b35012c5328fd7f800707f80925f207c2ea4fbf59f76120000/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/152fe63bae013960eb84a72664e18eaa9a9dd4cf2c2bdc2c59677191f006d956/","title":"Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF","text":"<p>Arxiv Link - 2024-06-06 20:10:34 </p>"},{"location":"sections/papers/-2211300724814296630/152fe63bae013960eb84a72664e18eaa9a9dd4cf2c2bdc2c59677191f006d956/#abstract","title":"Abstract","text":"<p>Reinforcement Learning from Human Feedback (RLHF) is popular in large language models (LLMs), whereas traditional Reinforcement Learning (RL) often falls short. Current autonomous driving methods typically utilize either human feedback in machine learning, including RL, or LLMs. Most feedback guides the car agent's learning process (e.g., controlling the car). RLHF is usually applied in the fine-tuning step, requiring direct human \"preferences,\" which are not commonly used in optimizing autonomous driving models. In this research, we innovatively combine RLHF and LLMs to enhance autonomous driving safety. Training a model with human guidance from scratch is inefficient. Our framework starts with a pre-trained autonomous car agent model and implements multiple human-controlled agents, such as cars and pedestrians, to simulate real-life road environments. The autonomous car model is not directly controlled by humans. We integrate both physical and physiological feedback to fine-tune the model, optimizing this process using LLMs. This multi-agent interactive environment ensures safe, realistic interactions before real-world application. Finally, we will validate our model using data gathered from real-life testbeds located in New Jersey and New York City. </p>"},{"location":"sections/papers/-2211300724814296630/152fe63bae013960eb84a72664e18eaa9a9dd4cf2c2bdc2c59677191f006d956/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude97 Exciting news in autonomous driving tech! \ud83d\ude97Interested in the latest research on enhancing autonomous driving safety using Reinforcement Learning from Human Feedback (RLHF) and Large Language Models (LLMs)? Check out this innovative approach that combines RLHF and LLMs to optimize autonomous driving models. Want to learn more about this cutting-edge research and how it can revolutionize autonomous driving technology? Dive into the details here: http://arxiv.org/abs/2406.04481v1#AutonomousDriving #RLHF #LLMs #Innovation #AI #TechResearch \ud83d\ude97\ud83e\udd16 Exciting research combining Reinforcement Learning from Human Feedback (RLHF) and Large Language Models (LLMs) to enhance autonomous driving safety! Learn how this innovative approach optimizes the learning process and ensures realistic interactions: http://arxiv.org/abs/2406.04481v1 #AI #RLHF #LLMs #AutonomousDriving \ud83d\udee3\ufe0f\ud83d\udd2c"},{"location":"sections/papers/-2211300724814296630/152fe63bae013960eb84a72664e18eaa9a9dd4cf2c2bdc2c59677191f006d956/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/161a89c1138b09aabb101e4feecf966e5f4d3fe3522d432fc23de3329ab94cdf/","title":"LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback","text":"<p>Arxiv Link - 2024-06-03 20:25:12 </p>"},{"location":"sections/papers/-2211300724814296630/161a89c1138b09aabb101e4feecf966e5f4d3fe3522d432fc23de3329ab94cdf/#abstract","title":"Abstract","text":"<p>To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages. </p>"},{"location":"sections/papers/-2211300724814296630/161a89c1138b09aabb101e4feecf966e5f4d3fe3522d432fc23de3329ab94cdf/#socials","title":"Socials","text":"LinkedIn X \ud83c\udf1f Exciting News in the World of Language Models! \ud83c\udf1fIn a recent paper, researchers have introduced xLLaMA-100 and xBLOOM-100 (xLLMs-100), scaling multilingual capabilities to an impressive 100 languages. By constructing datasets that include a multilingual instruction dataset with 100 languages and a cross-lingual human feedback dataset with 30 languages, these models have achieved a new state-of-the-art in multilingual language understanding and generation.Read the full paper here for details on how xLLMs-100 outperforms its peers across five multilingual benchmarks: http://arxiv.org/abs/2406.01771v1#LanguageModels #AI #NLP #Multilingual #TechInnovation \ud83c\udf10 Exciting development in the world of multilingual large language models! Introducing xLLMs-100, a new state-of-the-art model supporting 100 languages. Learn more about its remarkable performance and capabilities in this research paper: http://arxiv.org/abs/2406.01771v1 #AI #NLP #LLMs #MultilingualModels \ud83d\ude80\ud83d\udcda"},{"location":"sections/papers/-2211300724814296630/161a89c1138b09aabb101e4feecf966e5f4d3fe3522d432fc23de3329ab94cdf/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/26fda650c80ea2b7e0b8008755b9c4501791c8d9746bcd00367481eee390a8a1/","title":"Prototypical Reward Network for Data-Efficient RLHF","text":"<p>Arxiv Link - 2024-06-06 15:23:30 </p>"},{"location":"sections/papers/-2211300724814296630/26fda650c80ea2b7e0b8008755b9c4501791c8d9746bcd00367481eee390a8a1/#abstract","title":"Abstract","text":"<p>The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences. Extensive experiments on various datasets demonstrate that Proto-RM significantly improves the performance of reward models and LLMs in human feedback tasks, achieving comparable and usually better results than traditional methods, while requiring significantly less data. in data-limited scenarios. This research offers a promising direction for enhancing the efficiency of reward models and optimizing the fine-tuning of language models under restricted feedback conditions. </p>"},{"location":"sections/papers/-2211300724814296630/26fda650c80ea2b7e0b8008755b9c4501791c8d9746bcd00367481eee390a8a1/#socials","title":"Socials","text":"LinkedIn X \ud83c\udf1f Exciting News in AI and NLP! \ud83c\udf1fThe latest research introduces Proto-RM, a cutting-edge framework leveraging prototypical networks to enhance reward models for Large Language Models (LLMs) in Reinforcement Learning from Human Feedback (RLHF) tasks. Proto-RM enables stable and reliable structural learning from limited human feedback, significantly boosting LLMs' adaptability and accuracy in interpreting human preferences.\ud83d\udd0d Research Results:Extensive experiments across various datasets demonstrate that Proto-RM outperforms traditional methods in human feedback tasks, achieving comparable or even superior results while requiring significantly less data in data-limited scenarios.\ud83d\ude80 Dive into the details of this groundbreaking research at:http://arxiv.org/abs/2406.06606v1#AI #NLP #LLMs #ProtoRM #HumanFeedback #ReinforcementLearning #Research #Innovation #TechBreakthrough \ud83d\ude80 Exciting new research alert! Proto-RM framework enhances reward models for Large Language Models under limited human feedback, boosting adaptability and accuracy. \ud83e\udd16\ud83d\udcc8 Check out the results here: http://arxiv.org/abs/2406.06606v1 #AI #NLP #LLMs #ProtoRM #Research #TechInnovation"},{"location":"sections/papers/-2211300724814296630/26fda650c80ea2b7e0b8008755b9c4501791c8d9746bcd00367481eee390a8a1/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/4a7341f384539567332169b8690ba66532249f88f28a5bed368d04e5fae6bf96/","title":"LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models","text":"<p>Arxiv Link - 2023-12-31 21:18:16 </p>"},{"location":"sections/papers/-2211300724814296630/4a7341f384539567332169b8690ba66532249f88f28a5bed368d04e5fae6bf96/#abstract","title":"Abstract","text":"<p>Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion of human-annotated data in the annotated datasets affects the fine-tuning performance. </p>"},{"location":"sections/papers/-2211300724814296630/4a7341f384539567332169b8690ba66532249f88f28a5bed368d04e5fae6bf96/#socials","title":"Socials","text":"LinkedIn X \ud83c\udf1f Exciting News in the World of Large Language Models (LLMs) \ud83c\udf1fFine-tuning LLMs for specific tasks has always been crucial for enhancing performance. However, Supervised Fine-Tuning (SFT) can sometimes lead to errors and hallucinations in reasoning tasks. But fear not, a new approach called LaFFi (Natural Language Feedback for Finetuning LLMs) is here to revolutionize the game!This innovative method has LLMs predict the feedback they would receive from an annotator, leading to a significant boost in accuracy for in-domain question-answering tasks. The study also highlights the impact of human-annotated data on fine-tuning performance.Excited to delve deeper into this groundbreaking research? Check out the full paper here: http://arxiv.org/abs/2401.00907v1#LLMs #AI #NLP #LaFFi #TechInnovation #ResearchHighlight \ud83d\ude80 Exciting research alert! Discover LaFFi, a new approach to fine-tuning Large Language Models for improved task performance in question-answering tasks. Learn more about how natural language feedback enhances accuracy and fine-tuning performance: http://arxiv.org/abs/2401.00907v1 #AI #NLP #LLMs #LaFFi #ResearchPub"},{"location":"sections/papers/-2211300724814296630/4a7341f384539567332169b8690ba66532249f88f28a5bed368d04e5fae6bf96/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/63f77ec106ec48f8bdf1b500b894faddf3808367057f1138ce80396ce9681d20/","title":"IterAlign: Iterative Constitutional Alignment of Large Language Models","text":"<p>Arxiv Link - 2024-03-27 08:32:19 </p>"},{"location":"sections/papers/-2211300724814296630/63f77ec106ec48f8bdf1b500b894faddf3808367057f1138ce80396ce9681d20/#abstract","title":"Abstract","text":"<p>With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\\%$ in harmlessness. </p>"},{"location":"sections/papers/-2211300724814296630/63f77ec106ec48f8bdf1b500b894faddf3808367057f1138ce80396ce9681d20/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in the field of Large Language Models (LLMs) are shaping the future of AI! Ensuring the alignment of LLMs with human values and societal norms is crucial for their reliability and safety. Discover how IterAlign, a data-driven constitution discovery and self-alignment framework, is revolutionizing LLM alignment by automatically uncovering new constitutions to guide self-correction. This innovative approach leverages red teaming to identify weaknesses in LLMs and enhance their alignment without heavy human annotations or predefined constitutions.Check out the research paper to learn more about IterAlign and its impressive results in improving LLM alignment by up to 13.5% in harmlessness: http://arxiv.org/abs/2403.18341v1#LLM #AIalignment #TechInnovation #AIethics #IterAlign #AIresearch #TechAdvancements \ud83d\ude80 Exciting development in the realm of LLM alignment! Introducing IterAlign, a data-driven constitution discovery and self-alignment framework for large language models. Discover how IterAlign leverages red teaming to enhance LLM alignment by up to 13.5% in harmlessness! Check out the research at: http://arxiv.org/abs/2403.18341v1 #AI #LLM #Alignment #TechResearch"},{"location":"sections/papers/-2211300724814296630/63f77ec106ec48f8bdf1b500b894faddf3808367057f1138ce80396ce9681d20/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/8b58607e712ea934d601ff2b19265e37cfd367434e6f21591cb2a9b4f6a2d8b9/","title":"Verbosity Bias in Preference Labeling by Large Language Models","text":"<p>Arxiv Link - 2023-10-16 05:19:02 </p>"},{"location":"sections/papers/-2211300724814296630/8b58607e712ea934d601ff2b19265e37cfd367434e6f21591cb2a9b4f6a2d8b9/#abstract","title":"Abstract","text":"<p>In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias. </p>"},{"location":"sections/papers/-2211300724814296630/8b58607e712ea934d601ff2b19265e37cfd367434e6f21591cb2a9b4f6a2d8b9/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in Large Language Models (LLMs)! \ud83e\udd16\ud83e\udde0The rise of LLMs has reshaped the field of natural language processing and machine learning, with Reinforcement Learning from Human Feedback (RLHF) playing a crucial role in enhancing their performance. Recent studies are delving into the realm of Reinforcement Learning from AI Feedback (RLAIF), exploring how LLMs can learn from one another.\ud83d\udd0d However, it's essential to consider biases that may arise when evaluating LLMs with other LLMs. A recent study highlighted the verbosity bias, where LLMs tend to favor longer answers, even if shorter responses are equally effective. For example, GPT-4 has shown a preference for lengthier outputs compared to human preferences.Curious to learn more about this bias and its implications? Check out the full study here: http://arxiv.org/abs/2310.10076v1#AI #NLP #LLMs #MachineLearning #RLHF #RLAIF #BiasInAI #TechResearch #GPT4 #Bard \"\ud83e\udd16\ud83d\udd0d Exciting developments in the world of Large Language Models (LLMs)! Learn how Reinforcement Learning from AI Feedback is shaping the future of LLMs like GPT-4 and Bard. Discover insights on biases in evaluating LLMs and delve into the verbosity bias phenomenon. Check out the study here: http://arxiv.org/abs/2310.10076v1 #AI #NLP #LLMs\""},{"location":"sections/papers/-2211300724814296630/8b58607e712ea934d601ff2b19265e37cfd367434e6f21591cb2a9b4f6a2d8b9/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/a90c9c743f8dc92203477ae13fca98f9cc08811673b2b108bc03fca13e479eda/","title":"A Survey on Human Preference Learning for Large Language Models","text":"<p>Arxiv Link - 2024-06-17 03:52:51 </p>"},{"location":"sections/papers/-2211300724814296630/a90c9c743f8dc92203477ae13fca98f9cc08811673b2b108bc03fca13e479eda/#abstract","title":"Abstract","text":"<p>The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs. </p>"},{"location":"sections/papers/-2211300724814296630/a90c9c743f8dc92203477ae13fca98f9cc08811673b2b108bc03fca13e479eda/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the world of AI and NLP! Check out this insightful survey on how human preferences are introduced into Large Language Models (LLMs) to enhance their effectiveness across diverse contexts. The survey categorizes human feedback sources, preference modeling techniques, usage methods, and evaluation approaches for aligned LLMs. Dive deeper into the relationship between human preferences and LLMs for a comprehensive understanding.Read the full survey here: http://arxiv.org/abs/2406.11191v1#AI #NLP #LLMs #PreferenceLearning #TechSurvey #HumanIntentions #Innovation #TechTrends \ud83d\ude80 Exciting insights on aligning human preferences with Large Language Models (LLMs) for enhanced effectiveness! This survey delves into the sources, formats, modeling, and evaluation of human feedback in LLMs. Dive deeper into the realm of preference-centered learning here: http://arxiv.org/abs/2406.11191v1 #AI #NLP #LLMs #TechResearch"},{"location":"sections/papers/-2211300724814296630/a90c9c743f8dc92203477ae13fca98f9cc08811673b2b108bc03fca13e479eda/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/be9144b6323f1bb4e1c337a67266379297875b771915a34d4cc4a740aa1c6a6d/","title":"LLM A: Human in the Loop Large Language Models Enabled A Search for Robotics","text":"<p>Arxiv Link - 2023-12-04 10:37:58 </p>"},{"location":"sections/papers/-2211300724814296630/be9144b6323f1bb4e1c337a67266379297875b771915a34d4cc4a740aa1c6a6d/#abstract","title":"Abstract","text":"<p>This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A, aims to leverage the commonsense of LLMs, and the utility-optimal A is proposed to facilitate few-shot near-optimal path planning. Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to LLMs on intermediate planning results. This makes the whole path planning process a `white box' and human feedback guides LLM A to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning. In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques. Comparative analysis against A and RL shows that LLM A is more efficient in terms of search space and achieves an on-a-par path with A and a better path than RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks. </p>"},{"location":"sections/papers/-2211300724814296630/be9144b6323f1bb4e1c337a67266379297875b771915a34d4cc4a740aa1c6a6d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in AI Research! \ud83e\udd16Delving into the realm of Large Language Models (LLMs), a groundbreaking study unveils the potential of LLMs in enhancing path planning for mobile robots. The introduction of LLM A showcases a novel framework that merges LLMs' commonsense with the efficiency of A to revolutionize few-shot near-optimal path planning. By integrating human feedback through prompts, LLM A* emerges as a 'white box' solution, outperforming RL-based methods and streamlining the convergence process.Discover more about this cutting-edge research and the promising implications for collaborative human-robot tasks: Read the full study here!#AI #LLM #PathPlanning #Robotics #Research #Innovation #TechNews Exciting research on leveraging Large Language Models (LLMs) for path planning in robots! The novel LLM A framework combines LLM commonsense and A for efficient few-shot path planning. Human feedback guides LLM A* to quick convergence, outperforming RL methods. Learn more at: http://arxiv.org/abs/2312.01797v1 #AI #LLMs #Robotics"},{"location":"sections/papers/-2211300724814296630/be9144b6323f1bb4e1c337a67266379297875b771915a34d4cc4a740aa1c6a6d/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/d4a1ed36a75d69ff02912f6cc6ea4ece3b27a1956eb3c9e660709ce7dd363d3f/","title":"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values","text":"<p>Arxiv Link - 2023-10-11 16:18:13 </p>"},{"location":"sections/papers/-2211300724814296630/d4a1ed36a75d69ff02912f6cc6ea4ece3b27a1956eb3c9e660709ce7dd363d3f/#abstract","title":"Abstract","text":"<p>Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories.First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges. </p>"},{"location":"sections/papers/-2211300724814296630/d4a1ed36a75d69ff02912f6cc6ea4ece3b27a1956eb3c9e660709ce7dd363d3f/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting insights into the world of Large Language Models (LLMs) and human feedback! \ud83c\udf1fCurious about how human feedback shapes the behavior of LLMs? Dive into this comprehensive survey of existing approaches in the field, drawing on 95 papers from ACL and arXiv repositories. Learn about past trends, current techniques, and future challenges in integrating human feedback into language models.Read more about this fascinating research at: http://arxiv.org/abs/2310.07629v1#LLMs #HumanFeedback #AI #NLP #Research #TechInnovation \ud83d\ude80 Exciting read! Check out this insightful paper on incorporating human feedback into Large Language Models (LLMs) efficiently and effectively. Learn about past trends, present techniques, and future challenges in feedback learning for LLMs. #AI #NLP\ud83d\udcc4 Read more: http://arxiv.org/abs/2310.07629v1"},{"location":"sections/papers/-2211300724814296630/d4a1ed36a75d69ff02912f6cc6ea4ece3b27a1956eb3c9e660709ce7dd363d3f/#pdf","title":"PDF","text":""},{"location":"sections/papers/-2211300724814296630/ef7e94f530790fb26073eedbef0c2df58dd48ae9deca359707bc47cf4d040df5/","title":"Aligning Large Language Models through Synthetic Feedback","text":"<p>Arxiv Link - 2023-10-21 01:50:54 </p>"},{"location":"sections/papers/-2211300724814296630/ef7e94f530790fb26073eedbef0c2df58dd48ae9deca359707bc47cf4d040df5/#abstract","title":"Abstract","text":"<p>Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost </p>"},{"location":"sections/papers/-2211300724814296630/ef7e94f530790fb26073eedbef0c2df58dd48ae9deca359707bc47cf4d040df5/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of large language models (LLMs)! A new alignment learning framework, ALMoST, has been introduced to enhance LLMs' alignment with human values without relying on extensive human annotations or proprietary LLMs like ChatGPT. By utilizing synthetic feedback and innovative reward modeling techniques, ALMoST outperforms existing open-sourced models in alignment benchmarks. Human evaluation shows that ALMoST is preferred over Alpaca and Dolly-v2 a significant percentage of the time.Read the full research paper and access the code at: http://arxiv.org/abs/2305.13735v2#AI #NLP #LLMs #AlignmentLearning #Research #TechInnovation \ud83d\ude80 Exciting new research alert! A novel alignment learning framework, ALMoST, outperforms recent models in alignment benchmarks without extensive human annotations or proprietary LLMs. Learn more at: http://arxiv.org/abs/2305.13735v2 #AI #NLP #LLMs #Tech #Research #ALMoST"},{"location":"sections/papers/-2211300724814296630/ef7e94f530790fb26073eedbef0c2df58dd48ae9deca359707bc47cf4d040df5/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/237ff27acb132247fa6ce0e92ff5489c49b62ec7834673f8ba035850a131b670/","title":"Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs","text":"<p>Arxiv Link - 2024-04-01 06:01:17 </p>"},{"location":"sections/papers/-8377067242701614157/237ff27acb132247fa6ce0e92ff5489c49b62ec7834673f8ba035850a131b670/#abstract","title":"Abstract","text":"<p>The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of LLM outputs. The code is publicly available at https://github.com/xz-liu/GraphEval. </p>"},{"location":"sections/papers/-8377067242701614157/237ff27acb132247fa6ce0e92ff5489c49b62ec7834673f8ba035850a131b670/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in the world of AI and Large Language Models (LLMs)! \ud83c\udf10The rise of LLMs has revolutionized AI capabilities, but the issue of factuality remains a crucial concern. How can we ensure that LLMs provide accurate responses? \ud83e\udd16Introducing GraphEval - a novel approach to evaluating LLM performance using a vast test dataset sourced from a knowledge graph with over 10 million facts. \ud83d\udcca By leveraging a judge model, GraphEval estimates the correctness of LLM outputs, streamlining evaluation processes and reducing costs significantly.Our experiments have shown that the judge model's factuality assessment closely aligns with the accuracy of LLM-generated responses, offering valuable insights into performance metrics and paving the way for future enhancements in ensuring factual integrity. \ud83d\udcc8Curious to learn more? Dive into the details and explore the code at: https://github.com/xz-liu/GraphEval \ud83d\udcddRead the full paper here: http://arxiv.org/abs/2404.00942v1 \ud83d\udcd1#AI #LLMs #GraphEval #MachineLearning #ArtificialIntelligence #TechInnovation #ResearchPaper #GitHubLet's continue pushing the boundaries of AI together! \ud83d\udca1\ud83d\udd0d \ud83c\udf1f Exciting research on evaluating Large Language Models (LLMs) using GraphEval for factuality assessment without costly human efforts! Find out how this approach enhances LLM performance and reduces evaluation costs. Read the paper at: http://arxiv.org/abs/2404.00942v1 #AI #NLP #LLM #GraphEval \ud83e\udd16\ud83d\udcca"},{"location":"sections/papers/-8377067242701614157/237ff27acb132247fa6ce0e92ff5489c49b62ec7834673f8ba035850a131b670/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/4a8e937da1200627830870c07497131fe7c84309187edbcf18961a0425853aa2/","title":"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena","text":"<p>Arxiv Link - 2023-12-24 02:01:34 </p>"},{"location":"sections/papers/-8377067242701614157/4a8e937da1200627830870c07497131fe7c84309187edbcf18961a0425853aa2/#abstract","title":"Abstract","text":"<p>Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge. </p>"},{"location":"sections/papers/-8377067242701614157/4a8e937da1200627830870c07497131fe7c84309187edbcf18961a0425853aa2/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of AI and large language models (LLMs)! Researchers have delved into using strong LLMs as judges to evaluate chat assistants on open-ended questions, addressing the challenges of measuring human preferences. The study explores the limitations and biases of LLM-as-a-judge and proposes solutions to enhance evaluation accuracy. Results show that powerful LLM judges like GPT-4 can closely match human preferences, achieving over 80% agreement. This approach offers a scalable and explainable method to approximate human preferences efficiently. The study introduces two benchmarks, MT-bench, and Chatbot Arena, to verify agreement between LLM judges and human preferences.Access the MT-bench questions, expert votes, and conversations at: GitHub - FastChat. Curious to learn more? Dive into the research here: Research Paper#AI #LLM #Research #NLP #ChatAssistants #ArtificialIntelligence Exciting new research on using large language models as judges to evaluate chat assistants reveals promising results! Strong LLM judges like GPT-4 can match human preferences with over 80% agreement, making them a scalable and cost-effective alternative. Learn more about this innovative approach and access the benchmarks at: http://arxiv.org/abs/2306.05685v4 #AI #NLP #LLMs #TechResearch"},{"location":"sections/papers/-8377067242701614157/4a8e937da1200627830870c07497131fe7c84309187edbcf18961a0425853aa2/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/70765de51dfb956ec7b0ff4a774142731ca4d9c4136387a3071b15c894d0b777/","title":"Humans or LLMs as the Judge? A Study on Judgement Biases","text":"<p>Arxiv Link - 2024-04-17 09:56:26 </p>"},{"location":"sections/papers/-8377067242701614157/70765de51dfb956ec7b0ff4a774142731ca4d9c4136387a3071b15c894d0b777/#abstract","title":"Abstract","text":"<p>Adopting human and large language models (LLM) as judges (\\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating Fallacy Oversight Bias, Authority Bias and Beauty Bias on LLM and human judges. We curate a dataset referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of developing robust evaluation systems. </p>"},{"location":"sections/papers/-8377067242701614157/70765de51dfb956ec7b0ff4a774142731ca4d9c4136387a3071b15c894d0b777/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting new research alert! \ud83d\ude80Adopting human and large language models (LLM) as judges for evaluating LLM performance has been a hot topic lately. But, what if I told you that this approach might introduce biases that could impact the reliability of evaluation results? \ud83d\ude31In a recent paper, a novel framework was proposed to investigate Fallacy Oversight Bias, Authority Bias, and Beauty Bias on LLM and human judges without relying on groundtruth annotations. Thousands of human and LLM evaluations were conducted using a curated dataset based on the revised Bloom's Taxonomy.The results were eye-opening! Both human and LLM judges showed vulnerabilities to perturbations, with even cutting-edge judges displaying significant biases. The study went a step further by conducting attacks on LLM judges to exploit these weaknesses.Check out the full paper to learn more about the vulnerability of human- and LLM-as-a-judge against perturbations and the importance of developing robust evaluation systems. Knowledge is power! \ud83d\udcaaRead the full paper here: Link to the research paper#AI #NLP #LLM #Research #Tech #EvaluationBias #Innovation #TechNews #ArtificialIntelligence \ud83e\udd16\ud83e\udde0 New research alert! Investigating biases in human and large language models (LLM) as judges for evaluating LLM performance. Results show vulnerability to perturbations and considerable biases even in cutting-edge judges. Learn more at: http://arxiv.org/abs/2402.10669v3 #AI #NLP #LLM #Research #TechBias"},{"location":"sections/papers/-8377067242701614157/70765de51dfb956ec7b0ff4a774142731ca4d9c4136387a3071b15c894d0b777/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/9cddd29dd173d2bc65c5919560c8b6250120323af4e1529c23a5c890389fec89/","title":"Can LLM be a Personalized Judge?","text":"<p>Arxiv Link - 2024-06-17 15:41:30 </p>"},{"location":"sections/papers/-8377067242701614157/9cddd29dd173d2bc65c5919560c8b6250120323af4e1529c23a5c890389fec89/#abstract","title":"Abstract","text":"<p>Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization. </p>"},{"location":"sections/papers/-8377067242701614157/9cddd29dd173d2bc65c5919560c8b6250120323af4e1529c23a5c890389fec89/#socials","title":"Socials","text":"LinkedIn X \ud83c\udf1f Exciting Research Alert! \ud83c\udf1fEnsuring that large language models truly reflect diverse user values and preferences is crucial as their global user bases expand. The latest research delves into the realm of LLM personalization, aiming to enhance user experience. Check out the intriguing findings on the reliability of LLM-as-a-Personalized-Judge approach in evaluating user preferences based on personas. Discover more about this insightful study and its implications for the future of LLM personalization here: http://arxiv.org/abs/2406.11657v1#AI #LLM #NLP #Personalization #Research #Tech #Innovation #UserExperience \"Just in: Research investigates the reliability of LLM-as-a-Personalized-Judge for evaluating user preferences based on personas. Findings show low reliability without verbal uncertainty estimation. Learn more at: http://arxiv.org/abs/2406.11657v1 #LLM #personalization #AI\""},{"location":"sections/papers/-8377067242701614157/9cddd29dd173d2bc65c5919560c8b6250120323af4e1529c23a5c890389fec89/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/ad26039fc4e1875e4fe645df28ec3e8ea82fd42edf299860ff2ab0939bf8118d/","title":"Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment","text":"<p>Arxiv Link - 2024-02-21 18:55:20 </p>"},{"location":"sections/papers/-8377067242701614157/ad26039fc4e1875e4fe645df28ec3e8ea82fd42edf299860ff2ab0939bf8118d/#abstract","title":"Abstract","text":"<p>Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5. This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios. </p>"},{"location":"sections/papers/-8377067242701614157/ad26039fc4e1875e4fe645df28ec3e8ea82fd42edf299860ff2ab0939bf8118d/#socials","title":"Socials","text":"LinkedIn X \ud83d\udea8 New Research Alert! \ud83d\udea8Exciting findings on the vulnerability of assessment Large Language Models (LLMs) have been published. This groundbreaking study sheds light on the susceptibility of judge-LLMs to manipulation by adversaries aiming to deceive the system and obtain high assessment scores.Discover more about this innovative research and its implications for the reliability of LLMs in real-world scenarios by reading the full paper here: http://arxiv.org/abs/2402.14016v1#AI #NLP #LLMs #Research #TechInnovation #ArtificialIntelligence #MachineLearning #TechNews \ud83d\udea8 New research alert! A study on the vulnerability of assessment Large Language Models (LLMs) reveals susceptibility to simple attacks, raising concerns about their reliability in real-world scenarios. Learn more at: http://arxiv.org/abs/2402.14016v1 #AI #NLP #LLMs #Research #TechEthics \ud83e\udd16\ud83d\udd0d\ud83d\udcda"},{"location":"sections/papers/-8377067242701614157/ad26039fc4e1875e4fe645df28ec3e8ea82fd42edf299860ff2ab0939bf8118d/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/bcbec230eddceaf3f7eef0799d1cf561ff6fa83a8ce09ed3233fe97112d5dac4/","title":"Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models","text":"<p>Arxiv Link - 2023-08-24 05:35:58 </p>"},{"location":"sections/papers/-8377067242701614157/bcbec230eddceaf3f7eef0799d1cf561ff6fa83a8ce09ed3233fe97112d5dac4/#abstract","title":"Abstract","text":"<p>Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as \"re-judge inconsistency\" in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs' capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs. </p>"},{"location":"sections/papers/-8377067242701614157/bcbec230eddceaf3f7eef0799d1cf561ff6fa83a8ce09ed3233fe97112d5dac4/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting insights into the cognitive constructs of Large Language Models (LLMs) have been uncovered! Research suggests that LLMs exhibit cognitive phenomena akin to human behavior, such as explicit and implicit social bias. Learn more about a new study delving into this intriguing parallel between human cognition and LLM behavior. Check out the research paper here: http://arxiv.org/abs/2308.12578v1 #AI #NLP #LLMs #Research #CognitiveScience \ud83e\udde0\ud83d\udd0d \ud83e\udde0 New research suggests that Large Language Models exhibit cognitive constructs resembling humans, sparking interest in investigating their cognitive aspects. Learn about the parallel phenomenon of \"re-judge inconsistency\" in social bias discovered in LLMs. Check out the study here: http://arxiv.org/abs/2308.12578v1 #AI #NLP #LLM #Research #CognitiveBias"},{"location":"sections/papers/-8377067242701614157/bcbec230eddceaf3f7eef0799d1cf561ff6fa83a8ce09ed3233fe97112d5dac4/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/cc7b4cb45b132cc3d23ba68a3834cd8451e8162897864fc7aba7ad6a63370881/","title":"JudgeLM: Fine-tuned Large Language Models are Scalable Judges","text":"<p>Arxiv Link - 2023-10-26 17:48:58 </p>"},{"location":"sections/papers/-8377067242701614157/cc7b4cb45b132cc3d23ba68a3834cd8451e8162897864fc7aba7ad6a63370881/#abstract","title":"Abstract","text":"<p>Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat. </p>"},{"location":"sections/papers/-8377067242701614157/cc7b4cb45b132cc3d23ba68a3834cd8451e8162897864fc7aba7ad6a63370881/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in the field of Large Language Models (LLMs)! Researchers have introduced JudgeLM, fine-tuned LLMs acting as scalable judges to efficiently evaluate LLMs in open-ended scenarios. By leveraging a high-quality dataset and novel benchmark, JudgeLM has achieved state-of-the-art performance, outperforming human-level agreement. Curious to learn more about this groundbreaking approach and its implications? Check out the full paper at: http://arxiv.org/abs/2310.17631v1#AI #NLP #LLMs #JudgeLM #TechInnovation #Research #ArtificialIntelligence \ud83d\ude80 Exciting new research on evaluating Large Language Models (LLMs) efficiently and effectively in open-ended scenarios! Learn how JudgeLM fine-tunes LLMs as scalable judges, achieving state-of-the-art performance and surpassing human agreement. Check out the paper here: http://arxiv.org/abs/2310.17631v1 #AI #NLP #LLMs #JudgeLM"},{"location":"sections/papers/-8377067242701614157/cc7b4cb45b132cc3d23ba68a3834cd8451e8162897864fc7aba7ad6a63370881/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/cfbe349f56e950ad4fde2cc529897893012f1d0466fcc1645f8530572efc04fd/","title":"Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!","text":"<p>Arxiv Link - 2024-06-17 15:11:58 </p>"},{"location":"sections/papers/-8377067242701614157/cfbe349f56e950ad4fde2cc529897893012f1d0466fcc1645f8530572efc04fd/#abstract","title":"Abstract","text":"<p>Leveraging Large Language Models (LLMs) as judges for evaluating the performance of LLMs has recently garnered attention. Nonetheless, this type of approach concurrently introduces potential biases from LLMs, raising concerns about the reliability of the evaluation results. To mitigate this issue, we propose and study two versions of many-shot in-context prompts, Reinforced and Unsupervised ICL, for helping GPT-4o-as-a-Judge in single answer grading. Based on the designed prompts, we investigate the impact of scaling the number of in-context examples on the agreement and quality of the evaluation. Furthermore, we first reveal the symbol bias in GPT-4o-as-a-Judge for pairwise comparison and then propose a simple yet effective approach to mitigate it. Experimental results show that advanced long-context LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot regime. Meanwhile, the experimental results further verify the effectiveness of the symbol bias mitigation approach. </p>"},{"location":"sections/papers/-8377067242701614157/cfbe349f56e950ad4fde2cc529897893012f1d0466fcc1645f8530572efc04fd/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in the world of Large Language Models (LLMs)! \ud83e\udd16 Our latest study explores the use of many-shot in-context prompts, Reinforced and Unsupervised ICL, to enhance the evaluation process of LLMs like GPT-4o-as-a-Judge in single answer grading. Find out how scaling the number of in-context examples impacts evaluation quality and agreement. Discover the revealed symbol bias in GPT-4o-as-a-Judge and a novel approach to address it effectively. \ud83d\udcca\ud83d\udd0dRead more about our research and experimental results here: Link to the study #AI #NLP #LLMs #Research #GPT4o #TechInnovation #ArtificialIntelligence #LanguageModels \ud83d\ude80 Exciting new research on leveraging Large Language Models (LLMs) as judges for evaluating LLM performance! Learn about the proposed many-shot in-context prompts for GPT-4o-as-a-Judge in single-answer grading and how to mitigate potential biases. Check out the study at: http://arxiv.org/abs/2406.11629v1 #AI #NLP #LLMs #Research #GPT4o"},{"location":"sections/papers/-8377067242701614157/cfbe349f56e950ad4fde2cc529897893012f1d0466fcc1645f8530572efc04fd/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/ecd1090feb2cefa189f30901fc838f310b18873555d3760d9aadfd495d0aa83d/","title":"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models","text":"<p>Arxiv Link - 2024-05-01 15:37:11 </p>"},{"location":"sections/papers/-8377067242701614157/ecd1090feb2cefa189f30901fc838f310b18873555d3760d9aadfd495d0aa83d/#abstract","title":"Abstract","text":"<p>As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive. </p>"},{"location":"sections/papers/-8377067242701614157/ecd1090feb2cefa189f30901fc838f310b18873555d3760d9aadfd495d0aa83d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in Large Language Models (LLMs)! A recent study delves into evaluating LLM quality with a fresh approach - using a Panel of LLM evaluators (PoLL). The research suggests that employing multiple smaller models in the evaluation process yields superior results compared to a single large model like GPT4, while also reducing costs significantly. Learn more about this innovative methodology here: http://arxiv.org/abs/2404.18796v2 #AI #NLP #LLMs #TechInnovation \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udcc8 \ud83d\ude80 New study on evaluating the quality of Large Language Models (LLMs) reveals a more efficient method - Panel of LLM evaluators (PoLL). Using multiple smaller models outperforms a single large judge, reduces bias, and is over seven times less expensive. Check out the research at: http://arxiv.org/abs/2404.18796v2 #AI #NLP #LLM #Research #TechInnovation"},{"location":"sections/papers/-8377067242701614157/ecd1090feb2cefa189f30901fc838f310b18873555d3760d9aadfd495d0aa83d/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8377067242701614157/ef6d813e2c26d16a046f50650c4514ae0924c549b49b4a1677503d0ff844be92/","title":"On the Limitations of Fine-tuned Judge Models for LLM Evaluation","text":"<p>Arxiv Link - 2024-06-17 12:10:34 </p>"},{"location":"sections/papers/-8377067242701614157/ef6d813e2c26d16a046f50650c4514ae0924c549b49b4a1677503d0ff844be92/#abstract","title":"Abstract","text":"<p>Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT-4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this study, we conduct an empirical study of judge models. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness, aspect-specific evaluation, and scalability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations. Finally, we propose an effective indicator to measure the reliability of fine-tuned judges, with the aim of maximizing their utility in LLM evaluation. </p>"},{"location":"sections/papers/-8377067242701614157/ef6d813e2c26d16a046f50650c4514ae0924c549b49b4a1677503d0ff844be92/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Just in: A new study delves into the evaluation of Large Language Models (LLMs) using fine-tuned judge models. Findings reveal key insights on performance, generalizability, fairness, and scalability in comparison to using GPT-4 as the evaluator. Dive deeper into the research here: Read more #LLM #AI #NLP #Research #GPT4 #TechInnovation \ud83d\ude80 New research alert! Discover the latest insights on utilizing Large Language Models for evaluation and fine-tuning judge models in AI. Find out more about the study's findings and proposed indicators for maximizing utility in LLM evaluation: http://arxiv.org/abs/2403.02839v2 #AI #LLM #NLP #Research #TechInnovation"},{"location":"sections/papers/-8377067242701614157/ef6d813e2c26d16a046f50650c4514ae0924c549b49b4a1677503d0ff844be92/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/1b2b11a90753f096660e31ed0f2544b4479500627a8415268d94e33f3915e85b/","title":"Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate","text":"<p>Arxiv Link - 2024-01-30 07:03:32 </p>"},{"location":"sections/papers/-8732978400105165891/1b2b11a90753f096660e31ed0f2544b4479500627a8415268d94e33f3915e85b/#abstract","title":"Abstract","text":"<p>Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \\url{https://github.com/GAIR-NLP/scaleeval}. </p>"},{"location":"sections/papers/-8732978400105165891/1b2b11a90753f096660e31ed0f2544b4479500627a8415268d94e33f3915e85b/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! Evaluating LLMs across diverse tasks and scenarios is crucial yet challenging. To address this, we introduce ScaleEval, a cutting-edge meta-evaluation framework that employs agent-debate assistance to assess the effectiveness of LLMs as evaluators efficiently and reliably. Our framework facilitates multi-round discussions, aiding human annotators in identifying the most adept LLM evaluators.Curious to learn more about ScaleEval and how it enhances the evaluation of LLMs? Dive into the details and access our framework's code at: http://arxiv.org/abs/2401.16788v1#AI #NLP #LLMs #TechInnovation #ScaleEval \ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! Check out ScaleEval, a cutting-edge meta-evaluation framework for assessing LLM performance across diverse tasks and scenarios. Developed by experts, this framework leverages multiple communicative LLM agents to streamline evaluation processes. Dive into the details and access the code here: http://arxiv.org/abs/2401.16788v1 #AI #NLP #LLMs #ScaleEval"},{"location":"sections/papers/-8732978400105165891/1b2b11a90753f096660e31ed0f2544b4479500627a8415268d94e33f3915e85b/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/28d24a1dcc1e849173edb68238fd4ce42ee6bdedf6789b8d43d038142d42c5f1/","title":"How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation","text":"<p>Arxiv Link - 2024-06-15 14:08:30 </p>"},{"location":"sections/papers/-8732978400105165891/28d24a1dcc1e849173edb68238fd4ce42ee6bdedf6789b8d43d038142d42c5f1/#abstract","title":"Abstract","text":"<p>In recent years, AI has demonstrated remarkable capabilities in simulating human behaviors, particularly those implemented with large language models (LLMs). However, due to the lack of systematic evaluation of LLMs' simulated behaviors, the believability of LLMs among humans remains ambiguous, i.e., it is unclear which behaviors of LLMs are convincingly human-like and which need further improvements. In this work, we design SimulateBench to evaluate the believability of LLMs when simulating human behaviors. In specific, we evaluate the believability of LLMs based on two critical dimensions: 1) consistency: the extent to which LLMs can behave consistently with the given information of a human to simulate; and 2) robustness: the ability of LLMs' simulated behaviors to remain robust when faced with perturbations. SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs' simulated behaviors. Based on SimulateBench, we evaluate the performances of 10 widely used LLMs when simulating characters. The experimental results reveal that current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations in certain factors. </p>"},{"location":"sections/papers/-8732978400105165891/28d24a1dcc1e849173edb68238fd4ce42ee6bdedf6789b8d43d038142d42c5f1/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the world of AI and Large Language Models (LLMs) are continuously pushing boundaries, but what about the believability of their simulated behaviors?A recent study introduces SimulateBench, a novel framework evaluating LLMs' believability in simulating human behaviors. This evaluation is based on two crucial dimensions: consistency and robustness. By assessing 10 popular LLMs using 65 character profiles and 8,400 questions, the results shed light on the current challenges faced by LLMs in aligning behaviors with assigned characters and their susceptibility to perturbations.Curious to dive deeper into the findings? Check out the full study at: http://arxiv.org/abs/2312.17115v2#AI #NLP #LLMs #ArtificialIntelligence #TechResearch #Innovation #SimulateBench #BelievabilityEvaluation #TechStudy \ud83e\udd16\ud83d\udcca Exciting insights on evaluating believability of Language Models in simulating human behaviors! Check out how SimulateBench assesses LLMs based on consistency and robustness. Discover the challenges and results of 10 popular LLMs in character simulation here: http://arxiv.org/abs/2312.17115v2 #AI #NLP #LLMs #SimulateBench"},{"location":"sections/papers/-8732978400105165891/28d24a1dcc1e849173edb68238fd4ce42ee6bdedf6789b8d43d038142d42c5f1/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/3baee9d2fe77db6122aeb2d0f72b0b51f6393cde2676ea079c23ff2f1ba25539/","title":"ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models","text":"<p>Arxiv Link - 2023-08-28 06:56:44 </p>"},{"location":"sections/papers/-8732978400105165891/3baee9d2fe77db6122aeb2d0f72b0b51f6393cde2676ea079c23ff2f1ba25539/#abstract","title":"Abstract","text":"<p>The unprecedented performance of large language models (LLMs) requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the ZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks. Especially, we also propose a new benchmark that focuses on knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration: We use 3 different yet complementary evaluation methods to comprehensively evaluate LLMs, which can ensure the authority and accuracy of the evaluation results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering benchmark that fully assesses LLMs in Chinese, while also providing equally robust evaluation abilities in English. (4) Avoiding potential data leakage: To avoid data leakage, we construct evaluation data specifically for 37 tasks. We evaluate 10 current mainstream LLMs and conduct an in-depth discussion and analysis of their results. The ZhuJiu benchmark and open-participation leaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we also provide a demo video at https://youtu.be/qypkJ89L1Ic. </p>"},{"location":"sections/papers/-8732978400105165891/3baee9d2fe77db6122aeb2d0f72b0b51f6393cde2676ea079c23ff2f1ba25539/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of AI evaluation! Check out the ZhuJiu benchmark, a groundbreaking initiative for comprehensive and accurate evaluation of large language models (LLMs). This benchmark covers 51 tasks across 7 ability dimensions, with a focus on knowledge ability. Using 3 evaluation methods, ZhuJiu ensures thorough and precise assessment results. What makes ZhuJiu stand out? It's the first benchmark to fully evaluate LLMs in Chinese, alongside robust evaluation in English, while also addressing potential data leakage concerns. Want to dive deeper into the results and analysis? Explore the ZhuJiu benchmark and leaderboard at http://www.zhujiu-benchmark.com/ and watch the demo video at https://youtu.be/qypkJ89L1Ic. For more details, check out the research paper at http://arxiv.org/abs/2308.14353v1. \ud83d\udcca\ud83d\udca1 #AI #LLMs #ZhuJiuBenchmark #TechInnovation \ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! Check out the ZhuJiu benchmark - a comprehensive evaluation method covering 51 tasks across 7 dimensions, including a focus on knowledge ability. Dive into the details and results of evaluating 10 mainstream LLMs here: http://arxiv.org/abs/2308.14353v1 #AI #NLP #LLMs #ZhuJiuBenchmark \ud83d\udcca\ud83d\udd0d"},{"location":"sections/papers/-8732978400105165891/3baee9d2fe77db6122aeb2d0f72b0b51f6393cde2676ea079c23ff2f1ba25539/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/4f2938c0796c1cb7c6deb232cf489bd37c86fec5b6949321d1f09450eab2ba6a/","title":"tinyBenchmarks: evaluating LLMs with fewer examples","text":"<p>Arxiv Link - 2024-05-26 22:27:23 </p>"},{"location":"sections/papers/-8732978400105165891/4f2938c0796c1cb7c6deb232cf489bd37c86fec5b6949321d1f09450eab2ba6a/#abstract","title":"Abstract","text":"<p>The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results. </p>"},{"location":"sections/papers/-8732978400105165891/4f2938c0796c1cb7c6deb232cf489bd37c86fec5b6949321d1f09450eab2ba6a/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of AI and NLP! A recent study explores strategies to streamline the evaluation of large language models (LLMs) on various benchmarks. By reducing the number of evaluations required, assessing LLM performance becomes more efficient and cost-effective.One striking finding from the research is that evaluating an LLM on just 100 carefully selected examples can accurately estimate its performance on a benchmark with tens of thousands of examples. The study also introduces evaluation tools and compact versions of popular benchmarks like MMLU and HELM.For further insights into this groundbreaking research, check out the full paper at: http://arxiv.org/abs/2402.14992v2#AI #NLP #LLMs #Research #Tech #Innovation \ud83d\ude80 Exciting research on optimizing evaluation of Large Language Models (LLMs)! Find out how to assess LLM performance on key benchmarks more efficiently. Check out the paper here: http://arxiv.org/abs/2402.14992v2 #AI #NLP #LLMs #TechResearch"},{"location":"sections/papers/-8732978400105165891/4f2938c0796c1cb7c6deb232cf489bd37c86fec5b6949321d1f09450eab2ba6a/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/5ae61d4f0c8bb762479a22d3fb4c1155077d5b3ae5d1affb4f5fc8397ccc44ee/","title":"MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition","text":"<p>Arxiv Link - 2024-03-03 02:23:19 </p>"},{"location":"sections/papers/-8732978400105165891/5ae61d4f0c8bb762479a22d3fb4c1155077d5b3ae5d1affb4f5fc8397ccc44ee/#abstract","title":"Abstract","text":"<p>Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' performance objectively and scientifically; 2) LLMs only get a small percentage of the right reasoning chain, e.g. GPT-4 only gets 36.3\\% right reasoning chain. We believe this new Multi-hop QA evaluation benchmark and novel evaluation methods will facilitate the development of trustworthy LLM evaluation on the MHQA task. </p>"},{"location":"sections/papers/-8732978400105165891/5ae61d4f0c8bb762479a22d3fb4c1155077d5b3ae5d1affb4f5fc8397ccc44ee/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of Large Language Models (LLMs) and Multi-hop Question Answering (MHQA) tasks! \ud83c\udf1fA recent study has shed light on the limitations of current LLM QA evaluation benchmarks, highlighting issues such as data contamination and the lack of evaluation of reasoning chains. To address these shortcomings, a new LLM MHQA evaluation benchmark has been introduced, based on the HotpotQA dataset. This benchmark includes annotated reasoning chains in the form of sub-questions and intermediate answers, providing a more comprehensive evaluation of LLM performance.Key findings from the study include a performance gap between the original HotpotQA dataset and the new benchmark, as well as LLMs achieving only a small percentage of correct reasoning chains. For example, GPT-4 scored 36.3% in this aspect.For more details on this groundbreaking research and its implications for the development of trustworthy LLM evaluation in MHQA tasks, check out the full paper here: http://arxiv.org/abs/2402.11924v2#LLM #MHQA #AI #NLP #Research #Tech #InnovationLet's continue pushing the boundaries of AI technology together! \ud83d\ude80\ud83d\udca1\ud83d\udd0d \ud83d\ude80 Exciting news in the world of Large Language Models (LLMs) and Multi-hop Question Answering (MHQA)! A new evaluation benchmark has been introduced to address current limitations and enhance the assessment of LLM reasoning abilities. Check out the details in the research paper here: http://arxiv.org/abs/2402.11924v2 #AI #NLP #LLMs #MHQA #TechResearch \ud83e\udd16\ud83d\udcda"},{"location":"sections/papers/-8732978400105165891/5ae61d4f0c8bb762479a22d3fb4c1155077d5b3ae5d1affb4f5fc8397ccc44ee/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/621f2ed5b623ffdbb85483c6bfe4b7d88320bad9abdb32aeffd9910562368257/","title":"LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond","text":"<p>Arxiv Link - 2023-05-23 21:50:06 </p>"},{"location":"sections/papers/-8732978400105165891/621f2ed5b623ffdbb85483c6bfe4b7d88320bad9abdb32aeffd9910562368257/#abstract","title":"Abstract","text":"<p>With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8\\% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur. </p>"},{"location":"sections/papers/-8732978400105165891/621f2ed5b623ffdbb85483c6bfe4b7d88320bad9abdb32aeffd9910562368257/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in the field of Language Models! \ud83e\udd16\ud83d\udcdaDetecting factual inconsistencies in AI-generated content is vital for combatting misinformation and enhancing trust in AI models. Recent research has shown that while some Large Language Models (LLMs) excel in identifying factual inconsistencies in standard benchmarks, they struggle with more complex tasks, showcasing the need for improved evaluation methods.Discover more about the cutting-edge research on factual inconsistency detection and the proposed SummEdits benchmark in the full article here: Check out the research paper!#AI #NLP #LLMs #Research #FactualInconsistencyDetection #TechInnovation \u2728 \ud83d\ude80 Just in: New research on detecting factual inconsistencies in LLMs! While some LLMs perform well on existing benchmarks, a new 10-domain benchmark called SummEdits reveals their limitations. Explore the study at: http://arxiv.org/abs/2305.14540v1 #AI #NLP #LLMs #TechResearch \ud83e\udd16\ud83d\udd0d"},{"location":"sections/papers/-8732978400105165891/621f2ed5b623ffdbb85483c6bfe4b7d88320bad9abdb32aeffd9910562368257/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/a70fcfada9bb004b4006decfa221c19975eedd91dd6b43a28f2a0c762b704a09/","title":"PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain","text":"<p>Arxiv Link - 2023-10-22 02:20:38 </p>"},{"location":"sections/papers/-8732978400105165891/a70fcfada9bb004b4006decfa221c19975eedd91dd6b43a28f2a0c762b704a09/#abstract","title":"Abstract","text":"<p>Biomedical language understanding benchmarks are the driving forces for artificial intelligence applications with large language model (LLM) back-ends. However, most current benchmarks: (a) are limited to English which makes it challenging to replicate many of the successes in English for other languages, or (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs apply these knowledge to perform on a wide range of bio-medical tasks, or (c) have become a publicly available corpus and are leaked to LLMs during pre-training. To facilitate the research in medical LLMs, we re-build the Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a large scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable test-bed and an online platform for evaluating Chinese LLMs' multi-task capabilities on a wide range bio-medical tasks including medical entity recognition, medical text classification, medical natural language inference, medical dialogue understanding and medical content/dialogue generation. To establish evaluation on these tasks, we have experimented and report the results with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning techniques. </p>"},{"location":"sections/papers/-8732978400105165891/a70fcfada9bb004b4006decfa221c19975eedd91dd6b43a28f2a0c762b704a09/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in the World of AI and NLP! \ud83c\udf1fBiomedical language understanding benchmarks play a crucial role in advancing artificial intelligence applications with large language model (LLM) back-ends. However, existing benchmarks face limitations such as being restricted to English, focusing solely on knowledge probing of LLMs, or being leaked to LLMs during pre-training.To address these challenges and drive innovation in medical LLMs, we introduce PromptCBLUE - a revamped version of the Chinese Biomedical Language Understanding Evaluation benchmark. PromptCBLUE serves as a comprehensive platform for evaluating Chinese LLMs' multi-task capabilities across various bio-medical tasks.Curious about the results of our experiments with 9 Chinese LLMs fine-tuned using different techniques? Dive into our research paper to learn more: http://arxiv.org/abs/2310.14151v1Let's continue pushing the boundaries of AI and NLP together! \ud83c\udf10\ud83d\udca1#AI #NLP #LLM #MedicalAI #Research #Innovation \ud83d\ude80 Exciting news in the world of AI and NLP! Explore how the new PromptCBLUE benchmark is revolutionizing Chinese biomedical language understanding evaluation for LLMs. Dive into the research and results here: http://arxiv.org/abs/2310.14151v1 #AI #NLP #LLMs #MedTech #Research #TechInnovation \ud83e\uddec\ud83d\udd0d\ud83e\udd16"},{"location":"sections/papers/-8732978400105165891/a70fcfada9bb004b4006decfa221c19975eedd91dd6b43a28f2a0c762b704a09/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/ae8e78e8f62414c5bc1398e6e7fa6d7ed5e92b5657940b40bde091648cf6a04e/","title":"OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety","text":"<p>Arxiv Link - 2024-03-18 23:21:37 </p>"},{"location":"sections/papers/-8732978400105165891/ae8e78e8f62414c5bc1398e6e7fa6d7ed5e92b5657940b40bde091648cf6a04e/#abstract","title":"Abstract","text":"<p>The rapid development of Chinese large language models (LLMs) poses big challenges for efficient LLM evaluation. While current initiatives have introduced new benchmarks or evaluation platforms for assessing Chinese LLMs, many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues. To address this gap, we introduce OpenEval, an evaluation testbed that benchmarks Chinese LLMs across capability, alignment and safety. For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning. For alignment assessment, OpenEval contains 7 datasets that examines the bias, offensiveness and illegalness in the outputs yielded by Chinese LLMs. To evaluate safety, especially anticipated risks (e.g., power-seeking, self-awareness) of advanced LLMs, we include 6 datasets. In addition to these benchmarks, we have implemented a phased public evaluation and benchmark update strategy to ensure that OpenEval is in line with the development of Chinese LLMs or even able to provide cutting-edge benchmark datasets to guide the development of Chinese LLMs. In our first public evaluation, we have tested a range of Chinese LLMs, spanning from 7B to 72B parameters, including both open-source and proprietary models. Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety. </p>"},{"location":"sections/papers/-8732978400105165891/ae8e78e8f62414c5bc1398e6e7fa6d7ed5e92b5657940b40bde091648cf6a04e/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in the World of Chinese Large Language Models (LLMs) \ud83d\ude80The fast-paced advancement of Chinese Large Language Models (LLMs) has brought about new challenges in their evaluation process. While existing benchmarks primarily focus on capabilities, the critical aspects of alignment and safety often get overlooked. To bridge this gap, we are thrilled to introduce OpenEval, a comprehensive evaluation testbed designed to assess Chinese LLMs across capability, alignment, and safety.\ud83d\udd0d Capability Assessment:- 12 benchmark datasets covering NLP tasks, disciplinary knowledge, commonsense reasoning, and mathematical reasoning.\ud83d\udccf Alignment Assessment:- 7 datasets examining bias, offensiveness, and illegalness in the outputs of Chinese LLMs.\ud83d\udee1\ufe0f Safety Evaluation:- 6 datasets focusing on anticipated risks like power-seeking and self-awareness in advanced LLMs.Our phased public evaluation and benchmark update strategy ensures that OpenEval stays aligned with the evolving landscape of Chinese LLMs, offering cutting-edge datasets for their development.In our inaugural public evaluation, we tested various Chinese LLMs ranging from 7B to 72B parameters, including open-source and proprietary models. While these LLMs displayed remarkable performance in specific tasks, there is a clear need to shift focus towards broader aspects such as commonsense reasoning, alignment, and safety.For more details, check out the research paper at: http://arxiv.org/abs/2403.12316v1#LLMs #ChineseLLMs #EvaluationTestbed #AIResearch #TechInnovation #OpenEval #NLP #SafetyEvaluation #AlignmentAssessment \ud83d\ude80 Exciting news in the world of Chinese large language models (LLMs)! Introducing OpenEval, a comprehensive evaluation testbed for Chinese LLMs covering capability, alignment, and safety. Learn more about the benchmarks and evaluation results in the latest research paper: http://arxiv.org/abs/2403.12316v1 #AI #NLP #LLMs #OpenEval \ud83e\udde0\ud83c\udde8\ud83c\uddf3\ud83d\udd0d"},{"location":"sections/papers/-8732978400105165891/ae8e78e8f62414c5bc1398e6e7fa6d7ed5e92b5657940b40bde091648cf6a04e/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/c617cda5156276c2c0de16e3276b09b3d3e6652cda052a4c704a2411fce87e9d/","title":"A User-Centric Benchmark for Evaluating Large Language Models","text":"<p>Arxiv Link - 2024-04-23 01:58:24 </p>"},{"location":"sections/papers/-8732978400105165891/c617cda5156276c2c0de16e3276b09b3d3e6652cda052a4c704a2411fce87e9d/#abstract","title":"Abstract","text":"<p>Large Language Models (LLMs) are essential tools to collaborate with users on different tasks. Evaluating their performance to serve users' needs in real-world scenarios is important. While many benchmarks have been created, they mainly focus on specific predefined model abilities. Few have covered the intended utilization of LLMs by real users. To address this oversight, we propose benchmarking LLMs from a user perspective in both dataset construction and evaluation designs. We first collect 1846 real-world use cases with 15 LLMs from a user study with 712 participants from 23 countries. These self-reported cases form the User Reported Scenarios(URS) dataset with a categorization of 7 user intents. Secondly, on this authentic multi-cultural dataset, we benchmark 10 LLM services on their efficacy in satisfying user needs. Thirdly, we show that our benchmark scores align well with user-reported experience in LLM interactions across diverse intents, both of which emphasize the overlook of subjective scenarios. In conclusion, our study proposes to benchmark LLMs from a user-centric perspective, aiming to facilitate evaluations that better reflect real user needs. The benchmark dataset and code are available at https://github.com/Alice1998/URS. </p>"},{"location":"sections/papers/-8732978400105165891/c617cda5156276c2c0de16e3276b09b3d3e6652cda052a4c704a2411fce87e9d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in AI and NLP! \ud83d\ude80When it comes to evaluating Large Language Models (LLMs), understanding real user needs and experiences is key. A recent study has proposed a groundbreaking approach to benchmarking LLMs from a user-centric perspective. By collecting 1846 real-world use cases from 712 participants across 23 countries, the User Reported Scenarios (URS) dataset was created to categorize user intents. This dataset was then used to benchmark 10 LLM services on their effectiveness in meeting user needs, with results aligning closely with user-reported experiences. This innovative approach sheds light on the importance of considering subjective scenarios in evaluating LLMs and aims to better reflect real user needs. If you're interested in learning more or accessing the benchmark dataset and code, check out the study at: http://arxiv.org/abs/2404.13940v2 #AI #NLP #LLMs #UserExperience #Benchmarking #TechInnovation \ud83d\ude80 Exciting new research on Large Language Models (LLMs) evaluation! A user-centric benchmarking approach was taken to assess 10 LLM services using 1846 real-world use cases from 712 participants across 23 countries. Results show alignment between benchmark scores and user-reported experiences. Check out the study here: http://arxiv.org/abs/2404.13940v2 #AI #NLP #LLMs #TechResearch"},{"location":"sections/papers/-8732978400105165891/c617cda5156276c2c0de16e3276b09b3d3e6652cda052a4c704a2411fce87e9d/#pdf","title":"PDF","text":""},{"location":"sections/papers/-8732978400105165891/e7ac09c228c26db37659b7317ef439f859f7da03abcd73bcc1465d121172e618/","title":"State of What Art? A Call for Multi-Prompt LLM Evaluation","text":"<p>Arxiv Link - 2024-05-06 10:20:26 </p>"},{"location":"sections/papers/-8732978400105165891/e7ac09c228c26db37659b7317ef439f859f7da03abcd73bcc1465d121172e618/#abstract","title":"Abstract","text":"<p>Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs. </p>"},{"location":"sections/papers/-8732978400105165891/e7ac09c228c26db37659b7317ef439f859f7da03abcd73bcc1465d121172e618/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the field of large language models (LLMs)! A recent study delves into the brittleness of single-prompt evaluations and proposes a more robust approach using diverse prompts. With analysis across 6.5M instances and 20 LLMs, this research offers tailored evaluation metrics for different user cases. Check out the full paper for insights into enhancing the assessment of LLM capabilities: http://arxiv.org/abs/2401.00595v3 #AI #NLP #LLMs #TechResearch \ud83d\udcca\ud83d\udd0d \ud83d\ude80 Exciting new research on large language models (LLMs) evaluation benchmarks! This study analyzes the brittleness of single-prompt evaluations across 6.5M instances with 20 LLMs and 39 tasks. Explore how diverse prompts can lead to more robust assessments of LLM capabilities. Check out the full paper here: http://arxiv.org/abs/2401.00595v3 #AI #NLP #LLMs #Research #Tech \ud83d\udd0d\ud83d\udcca"},{"location":"sections/papers/-8732978400105165891/e7ac09c228c26db37659b7317ef439f859f7da03abcd73bcc1465d121172e618/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/0506d5fc2b418d0ae02fd47db31d957a14c15b84dbaf545a1d2536a4da0d47c4/","title":"ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs","text":"<p>Arxiv Link - 2024-02-19 01:28:48 </p>"},{"location":"sections/papers/7687185086521095419/0506d5fc2b418d0ae02fd47db31d957a14c15b84dbaf545a1d2536a4da0d47c4/#abstract","title":"Abstract","text":"<p>Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost. </p>"},{"location":"sections/papers/7687185086521095419/0506d5fc2b418d0ae02fd47db31d957a14c15b84dbaf545a1d2536a4da0d47c4/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting new research alert! \ud83d\ude80Debiasing Large Language models (LLMs) just got a major boost with a novel approach using ChatGPT to generate synthetic training data. This innovative method aims to enhance debiasing while maintaining multi-task language capabilities. Learn more about this cutting-edge research on leveraging synthetic data for LLM debiasing and its impressive results here: http://arxiv.org/abs/2402.11764v1#AI #NLP #LLMs #Debiasing #Research #TechInnovation \ud83d\ude80 Exciting new research on debiasing Large Language Models (LLMs) using ChatGPT synthetic data! Learn how this innovative approach enhances debiasing while preserving multi-task capabilities. Find out more at: http://arxiv.org/abs/2402.11764v1 #AI #NLP #LLMs #Debiasing #ChatGPT"},{"location":"sections/papers/7687185086521095419/0506d5fc2b418d0ae02fd47db31d957a14c15b84dbaf545a1d2536a4da0d47c4/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/","title":"Synthetic Test Collections for Retrieval Evaluation","text":"<p>Arxiv Link - 2024-05-13 14:11:09 </p>"},{"location":"sections/papers/7687185086521095419/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#abstract","title":"Abstract","text":"<p>Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. </p>"},{"location":"sections/papers/7687185086521095419/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in Information Retrieval evaluation using Large Language Models (LLMs)!Test collections are crucial for evaluating IR systems, but acquiring diverse user queries and relevance judgments can be challenging. Recent studies have shown the potential of LLMs in generating synthetic datasets for various applications, including IR.Check out this comprehensive investigation on constructing fully synthetic test collections using LLMs. The research explores generating synthetic queries and relevance judgments, demonstrating the feasibility of using LLMs for reliable retrieval evaluation.Read the full paper here: http://arxiv.org/abs/2405.07767v1#InformationRetrieval #LLMs #AI #TechResearch #ArtificialIntelligence #NLP #TechInnovation \ud83c\udf1f \ud83d\ude80 Exciting research alert! Can Large Language Models (LLMs) revolutionize the construction of test collections for information retrieval systems? Find out in this comprehensive study on using LLMs to generate synthetic test collections and relevance judgments: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLM #InformationRetrieval"},{"location":"sections/papers/7687185086521095419/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/0cae8c50bbf22ac6458d849ff0838a346db2433301d97f0d064956c433eb81dc/","title":"Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data","text":"<p>Arxiv Link - 2024-05-23 06:14:35 </p>"},{"location":"sections/papers/7687185086521095419/0cae8c50bbf22ac6458d849ff0838a346db2433301d97f0d064956c433eb81dc/#abstract","title":"Abstract","text":"<p>As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations. For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention. By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs. However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks. In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy. The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy. Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs. The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data. </p>"},{"location":"sections/papers/7687185086521095419/0cae8c50bbf22ac6458d849ff0838a346db2433301d97f0d064956c433eb81dc/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in AI and privacy protection! \ud83d\udee1\ufe0f Our latest research introduces the Federated Domain-specific Knowledge Transfer (FDKT) framework, enabling secure knowledge transfer from large language models (LLMs) to small language models (SLMs) while safeguarding clients' data privacy. By leveraging LLMs to augment data based on domain-specific few-shot demonstrations synthesized from private domain data using differential privacy, FDKT significantly boosts SLMs' task performance by approximately 5% with a privacy budget of less than 10. Learn more about this cutting-edge approach here: http://arxiv.org/abs/2405.14212v1 #AI #PrivacyProtection #FDKT #LLMs #SLMs #Innovation \ud83c\udf1f \ud83d\ude80 Exciting new research on Federated Domain-specific Knowledge Transfer (FDKT) framework for enhancing small language models (SLMs) using large language models (LLMs) while ensuring data privacy. The FDKT framework yields significant performance improvements in domain-specific tasks. Check out the full paper here: http://arxiv.org/abs/2405.14212v1 #AI #NLP #LLMs #PrivacyPreservation"},{"location":"sections/papers/7687185086521095419/0cae8c50bbf22ac6458d849ff0838a346db2433301d97f0d064956c433eb81dc/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/","title":"Differentially Private Synthetic Data via Foundation Model APIs 2: Text","text":"<p>Arxiv Link - 2024-03-04 05:57:50 </p>"},{"location":"sections/papers/7687185086521095419/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/#abstract","title":"Abstract","text":"<p>Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe. </p>"},{"location":"sections/papers/7687185086521095419/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/#socials","title":"Socials","text":"LinkedIn X \ud83c\udf1f Exciting News in the World of AI and Privacy-Preserving Technologies! \ud83c\udf1fText data privacy is a crucial concern in the age of machine learning. Check out the groundbreaking work by Lin et al. introducing the Aug-PE algorithm, designed to generate differentially private (DP) synthetic text data using API access to large language models (LLMs) without the need for model training.Curious to learn more about how Aug-PE can revolutionize privacy-preserving LLM applications? Dive into the details and explore the impressive results from comprehensive experiments on benchmark datasets. The study showcases that Aug-PE produces high-quality DP synthetic text comparable to state-of-the-art DP finetuning baselines.Ready to explore the future of privacy-preserving AI applications? Access the code and data at: https://github.com/AI-secure/aug-pe#AI #PrivacyPreservation #MachineLearning #DifferentialPrivacy #AugmentedPE #TechInnovation \ud83d\ude80 Exciting new research alert! Aug-PE algorithm enables generating high-quality differentially private synthetic text without model training. Check out the groundbreaking study by Lin et al. (2024) and explore the code and data at: \ud83d\udd17 http://arxiv.org/abs/2403.01749v1#AI #NLP #LLMs #PrivacyPreservation #MachineLearning"},{"location":"sections/papers/7687185086521095419/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/395d9979720c3ff49e0dd480e6111855e74b76bf614421992f08252d2758bf35/","title":"Synthetic Test Collections for Retrieval Evaluation","text":"<p>Arxiv Link - 2024-05-13 14:11:09 </p>"},{"location":"sections/papers/7687185086521095419/395d9979720c3ff49e0dd480e6111855e74b76bf614421992f08252d2758bf35/#abstract","title":"Abstract","text":"<p>Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. </p>"},{"location":"sections/papers/7687185086521095419/395d9979720c3ff49e0dd480e6111855e74b76bf614421992f08252d2758bf35/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting development in the field of Information Retrieval! \ud83c\udf10Constructing test collections for evaluating Information Retrieval (IR) systems can be challenging and resource-intensive. However, a recent study has shown promising results in using Large Language Models (LLMs) to generate fully synthetic test collections, including queries and relevance judgments.Check out the research paper to learn more about how LLMs can be leveraged to construct synthetic test collections for IR evaluation: http://arxiv.org/abs/2405.07767v1#AI #NLP #LLMs #InformationRetrieval #TechResearch #InnovationLet's continue pushing the boundaries of what is possible in the world of technology! \ud83c\udf1f \ud83d\ude80 Exciting research alert! Can Large Language Models (LLMs) be used to construct synthetic test collections for information retrieval systems? Find out more in this comprehensive investigation: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLMs #TechResearch #InformationRetrieval \ud83e\udd16\ud83d\udcda"},{"location":"sections/papers/7687185086521095419/395d9979720c3ff49e0dd480e6111855e74b76bf614421992f08252d2758bf35/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","text":"<p>Arxiv Link - 2024-04-16 17:59:10 </p>"},{"location":"sections/papers/7687185086521095419/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/#abstract","title":"Abstract","text":"<p>Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models. </p>"},{"location":"sections/papers/7687185086521095419/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of NLP and LLMs! Researchers have developed a groundbreaking approach to fact-checking LLM outputs, significantly reducing computational costs while maintaining GPT-4-level performance. By training small models on synthetic data generated with GPT-4, they have successfully improved the efficiency of verifying facts in model generations. The newly introduced benchmark LLM-AggreFact, along with the MiniCheck-FT5 system, outperforms comparable models and achieves GPT-4 accuracy. Learn more about this innovative work at: http://arxiv.org/abs/2404.10774v1 #NLP #LLM #AI #FactChecking #Innovation \ud83d\udd0d\ud83d\udcca\ud83d\udd2c \ud83d\ude80 Exciting new research on fact-checking in NLP! Learn how small models with GPT-4-level performance are built at 400x lower cost. Check out MiniCheck-FT5, outperforming others of comparable size. Find out more at: http://arxiv.org/abs/2404.10774v1 #AI #NLP #LLM #research #factchecking"},{"location":"sections/papers/7687185086521095419/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/","title":"Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks","text":"<p>Arxiv Link - 2023-06-13 16:46:24 </p>"},{"location":"sections/papers/7687185086521095419/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/#abstract","title":"Abstract","text":"<p>Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk </p>"},{"location":"sections/papers/7687185086521095419/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting insights into the impact of Large Language Models (LLMs) on crowdsourcing! \ud83e\udd16\ud83d\udcacA recent study delved into the prevalence of LLM usage by crowd workers in data annotation tasks. The findings showed that 33-46% of crowd workers leveraged LLMs to enhance their productivity and earnings. This raises important considerations for ensuring the integrity of human-generated data in the era of advanced AI technologies.For a detailed overview of the study and its implications, check out the full paper here: http://arxiv.org/abs/2306.07899v1#LLMs #Crowdsourcing #AI #DataAnnotation #TechResearch #ArtificialIntelligence #NLPCode and data from the study are available at: https://github.com/epfl-dlab/GPTurkLet's keep exploring the intersection of human intelligence and AI advancements! \ud83c\udf10\ud83d\udca1#TechInnovation #Research #DataScience #MachineLearning #SocialMediaExpert #LinkedInPost \ud83d\ude80 New research alert! Learn how Large Language Models impact crowd workers and human annotations in AI tasks. A case study found that 33-46% of workers used LLMs on Amazon Mechanical Turk. Check out the study here: http://arxiv.org/abs/2306.07899v1 #AI #LLMs #NLP #ResearchCode/data available at: https://github.com/epfl-dlab/GPTurk"},{"location":"sections/papers/7687185086521095419/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications","text":"<p>Arxiv Link - 2024-04-02 12:25:57 </p>"},{"location":"sections/papers/7687185086521095419/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/#abstract","title":"Abstract","text":"<p>Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable. </p>"},{"location":"sections/papers/7687185086521095419/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of Natural Language Processing! \ud83c\udf1fAre you interested in cutting-edge research on mitigating errors in NLP models during classification tasks? Check out this groundbreaking study that explores the use of large language models (LLMs) for data augmentation to address high confidence misclassifications. The research compares the effectiveness of synthetic data generated by LLMs versus human-provided data in reducing wrong predictions while maintaining accuracy levels. Results show that LLMs can significantly reduce high confidence misclassifications, offering a more scalable solution compared to human-provided data.Read more about this innovative approach and its implications for NLP models here: http://arxiv.org/abs/2403.17860v2#NLP #LLMs #DataAugmentation #AI #TechResearch #Innovation #ArtificialIntelligence #MachineLearning #TechTrends Exciting research on leveraging large language models for data augmentation in NLP to reduce high confidence misclassifications! \ud83d\ude80\ud83e\udd16 Check out the study comparing human-generated vs. LLM-generated synthetic data for classification tasks. Results show promising effectiveness and scalability. Read more at: http://arxiv.org/abs/2403.17860v2 #AI #NLP #LLM #DataAugmentation #Research"},{"location":"sections/papers/7687185086521095419/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/67e9a3baed5e4395e79f512ed4f675c0522d5ed7f2d1bc0e83f6946617293298/","title":"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations","text":"<p>Arxiv Link - 2023-10-13 01:31:59 </p>"},{"location":"sections/papers/7687185086521095419/67e9a3baed5e4395e79f512ed4f675c0522d5ed7f2d1bc0e83f6946617293298/#abstract","title":"Abstract","text":"<p>The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation. </p>"},{"location":"sections/papers/7687185086521095419/67e9a3baed5e4395e79f512ed4f675c0522d5ed7f2d1bc0e83f6946617293298/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting insights in AI and NLP research! \ud83e\udd16Curating high-quality training data is crucial for text classification models, but can be costly and time-consuming. Researchers are now exploring the use of Large Language Models (LLMs) to generate synthetic datasets as a cost-effective alternative. However, the effectiveness of LLM-generated synthetic data varies across different tasks.In a recent study, we delved into the impact of subjectivity on model performance when trained on synthetic data. Our findings reveal a negative association between subjectivity levels and model performance. This sheds light on the factors moderating the effectiveness of LLM-generated synthetic data.For a deep dive into our study and its implications on leveraging LLMs for synthetic data generation, check out the full article here: http://arxiv.org/abs/2310.07849v2#AI #NLP #LLMs #Research #Tech #TextClassification #DataScience Let's stay ahead in the world of AI and NLP together! \ud83c\udf1f\ud83d\udd0d\ud83d\udd2c \ud83d\ude80 New research alert! Discover how subjectivity impacts the effectiveness of large language models in generating synthetic data for text classification tasks. \ud83d\udcca Check out the study here: http://arxiv.org/abs/2310.07849v2 #AI #NLP #LLMs #TechResearch"},{"location":"sections/papers/7687185086521095419/67e9a3baed5e4395e79f512ed4f675c0522d5ed7f2d1bc0e83f6946617293298/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/76fdc885df31c6caa21f82525c3b3d2210531d7bd77feef288288e7295d234ee/","title":"Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks","text":"<p>Arxiv Link - 2023-06-13 16:46:24 </p>"},{"location":"sections/papers/7687185086521095419/76fdc885df31c6caa21f82525c3b3d2210531d7bd77feef288288e7295d234ee/#abstract","title":"Abstract","text":"<p>Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk </p>"},{"location":"sections/papers/7687185086521095419/76fdc885df31c6caa21f82525c3b3d2210531d7bd77feef288288e7295d234ee/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting findings in the world of AI and crowdsourcing! A recent study delves into the impact of Large Language Models (LLMs) on crowd workers. Research suggests that 33-46% of crowd workers used LLMs to complete tasks, raising important questions about the integrity of human-generated data. Check out the full study and methodology here: http://arxiv.org/abs/2306.07899v1#AI #NLP #LLMs #Crowdsourcing #Research #Tech #Innovation \ud83d\ude80 Investigating the impact of Large Language Models (LLMs) on crowd workers! A recent study found that 33-46% of crowd workers used LLMs for an abstract summarization task. How can we ensure human data remain human in the age of AI? Find out more: http://arxiv.org/abs/2306.07899v1 #AI #LLMs #Research #TechEthics"},{"location":"sections/papers/7687185086521095419/76fdc885df31c6caa21f82525c3b3d2210531d7bd77feef288288e7295d234ee/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/7ab0aedbde44a5f71fa90cf8e8f657700caf859cdbe2ede7b3c5c931bc3c49d9/","title":"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models","text":"<p>Arxiv Link - 2024-04-06 15:20:18 </p>"},{"location":"sections/papers/7687185086521095419/7ab0aedbde44a5f71fa90cf8e8f657700caf859cdbe2ede7b3c5c931bc3c49d9/#abstract","title":"Abstract","text":"<p>The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning. However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration. In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios. The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs. </p>"},{"location":"sections/papers/7687185086521095419/7ab0aedbde44a5f71fa90cf8e8f657700caf859cdbe2ede7b3c5c931bc3c49d9/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the field of Large Language Models (LLMs)! \ud83e\udde0\ud83d\udca1As LLMs advance in their capabilities for long-context understanding and reasoning, evaluating their performance accurately becomes increasingly challenging due to their ability to process text far beyond human assessment limits. In a recent paper, researchers propose a groundbreaking solution - S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs. By leveraging complex synthetic tasks, S3Eval allows for the systematic probing of LLM capabilities by adjusting text length and task difficulty. The correlation between S3Eval and real-world benchmarks showcases its reliability for evaluating LLMs.Curious to learn more? Dive into the details here: Read the full paper \ud83d\udcda #AI #NLP #LLMs #TechInnovation #ResearchPublication \ud83d\ude80 Exciting advancements in Large Language Models (LLMs)! A new evaluation method, S3Eval, offers a synthetic, scalable, systematic approach to assessing LLM capabilities. Learn more about this innovative evaluation suite and its impact on LLM development at: http://arxiv.org/abs/2310.15147v2 #AI #NLP #LLMs #TechInnovation \ud83e\udd16\ud83d\udcda"},{"location":"sections/papers/7687185086521095419/7ab0aedbde44a5f71fa90cf8e8f657700caf859cdbe2ede7b3c5c931bc3c49d9/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/","title":"Synthetic Test Collections for Retrieval Evaluation","text":"<p>Arxiv Link - 2024-05-13 14:11:09 </p>"},{"location":"sections/papers/7687185086521095419/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/#abstract","title":"Abstract","text":"<p>Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. </p>"},{"location":"sections/papers/7687185086521095419/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the world of Information Retrieval (IR) systems! A recent study delves into the potential of using Large Language Models (LLMs) to construct synthetic test collections for evaluation purposes. \ud83d\udd0d Generating diverse user queries and relevance judgments for test collections can be a challenge, but leveraging LLMs offers a promising solution. By creating synthetic queries and judgments, researchers are exploring the possibility of enhancing the evaluation process without the traditional resource-intensive methods.\ud83d\udcca The study thoroughly investigates the feasibility of using LLMs to construct fully synthetic test collections and evaluates the reliability of such collections. The results suggest that synthetic test collections generated using LLMs can indeed be reliable for evaluating IR systems.\ud83d\udd17 Dive deeper into the details of this innovative research at: Read more#ArtificialIntelligence #NLP #LLMs #InformationRetrieval #Research #TechInnovation \ud83d\ude80 Exciting research on using Large Language Models (LLMs) to construct synthetic test collections for Information Retrieval evaluation. Discover how LLMs can generate synthetic queries and relevance judgments efficiently! Check out the study here: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLMs #InformationRetrieval #TechResearch"},{"location":"sections/papers/7687185086521095419/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/8ed56cc0d9b21ca0436d41766433160218bdb454042088643fe04a91668ae8ef/","title":"Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal","text":"<p>Arxiv Link - 2024-05-25 12:17:29 </p>"},{"location":"sections/papers/7687185086521095419/8ed56cc0d9b21ca0436d41766433160218bdb454042088643fe04a91668ae8ef/#abstract","title":"Abstract","text":"<p>Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains. </p>"},{"location":"sections/papers/7687185086521095419/8ed56cc0d9b21ca0436d41766433160218bdb454042088643fe04a91668ae8ef/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting Breakthrough in AI Research! \ud83d\ude80Continual learning with Large Language Models (LLMs) just got a major boost! Conventional methods face challenges with catastrophic forgetting, but a new framework called Self-Synthesized Rehearsal (SSR) is changing the game.\ud83d\udd0d SSR leverages LLMs to generate synthetic instances for rehearsal, overcoming the need for previous training data. By refining instance outputs based on synthetic inputs, SSR maintains and even enhances the model's abilities. Experimental results show SSR outperforms traditional methods while being more data-efficient.Read more about this groundbreaking research at: http://arxiv.org/abs/2403.01244v2#AI #LLM #ContinualLearning #TechInnovation #ArtificialIntelligence #NLP \ud83d\ude80 New research in continual learning for Large Language Models (LLMs)! Introducing Self-Synthesized Rehearsal (SSR) framework that generates synthetic instances for rehearsal to address catastrophic forgetting. \ud83e\udde0\ud83d\udca1 Superior performance while being data-efficient! Check out the details at: http://arxiv.org/abs/2403.01244v2 #AI #NLP #LLMs #Research #Tech #Innovation \ud83e\udd16\ud83d\udcda"},{"location":"sections/papers/7687185086521095419/8ed56cc0d9b21ca0436d41766433160218bdb454042088643fe04a91668ae8ef/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/","title":"Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal","text":"<p>Arxiv Link - 2024-05-25 12:17:29 </p>"},{"location":"sections/papers/7687185086521095419/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/#abstract","title":"Abstract","text":"<p>Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains. </p>"},{"location":"sections/papers/7687185086521095419/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in AI Research \ud83d\ude80Large language models (LLMs) face a significant challenge in continual learning due to catastrophic forgetting. Conventional methods rely on previous training data for rehearsal, which may not be practical in real-world scenarios. Our latest research introduces a cutting-edge framework, Self-Synthesized Rehearsal (SSR), to tackle this issue.\ud83d\udd0d SSR leverages LLMs to generate synthetic instances for rehearsal, enabling continual learning without access to original training data. By utilizing the base LLM for in-context learning to create synthetic instances and refining them with the latest LLM, SSR maintains and enhances the model's acquired abilities. Moreover, diverse high-quality synthetic instances are selected for future rehearsal, ensuring data-efficient performance.\ud83d\udcca Experimental results showcase that SSR outperforms traditional rehearsal-based methods while preserving the generalization capabilities of LLMs in various domains.Learn more about our innovative SSR framework in our research paper: http://arxiv.org/abs/2403.01244v2#AI #LLM #ContinualLearning #Research #TechInnovation #NLP #SSR #ArtificialIntelligence #DataEfficiency \ud83d\ude80 Exciting research alert! Addressing catastrophic forgetting in Large Language Models (LLMs) during continual learning, a new framework called Self-Synthesized Rehearsal (SSR) has been proposed. SSR generates synthetic instances for rehearsal, achieving superior performance while being more data-efficient. Check out the results here: http://arxiv.org/abs/2403.01244v2 #AI #NLP #LLM #Research #TechInnovation \ud83e\udd16\ud83d\udcda"},{"location":"sections/papers/7687185086521095419/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/","title":"Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data","text":"<p>Arxiv Link - 2024-05-23 06:14:35 </p>"},{"location":"sections/papers/7687185086521095419/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/#abstract","title":"Abstract","text":"<p>As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations. For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention. By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs. However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks. In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy. The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy. Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs. The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data. </p>"},{"location":"sections/papers/7687185086521095419/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the field of AI and privacy protection! \ud83d\udee1\ufe0f Our latest research introduces the Federated Domain-specific Knowledge Transfer (FDKT) framework, enabling the transfer of domain-specific knowledge from large language models (LLMs) to small language models (SLMs) while safeguarding clients' data privacy. \ud83d\udcc8 The FDKT framework leverages LLMs to augment data based on domain-specific few-shot demonstrations synthesized from private domain data using differential privacy. This approach ensures that the synthetic samples share a similar data distribution with clients' private data, leading to significant improvements in SLMs' task performance by approximately 5% with a privacy budget of less than 10, compared to local training on private data.\ud83d\udd0d Dive deeper into the results and methodology by checking out the full research paper at: http://arxiv.org/abs/2405.14212v1#AI #PrivacyProtection #LLMs #FDKT #Research #DataPrivacy #TechInnovation \ud83d\ude80 Exciting research in enhancing domain-specific small language models (SLMs) using Federated Domain-specific Knowledge Transfer (FDKT) framework! \ud83e\udd16\ud83d\udcda Learn how to transfer knowledge from LLMs to SLMs while preserving data privacy: http://arxiv.org/abs/2405.14212v1 #AI #NLP #LLMs #DataPrivacy #Research #FDKT \ud83d\udcca\ud83d\udd12"},{"location":"sections/papers/7687185086521095419/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/","title":"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations","text":"<p>Arxiv Link - 2023-10-13 01:31:59 </p>"},{"location":"sections/papers/7687185086521095419/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/#abstract","title":"Abstract","text":"<p>The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation. </p>"},{"location":"sections/papers/7687185086521095419/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting new study alert! Researchers dive into the world of Large Language Models (LLMs) for synthetic data generation in text classification models. Check out the latest findings on the impact of subjectivity on model performance when using LLM-generated synthetic data. Curious to learn more? Dive into the full study here: http://arxiv.org/abs/2310.07849v2#AI #NLP #LLMs #TextClassification #Research #Tech #ArtificialIntelligence #MachineLearning \ud83d\ude80 New research alert! How effective are large language models in generating synthetic data for text classification models? Check out the findings on the impact of subjectivity on model performance: http://arxiv.org/abs/2310.07849v2 #AI #NLP #LLMs #DataSynthesis"},{"location":"sections/papers/7687185086521095419/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/","title":"ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs","text":"<p>Arxiv Link - 2024-02-19 01:28:48 </p>"},{"location":"sections/papers/7687185086521095419/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/#abstract","title":"Abstract","text":"<p>Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost. </p>"},{"location":"sections/papers/7687185086521095419/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in the world of AI and debiasing LLMs! \ud83e\udd16 This groundbreaking work introduces a novel approach using ChatGPT to generate synthetic training data, enhancing the debiasing of Large Language Models. Check out the full study here: http://arxiv.org/abs/2402.11764v1Key findings include:1\ufe0f\u20e3 Efficient production of high-quality training data for debiasing LLMs using ChatGPT.2\ufe0f\u20e3 Surpassing existing datasets in debiasing performance while preserving internal LLM knowledge.3\ufe0f\u20e3 Generalizability across categories, effectively mitigating various biases, including intersectional ones.These results highlight the potential of synthetic data in promoting fairness in LLMs with minimal retraining costs. A must-read for all tech enthusiasts and AI professionals! \ud83c\udf10\ud83d\udca1 #AI #LLMs #Debiasing #ChatGPT #TechInnovation \"Exciting research on using ChatGPT to enhance debiasing of Large Language Models (LLMs)! This innovative approach generates synthetic training data for efficient debiasing, surpassing existing datasets in performance. Learn more at: http://arxiv.org/abs/2402.11764v1 #AI #NLP #LLMs #Debiasing\""},{"location":"sections/papers/7687185086521095419/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/eca4f4d2c710dc7a6914f4a5bad442dbfa5291da3e9432a945b3429952cd743e/","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications","text":"<p>Arxiv Link - 2024-04-02 12:25:57 </p>"},{"location":"sections/papers/7687185086521095419/eca4f4d2c710dc7a6914f4a5bad442dbfa5291da3e9432a945b3429952cd743e/#abstract","title":"Abstract","text":"<p>Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable. </p>"},{"location":"sections/papers/7687185086521095419/eca4f4d2c710dc7a6914f4a5bad442dbfa5291da3e9432a945b3429952cd743e/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting new study alert! Researchers explore the use of Large Language Models (LLMs) for data augmentation to tackle high confidence errors in NLP models during classification tasks. Find out how synthetic data generated by LLMs compares to human data in mitigating misclassifications. Results show a significant reduction in high confidence errors while maintaining accuracy levels. Plus, LLMs prove to be a more cost-effective and scalable solution. Check out the full study here: http://arxiv.org/abs/2403.17860v2 #AI #NLP #LLMs #DataAugmentation #ResearchStudy #TechInnovation \ud83c\udf1f \ud83d\ude80 Exciting research alert! Can Large Language Models (#LLMs) help improve NLP model predictions? A recent study explores using LLMs for data augmentation to reduce high confidence misclassifications. Check out the results here: http://arxiv.org/abs/2403.17860v2 #AI #NLP #TechResearch \ud83e\udd16\ud83d\udcca"},{"location":"sections/papers/7687185086521095419/eca4f4d2c710dc7a6914f4a5bad442dbfa5291da3e9432a945b3429952cd743e/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/f3739ef851e87f09a5b90a88e25b6c39e4f68c2f1ee4a9c558d43bedcdc68b7f/","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","text":"<p>Arxiv Link - 2024-04-16 17:59:10 </p>"},{"location":"sections/papers/7687185086521095419/f3739ef851e87f09a5b90a88e25b6c39e4f68c2f1ee4a9c558d43bedcdc68b7f/#abstract","title":"Abstract","text":"<p>Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models. </p>"},{"location":"sections/papers/7687185086521095419/f3739ef851e87f09a5b90a88e25b6c39e4f68c2f1ee4a9c558d43bedcdc68b7f/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting breakthrough in NLP! Researchers have developed a cost-effective method to enhance fact-checking capabilities of language models like GPT-4. By training small models with synthetic data generated from GPT-4, they achieved GPT-4-level performance at 400x lower cost. The new benchmark LLM-AggreFact outperforms other systems in fact-checking and information synthesis. Learn more about this innovative approach at: http://arxiv.org/abs/2404.10774v1 #NLP #AI #LLM #FactChecking #Innovation \ud83d\udd0d\ud83d\udcca \ud83d\ude80 Exciting breakthrough in NLP research! Learn how MiniCheck-FT5, a small model with GPT-4-level performance at 400x lower cost, is revolutionizing fact-checking for LLMs. Check out the paper here: http://arxiv.org/abs/2404.10774v1 #AI #NLP #LLMs #TechInnovation"},{"location":"sections/papers/7687185086521095419/f3739ef851e87f09a5b90a88e25b6c39e4f68c2f1ee4a9c558d43bedcdc68b7f/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/","title":"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models","text":"<p>Arxiv Link - 2024-04-06 15:20:18 </p>"},{"location":"sections/papers/7687185086521095419/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/#abstract","title":"Abstract","text":"<p>The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning. However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration. In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios. The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs. </p>"},{"location":"sections/papers/7687185086521095419/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the field of Large Language Models (LLMs)! A recent paper introduces S3Eval, a Synthetic, Scalable, Systematic evaluation suite designed to assess LLM capabilities in processing long contexts. By utilizing complex synthetic tasks, S3Eval offers a flexible and controlled method to evaluate LLM performance across diverse scenarios. The correlation between S3Eval and real-world benchmarks showcases its effectiveness in gauging LLM capabilities. Experimental results with the S3Eval-Standard dataset have revealed significant challenges for existing LLMs, highlighting the potential of this evaluation method.Read more about S3Eval and its implications for LLM assessment here: http://arxiv.org/abs/2310.15147v2#AI #NLP #LLMs #Technology #Research #Innovation \ud83d\ude80 Exciting development in Large Language Models (LLMs) evaluation! Check out S3Eval, a Synthetic, Scalable, Systematic evaluation suite that challenges existing LLMs with complex synthetic tasks. Learn more at: http://arxiv.org/abs/2310.15147v2 #AI #NLP #LLMs #TechInnovation \ud83e\udde0\ud83d\udd0d"},{"location":"sections/papers/7687185086521095419/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/#pdf","title":"PDF","text":""},{"location":"sections/papers/7687185086521095419/f5dcb530892c4205aa99ce4227281fd4b1f09854f05c6bcebadbdcb19ba78827/","title":"Differentially Private Synthetic Data via Foundation Model APIs 2: Text","text":"<p>Arxiv Link - 2024-03-04 05:57:50 </p>"},{"location":"sections/papers/7687185086521095419/f5dcb530892c4205aa99ce4227281fd4b1f09854f05c6bcebadbdcb19ba78827/#abstract","title":"Abstract","text":"<p>Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe. </p>"},{"location":"sections/papers/7687185086521095419/f5dcb530892c4205aa99ce4227281fd4b1f09854f05c6bcebadbdcb19ba78827/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the realm of privacy-preserving text data generation! Researchers have introduced the Aug-PE algorithm, a novel approach that leverages API access to large language models (LLMs) to create differential privacy (DP) synthetic text without the need for model training. This breakthrough offers a scalable solution for generating high-quality synthetic text data while ensuring formal privacy guarantees.The study conducted by Lin et al. (2024) presents compelling results, showcasing the efficacy of Aug-PE in producing DP synthetic text that rivals state-of-the-art DP finetuning methods. By making use of API access to LLMs, this innovative algorithm paves the way for more accessible and efficient privacy-preserving LLM applications.For those interested in delving deeper into the research and exploring the code and data, check out the full paper at: http://arxiv.org/abs/2403.01749v1#AI #NLP #LLMs #PrivacyPreservation #TextGeneration #TechInnovation \ud83d\ude80 Exciting innovation in privacy-preserving text generation! Check out the Aug-PE algorithm, allowing for the generation of DP synthetic text without model training. Results show competitive utility with SOTA methods. Learn more at: http://arxiv.org/abs/2403.01749v1 #AI #NLP #LLMs #PrivacyPreservation"},{"location":"sections/papers/7687185086521095419/f5dcb530892c4205aa99ce4227281fd4b1f09854f05c6bcebadbdcb19ba78827/#pdf","title":"PDF","text":""},{"location":"sections/synthetic_data/","title":"Synthetic data","text":""}]}