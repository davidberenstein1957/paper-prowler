{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Paper Prowler","text":"<p>The Paper Prowler is A versatile tool that aggregates and organizes web content like RSS feeds, arXiv papers to streamline your research and information management.</p>"},{"location":"#pipeline","title":"Pipeline","text":"<p>The Paper Prowler pipeline is a series of steps that are executed  to fetch, process, and store the data. The pipeline relies on configuration files to specify the steps and their parameters and writes and updates a Haystack <code>InMemoryDocumentStore</code> with the processed data.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Paper Prowler</li> <li>Papers<ul> <li>Overview</li> <li>synthetic data generation llms<ul> <li>Synthetic Test Collections for Retrieval Evaluation</li> <li>Synthetic Test Collections for Retrieval Evaluation</li> <li>ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs</li> <li>Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations</li> <li>Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data</li> <li>Differentially Private Synthetic Data via Foundation Model APIs 2: Text</li> <li>Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal</li> <li>Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications</li> <li>S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models</li> <li>MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents</li> <li>Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks</li> </ul> </li> </ul> </li> </ul>"},{"location":"sections/papers/","title":"Overview","text":"synthetic data generation llms title abstract Synthetic Test Collections for Retrieval Evaluation Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. Synthetic Test Collections for Retrieval Evaluation Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost. Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation. Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations. For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention. By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs. However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks. In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy. The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy. Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs. The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data. Differentially Private Synthetic Data via Foundation Model APIs 2: Text Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe. Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains. Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable. S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning. However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration. In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios. The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs. MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models. Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk"},{"location":"sections/papers/-7045798582985877485/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/","title":"Synthetic Test Collections for Retrieval Evaluation","text":"<p>Arxiv Link - 2024-05-13 14:11:09 </p>"},{"location":"sections/papers/-7045798582985877485/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#abstract","title":"Abstract","text":"<p>Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. </p>"},{"location":"sections/papers/-7045798582985877485/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in Information Retrieval evaluation using Large Language Models (LLMs)!Test collections are crucial for evaluating IR systems, but acquiring diverse user queries and relevance judgments can be challenging. Recent studies have shown the potential of LLMs in generating synthetic datasets for various applications, including IR.Check out this comprehensive investigation on constructing fully synthetic test collections using LLMs. The research explores generating synthetic queries and relevance judgments, demonstrating the feasibility of using LLMs for reliable retrieval evaluation.Read the full paper here: http://arxiv.org/abs/2405.07767v1#InformationRetrieval #LLMs #AI #TechResearch #ArtificialIntelligence #NLP #TechInnovation \ud83c\udf1f \ud83d\ude80 Exciting research alert! Can Large Language Models (LLMs) revolutionize the construction of test collections for information retrieval systems? Find out in this comprehensive study on using LLMs to generate synthetic test collections and relevance judgments: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLM #InformationRetrieval"},{"location":"sections/papers/-7045798582985877485/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/","title":"Differentially Private Synthetic Data via Foundation Model APIs 2: Text","text":"<p>Arxiv Link - 2024-03-04 05:57:50 </p>"},{"location":"sections/papers/-7045798582985877485/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/#abstract","title":"Abstract","text":"<p>Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe. </p>"},{"location":"sections/papers/-7045798582985877485/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/#socials","title":"Socials","text":"LinkedIn X \ud83c\udf1f Exciting News in the World of AI and Privacy-Preserving Technologies! \ud83c\udf1fText data privacy is a crucial concern in the age of machine learning. Check out the groundbreaking work by Lin et al. introducing the Aug-PE algorithm, designed to generate differentially private (DP) synthetic text data using API access to large language models (LLMs) without the need for model training.Curious to learn more about how Aug-PE can revolutionize privacy-preserving LLM applications? Dive into the details and explore the impressive results from comprehensive experiments on benchmark datasets. The study showcases that Aug-PE produces high-quality DP synthetic text comparable to state-of-the-art DP finetuning baselines.Ready to explore the future of privacy-preserving AI applications? Access the code and data at: https://github.com/AI-secure/aug-pe#AI #PrivacyPreservation #MachineLearning #DifferentialPrivacy #AugmentedPE #TechInnovation \ud83d\ude80 Exciting new research alert! Aug-PE algorithm enables generating high-quality differentially private synthetic text without model training. Check out the groundbreaking study by Lin et al. (2024) and explore the code and data at: \ud83d\udd17 http://arxiv.org/abs/2403.01749v1#AI #NLP #LLMs #PrivacyPreservation #MachineLearning"},{"location":"sections/papers/-7045798582985877485/3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","text":"<p>Arxiv Link - 2024-04-16 17:59:10 </p>"},{"location":"sections/papers/-7045798582985877485/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/#abstract","title":"Abstract","text":"<p>Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models. </p>"},{"location":"sections/papers/-7045798582985877485/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of NLP and LLMs! Researchers have developed a groundbreaking approach to fact-checking LLM outputs, significantly reducing computational costs while maintaining GPT-4-level performance. By training small models on synthetic data generated with GPT-4, they have successfully improved the efficiency of verifying facts in model generations. The newly introduced benchmark LLM-AggreFact, along with the MiniCheck-FT5 system, outperforms comparable models and achieves GPT-4 accuracy. Learn more about this innovative work at: http://arxiv.org/abs/2404.10774v1 #NLP #LLM #AI #FactChecking #Innovation \ud83d\udd0d\ud83d\udcca\ud83d\udd2c \ud83d\ude80 Exciting new research on fact-checking in NLP! Learn how small models with GPT-4-level performance are built at 400x lower cost. Check out MiniCheck-FT5, outperforming others of comparable size. Find out more at: http://arxiv.org/abs/2404.10774v1 #AI #NLP #LLM #research #factchecking"},{"location":"sections/papers/-7045798582985877485/3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/","title":"Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks","text":"<p>Arxiv Link - 2023-06-13 16:46:24 </p>"},{"location":"sections/papers/-7045798582985877485/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/#abstract","title":"Abstract","text":"<p>Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk </p>"},{"location":"sections/papers/-7045798582985877485/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting insights into the impact of Large Language Models (LLMs) on crowdsourcing! \ud83e\udd16\ud83d\udcacA recent study delved into the prevalence of LLM usage by crowd workers in data annotation tasks. The findings showed that 33-46% of crowd workers leveraged LLMs to enhance their productivity and earnings. This raises important considerations for ensuring the integrity of human-generated data in the era of advanced AI technologies.For a detailed overview of the study and its implications, check out the full paper here: http://arxiv.org/abs/2306.07899v1#LLMs #Crowdsourcing #AI #DataAnnotation #TechResearch #ArtificialIntelligence #NLPCode and data from the study are available at: https://github.com/epfl-dlab/GPTurkLet's keep exploring the intersection of human intelligence and AI advancements! \ud83c\udf10\ud83d\udca1#TechInnovation #Research #DataScience #MachineLearning #SocialMediaExpert #LinkedInPost \ud83d\ude80 New research alert! Learn how Large Language Models impact crowd workers and human annotations in AI tasks. A case study found that 33-46% of workers used LLMs on Amazon Mechanical Turk. Check out the study here: http://arxiv.org/abs/2306.07899v1 #AI #LLMs #NLP #ResearchCode/data available at: https://github.com/epfl-dlab/GPTurk"},{"location":"sections/papers/-7045798582985877485/4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications","text":"<p>Arxiv Link - 2024-04-02 12:25:57 </p>"},{"location":"sections/papers/-7045798582985877485/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/#abstract","title":"Abstract","text":"<p>Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable. </p>"},{"location":"sections/papers/-7045798582985877485/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the world of Natural Language Processing! \ud83c\udf1fAre you interested in cutting-edge research on mitigating errors in NLP models during classification tasks? Check out this groundbreaking study that explores the use of large language models (LLMs) for data augmentation to address high confidence misclassifications. The research compares the effectiveness of synthetic data generated by LLMs versus human-provided data in reducing wrong predictions while maintaining accuracy levels. Results show that LLMs can significantly reduce high confidence misclassifications, offering a more scalable solution compared to human-provided data.Read more about this innovative approach and its implications for NLP models here: http://arxiv.org/abs/2403.17860v2#NLP #LLMs #DataAugmentation #AI #TechResearch #Innovation #ArtificialIntelligence #MachineLearning #TechTrends Exciting research on leveraging large language models for data augmentation in NLP to reduce high confidence misclassifications! \ud83d\ude80\ud83e\udd16 Check out the study comparing human-generated vs. LLM-generated synthetic data for classification tasks. Results show promising effectiveness and scalability. Read more at: http://arxiv.org/abs/2403.17860v2 #AI #NLP #LLM #DataAugmentation #Research"},{"location":"sections/papers/-7045798582985877485/4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/","title":"Synthetic Test Collections for Retrieval Evaluation","text":"<p>Arxiv Link - 2024-05-13 14:11:09 </p>"},{"location":"sections/papers/-7045798582985877485/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/#abstract","title":"Abstract","text":"<p>Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. </p>"},{"location":"sections/papers/-7045798582985877485/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the world of Information Retrieval (IR) systems! A recent study delves into the potential of using Large Language Models (LLMs) to construct synthetic test collections for evaluation purposes. \ud83d\udd0d Generating diverse user queries and relevance judgments for test collections can be a challenge, but leveraging LLMs offers a promising solution. By creating synthetic queries and judgments, researchers are exploring the possibility of enhancing the evaluation process without the traditional resource-intensive methods.\ud83d\udcca The study thoroughly investigates the feasibility of using LLMs to construct fully synthetic test collections and evaluates the reliability of such collections. The results suggest that synthetic test collections generated using LLMs can indeed be reliable for evaluating IR systems.\ud83d\udd17 Dive deeper into the details of this innovative research at: Read more#ArtificialIntelligence #NLP #LLMs #InformationRetrieval #Research #TechInnovation \ud83d\ude80 Exciting research on using Large Language Models (LLMs) to construct synthetic test collections for Information Retrieval evaluation. Discover how LLMs can generate synthetic queries and relevance judgments efficiently! Check out the study here: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLMs #InformationRetrieval #TechResearch"},{"location":"sections/papers/-7045798582985877485/7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/","title":"Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal","text":"<p>Arxiv Link - 2024-05-25 12:17:29 </p>"},{"location":"sections/papers/-7045798582985877485/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/#abstract","title":"Abstract","text":"<p>Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains. </p>"},{"location":"sections/papers/-7045798582985877485/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting News in AI Research \ud83d\ude80Large language models (LLMs) face a significant challenge in continual learning due to catastrophic forgetting. Conventional methods rely on previous training data for rehearsal, which may not be practical in real-world scenarios. Our latest research introduces a cutting-edge framework, Self-Synthesized Rehearsal (SSR), to tackle this issue.\ud83d\udd0d SSR leverages LLMs to generate synthetic instances for rehearsal, enabling continual learning without access to original training data. By utilizing the base LLM for in-context learning to create synthetic instances and refining them with the latest LLM, SSR maintains and enhances the model's acquired abilities. Moreover, diverse high-quality synthetic instances are selected for future rehearsal, ensuring data-efficient performance.\ud83d\udcca Experimental results showcase that SSR outperforms traditional rehearsal-based methods while preserving the generalization capabilities of LLMs in various domains.Learn more about our innovative SSR framework in our research paper: http://arxiv.org/abs/2403.01244v2#AI #LLM #ContinualLearning #Research #TechInnovation #NLP #SSR #ArtificialIntelligence #DataEfficiency \ud83d\ude80 Exciting research alert! Addressing catastrophic forgetting in Large Language Models (LLMs) during continual learning, a new framework called Self-Synthesized Rehearsal (SSR) has been proposed. SSR generates synthetic instances for rehearsal, achieving superior performance while being more data-efficient. Check out the results here: http://arxiv.org/abs/2403.01244v2 #AI #NLP #LLM #Research #TechInnovation \ud83e\udd16\ud83d\udcda"},{"location":"sections/papers/-7045798582985877485/b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/","title":"Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data","text":"<p>Arxiv Link - 2024-05-23 06:14:35 </p>"},{"location":"sections/papers/-7045798582985877485/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/#abstract","title":"Abstract","text":"<p>As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning scenarios, directly using external LLMs on private data is strictly prohibited by stringent data security and privacy regulations. For local clients, the utilization of LLMs to improve the domain-specific small language models (SLMs), characterized by limited computational resources and domain-specific data, has attracted considerable research attention. By observing that LLMs can empower domain-specific SLMs, existing methods predominantly concentrate on leveraging the public data or LLMs to generate more data to transfer knowledge from LLMs to SLMs. However, due to the discrepancies between LLMs' generated data and clients' domain-specific data, these methods cannot yield substantial improvements in the domain-specific tasks. In this paper, we introduce a Federated Domain-specific Knowledge Transfer (FDKT) framework, which enables domain-specific knowledge transfer from LLMs to SLMs while preserving clients' data privacy. The core insight is to leverage LLMs to augment data based on domain-specific few-shot demonstrations, which are synthesized from private domain data using differential privacy. Such synthetic samples share similar data distribution with clients' private data and allow the server LLM to generate particular knowledge to improve clients' SLMs. The extensive experimental results demonstrate that the proposed FDKT framework consistently and greatly improves SLMs' task performance by around 5\\% with a privacy budget of less than 10, compared to local training on private data. </p>"},{"location":"sections/papers/-7045798582985877485/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting news in the field of AI and privacy protection! \ud83d\udee1\ufe0f Our latest research introduces the Federated Domain-specific Knowledge Transfer (FDKT) framework, enabling the transfer of domain-specific knowledge from large language models (LLMs) to small language models (SLMs) while safeguarding clients' data privacy. \ud83d\udcc8 The FDKT framework leverages LLMs to augment data based on domain-specific few-shot demonstrations synthesized from private domain data using differential privacy. This approach ensures that the synthetic samples share a similar data distribution with clients' private data, leading to significant improvements in SLMs' task performance by approximately 5% with a privacy budget of less than 10, compared to local training on private data.\ud83d\udd0d Dive deeper into the results and methodology by checking out the full research paper at: http://arxiv.org/abs/2405.14212v1#AI #PrivacyProtection #LLMs #FDKT #Research #DataPrivacy #TechInnovation \ud83d\ude80 Exciting research in enhancing domain-specific small language models (SLMs) using Federated Domain-specific Knowledge Transfer (FDKT) framework! \ud83e\udd16\ud83d\udcda Learn how to transfer knowledge from LLMs to SLMs while preserving data privacy: http://arxiv.org/abs/2405.14212v1 #AI #NLP #LLMs #DataPrivacy #Research #FDKT \ud83d\udcca\ud83d\udd12"},{"location":"sections/papers/-7045798582985877485/b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/","title":"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations","text":"<p>Arxiv Link - 2023-10-13 01:31:59 </p>"},{"location":"sections/papers/-7045798582985877485/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/#abstract","title":"Abstract","text":"<p>The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation. </p>"},{"location":"sections/papers/-7045798582985877485/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting new study alert! Researchers dive into the world of Large Language Models (LLMs) for synthetic data generation in text classification models. Check out the latest findings on the impact of subjectivity on model performance when using LLM-generated synthetic data. Curious to learn more? Dive into the full study here: http://arxiv.org/abs/2310.07849v2#AI #NLP #LLMs #TextClassification #Research #Tech #ArtificialIntelligence #MachineLearning \ud83d\ude80 New research alert! How effective are large language models in generating synthetic data for text classification models? Check out the findings on the impact of subjectivity on model performance: http://arxiv.org/abs/2310.07849v2 #AI #NLP #LLMs #DataSynthesis"},{"location":"sections/papers/-7045798582985877485/c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/","title":"ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs","text":"<p>Arxiv Link - 2024-02-19 01:28:48 </p>"},{"location":"sections/papers/-7045798582985877485/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/#abstract","title":"Abstract","text":"<p>Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost. </p>"},{"location":"sections/papers/-7045798582985877485/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in the world of AI and debiasing LLMs! \ud83e\udd16 This groundbreaking work introduces a novel approach using ChatGPT to generate synthetic training data, enhancing the debiasing of Large Language Models. Check out the full study here: http://arxiv.org/abs/2402.11764v1Key findings include:1\ufe0f\u20e3 Efficient production of high-quality training data for debiasing LLMs using ChatGPT.2\ufe0f\u20e3 Surpassing existing datasets in debiasing performance while preserving internal LLM knowledge.3\ufe0f\u20e3 Generalizability across categories, effectively mitigating various biases, including intersectional ones.These results highlight the potential of synthetic data in promoting fairness in LLMs with minimal retraining costs. A must-read for all tech enthusiasts and AI professionals! \ud83c\udf10\ud83d\udca1 #AI #LLMs #Debiasing #ChatGPT #TechInnovation \"Exciting research on using ChatGPT to enhance debiasing of Large Language Models (LLMs)! This innovative approach generates synthetic training data for efficient debiasing, surpassing existing datasets in performance. Learn more at: http://arxiv.org/abs/2402.11764v1 #AI #NLP #LLMs #Debiasing\""},{"location":"sections/papers/-7045798582985877485/df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8/#pdf","title":"PDF","text":""},{"location":"sections/papers/-7045798582985877485/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/","title":"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models","text":"<p>Arxiv Link - 2024-04-06 15:20:18 </p>"},{"location":"sections/papers/-7045798582985877485/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/#abstract","title":"Abstract","text":"<p>The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning. However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration. In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios. The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs. </p>"},{"location":"sections/papers/-7045798582985877485/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting developments in the field of Large Language Models (LLMs)! A recent paper introduces S3Eval, a Synthetic, Scalable, Systematic evaluation suite designed to assess LLM capabilities in processing long contexts. By utilizing complex synthetic tasks, S3Eval offers a flexible and controlled method to evaluate LLM performance across diverse scenarios. The correlation between S3Eval and real-world benchmarks showcases its effectiveness in gauging LLM capabilities. Experimental results with the S3Eval-Standard dataset have revealed significant challenges for existing LLMs, highlighting the potential of this evaluation method.Read more about S3Eval and its implications for LLM assessment here: http://arxiv.org/abs/2310.15147v2#AI #NLP #LLMs #Technology #Research #Innovation \ud83d\ude80 Exciting development in Large Language Models (LLMs) evaluation! Check out S3Eval, a Synthetic, Scalable, Systematic evaluation suite that challenges existing LLMs with complex synthetic tasks. Learn more at: http://arxiv.org/abs/2310.15147v2 #AI #NLP #LLMs #TechInnovation \ud83e\udde0\ud83d\udd0d"},{"location":"sections/papers/-7045798582985877485/f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0/#pdf","title":"PDF","text":""},{"location":"sections/synthetic_data/","title":"Synthetic data","text":""}]}