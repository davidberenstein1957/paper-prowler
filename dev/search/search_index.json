{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Paper Prowler","text":"<p>The Paper Prowler is A versatile tool that aggregates and organizes web content like RSS feeds, arXiv papers to streamline your research and information management.</p>"},{"location":"#pipeline","title":"Pipeline","text":"<p>The Paper Prowler pipeline is a series of steps that are executed  to fetch, process, and store the data. The pipeline relies on configuration files to specify the steps and their parameters and writes and updates a Haystack <code>InMemoryDocumentStore</code> with the processed data.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Paper Prowler</li> <li>Papers<ul> <li>Overview</li> <li>synthetic data generation llms<ul> <li>Synthetic Test Collections for Retrieval Evaluation</li> </ul> </li> </ul> </li> </ul>"},{"location":"sections/papers/","title":"Overview","text":"synthetic data generation llms title abstract Synthetic Test Collections for Retrieval Evaluation Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation."},{"location":"sections/papers/8855485453102157101/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/","title":"Synthetic Test Collections for Retrieval Evaluation","text":"<p>Arxiv Link - 2024-05-13 14:11:09 </p>"},{"location":"sections/papers/8855485453102157101/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#abstract","title":"Abstract","text":"<p>Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation. </p>"},{"location":"sections/papers/8855485453102157101/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#socials","title":"Socials","text":"LinkedIn X \ud83d\ude80 Exciting advancements in Information Retrieval evaluation using Large Language Models (LLMs)!Test collections are crucial for evaluating IR systems, but acquiring diverse user queries and relevance judgments can be challenging. Recent studies have shown the potential of LLMs in generating synthetic datasets for various applications, including IR.Check out this comprehensive investigation on constructing fully synthetic test collections using LLMs. The research explores generating synthetic queries and relevance judgments, demonstrating the feasibility of using LLMs for reliable retrieval evaluation.Read the full paper here: http://arxiv.org/abs/2405.07767v1#InformationRetrieval #LLMs #AI #TechResearch #ArtificialIntelligence #NLP #TechInnovation \ud83c\udf1f \ud83d\ude80 Exciting research alert! Can Large Language Models (LLMs) revolutionize the construction of test collections for information retrieval systems? Find out in this comprehensive study on using LLMs to generate synthetic test collections and relevance judgments: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLM #InformationRetrieval"},{"location":"sections/papers/8855485453102157101/087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388/#pdf","title":"PDF","text":""},{"location":"sections/synthetic_data/","title":"Synthetic data","text":""}]}