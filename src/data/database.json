{"type": "paper_prowler.database.Database", "init_parameters": {"bm25_tokenization_regex": "(?u)\\b\\w\\w+\\b", "bm25_algorithm": "BM25L", "bm25_parameters": {}, "embedding_similarity_function": "dot_product"}, "documents": [{"id": "087a84528e00eddcebf167b07fecfc741cd0bd35d969d905535d2392c9515388", "content": "Test collections play a vital role in evaluation of information retrieval\n(IR) systems. Obtaining a diverse set of user queries for test collection\nconstruction can be challenging, and acquiring relevance judgments, which\nindicate the appropriateness of retrieved documents to a query, is often costly\nand resource-intensive. Generating synthetic datasets using Large Language\nModels (LLMs) has recently gained significant attention in various\napplications. In IR, while previous work exploited the capabilities of LLMs to\ngenerate synthetic queries or documents to augment training data and improve\nthe performance of ranking models, using LLMs for constructing synthetic test\ncollections is relatively unexplored. Previous studies demonstrate that LLMs\nhave the potential to generate synthetic relevance judgments for use in the\nevaluation of IR systems. In this paper, we comprehensively investigate whether\nit is possible to use LLMs to construct fully synthetic test collections by\ngenerating not only synthetic judgments but also synthetic queries. In\nparticular, we analyse whether it is possible to construct reliable synthetic\ntest collections and the potential risks of bias such test collections may\nexhibit towards LLM-based models. Our experiments indicate that using LLMs it\nis possible to construct synthetic test collections that can reliably be used\nfor retrieval evaluation.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2405.07767v1", "title": "Synthetic Test Collections for Retrieval Evaluation", "content": "http://arxiv.org/pdf/2405.07767v1", "datetime": "2024-05-13 14:11:09", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting advancements in Information Retrieval evaluation using Large Language Models (LLMs)!\n\nTest collections are crucial for evaluating IR systems, but acquiring diverse user queries and relevance judgments can be challenging. Recent studies have shown the potential of LLMs in generating synthetic datasets for various applications, including IR.\n\nCheck out this comprehensive investigation on constructing fully synthetic test collections using LLMs. The research explores generating synthetic queries and relevance judgments, demonstrating the feasibility of using LLMs for reliable retrieval evaluation.\n\nRead the full paper here: http://arxiv.org/abs/2405.07767v1\n\n#InformationRetrieval #LLMs #AI #TechResearch #ArtificialIntelligence #NLP #TechInnovation \ud83c\udf1f", "x": "\ud83d\ude80 Exciting research alert! Can Large Language Models (LLMs) revolutionize the construction of test collections for information retrieval systems? Find out in this comprehensive study on using LLMs to generate synthetic test collections and relevance judgments: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLM #InformationRetrieval", "source_id": "3ecdbc9213aa7c131b7b434ee9fbe34f20d323c7a5d3974a6a063ee271fc40b9", "page_number": 1}, "score": null, "embedding": [-0.3488380014896393, 0.3338946998119354, 0.08413291722536087, 0.10964095592498779, 0.12859179079532623, 0.07224202156066895, -0.019731678068637848, 0.03952661529183388, 0.2615777850151062, -0.37264952063560486, -0.19557243585586548, -0.06581247597932816, 0.46897831559181213, 0.44428256154060364, 0.10500131547451019, 0.18735069036483765, -0.19112159311771393, 0.1618434637784958, -0.11537206172943115, 0.21857215464115143, 0.3530536890029907, 0.015567734837532043, 0.09974409639835358, -0.126869797706604, -0.27004891633987427, -0.006609159056097269, -0.16978369653224945, -0.04895495995879173, -0.3716340959072113, -1.3569667339324951, -0.011670051142573357, -0.22214478254318237, 0.5758540034294128, 0.17441873252391815, -0.2518382668495178, 0.02638510800898075, -0.3789442479610443, 0.09183527529239655, -0.12206552922725677, 0.2223302721977234, 0.04671204090118408, 0.06959748268127441, -0.2695983350276947, -0.04594366252422333, -0.1423555463552475, -0.16361132264137268, -0.18820583820343018, -0.04420437663793564, -0.5235346555709839, 0.03960582986474037, -0.22041688859462738, -0.22400116920471191, 0.09858454018831253, 0.04045605659484863, -0.09954577684402466, 0.10600148886442184, 0.41527771949768066, 0.1968359798192978, 0.043290186673402786, -0.024271517992019653, 0.40207505226135254, 0.11292842775583267, -0.9541235566139221, 0.056218378245830536, -0.2092723250389099, 0.19989797472953796, -0.17148452997207642, -0.0532916784286499, 0.3597913384437561, -0.02734004519879818, -0.22305770218372345, 0.0759091004729271, 0.03520842641592026, 0.4253426194190979, 0.3314041495323181, 0.259103924036026, 0.04204763472080231, -0.2515597939491272, 0.2444983273744583, -0.043511632829904556, 0.11477456986904144, -0.1180158406496048, 0.05730162560939789, -0.3351309895515442, 0.03875800594687462, -0.049020908772945404, -0.07474979013204575, -0.14868275821208954, 0.31677570939064026, 0.11726599931716919, 0.04883856698870659, -0.0406145304441452, -0.26807019114494324, 0.5306349992752075, -0.17261222004890442, -0.22771364450454712, 0.016507156193256378, 0.27687957882881165, 0.06911960244178772, 0.9098932147026062, -0.2553708851337433, 0.22770002484321594, -0.017310045659542084, -0.2122838795185089, -0.06485328078269958, -0.11619210988283157, -0.05232671648263931, -0.3804282546043396, -0.3134671747684479, -0.33740097284317017, -0.12463773787021637, -0.0023636093828827143, 0.14407600462436676, -0.17506574094295502, -0.05184505879878998, 0.224426731467247, 0.5068951845169067, 0.24543239176273346, -0.18637822568416595, -0.21255572140216827, -0.44904622435569763, 0.2495708167552948, 0.02145923301577568, -0.1375126987695694, 0.048569776117801666, -0.08436274528503418, 0.1994674950838089, 0.30844545364379883, 0.04462209716439247, -0.06752575188875198, -0.13817381858825684, -0.21279610693454742, -0.49517741799354553, -0.13792210817337036, 0.052930887788534164, -0.058615073561668396, 0.1037425845861435, -0.12497226148843765, 0.10083121806383133, 0.40914279222488403, 0.024877987802028656, 0.10437438637018204, 0.287360817193985, 0.016661176458001137, -0.5302039384841919, 0.7812649011611938, -0.23877796530723572, 0.14603294432163239, -0.30603525042533875, -0.4594229459762573, 0.19407187402248383, 0.17000414431095123, 0.036664556711912155, -0.3552372455596924, 0.2092817723751068, 0.028148606419563293, 0.10844215750694275, 0.15349392592906952, -0.3364047408103943, -0.04677748307585716, -0.06744933128356934, 0.16364510357379913, -0.16535584628582, 0.7299871444702148, -0.24239201843738556, -0.1160176619887352, -0.330045610666275, 0.19473311305046082, 0.19232487678527832, -0.06628365069627762, 0.25151193141937256, -0.08752181380987167, -0.15758346021175385, 0.05937258526682854, -0.10137379169464111, -0.08342329412698746, -0.5459420084953308, 0.2391050010919571, -0.08339150995016098, 0.03975833207368851, -0.08261856436729431, -0.37755993008613586, -0.06665612757205963, 0.24842967092990875, 0.012023534625768661, -0.20541200041770935, -0.19862554967403412, -0.06462625414133072, 0.15881887078285217, 0.22388599812984467, 0.0011397517519071698, -0.0163363479077816, -0.17299006879329681, 0.05916108563542366, 0.10844120383262634, -0.11273932456970215, 0.005688616074621677, -0.00647024717181921, 0.03112596832215786, -0.09151685982942581, 0.31102949380874634, 0.044485852122306824, 0.07547425478696823, -0.10416846722364426, -0.12628716230392456, 0.1009984090924263, 0.08673952519893646, -0.21056316792964935, 0.15786123275756836, 0.15343652665615082, -0.27474918961524963, -0.053510066121816635, 0.11667656898498535, -0.09801740199327469, -0.2809230387210846, -0.19160744547843933, 0.49832791090011597, 0.05880458280444145, -0.1851906031370163, 0.19673852622509003, -0.10876186192035675, -0.1741679608821869, -0.07102486491203308, -1.251766562461853, 0.0028528389520943165, -0.10652931034564972, 0.2350829392671585, 0.2813642919063568, -0.4340212643146515, -0.09159282594919205, 0.13763844966888428, 0.05648760125041008, 0.3323647677898407, -0.06173934414982796, 0.12768396735191345, -0.1598219871520996, 0.19222846627235413, 0.034889742732048035, -0.20652160048484802, -0.2601494789123535, 0.07950989902019501, -0.28513121604919434, 0.28005847334861755, -0.21953609585762024, 0.1377190798521042, 0.40953484177589417, -0.7189938426017761, 0.27436545491218567, -0.06765013933181763, 0.7938413023948669, -0.1820749044418335, -0.18613994121551514, -0.09555216878652573, 0.3156413435935974, 0.015473487786948681, -0.14233040809631348, -0.4701741635799408, 0.4236377775669098, 0.16063831746578217, 0.0486532524228096, 0.17975956201553345, -0.02429269254207611, -0.030484389513731003, -0.25205808877944946, -0.07079949975013733, 0.2704841196537018, -0.4515663683414459, -0.024688974022865295, -0.10143442451953888, -0.17335164546966553, -0.11156231164932251, -0.15943172574043274, 0.5787139534950256, -0.02282247133553028, 0.13022923469543457, 0.28511956334114075, -0.07216301560401917, -0.07907325029373169, -0.08812041580677032, -0.6732575297355652, -0.23568402230739594, -0.12576399743556976, -0.08591104298830032, 0.0936310738325119, -0.006973958108574152, 0.19325505197048187, -0.209942027926445, 0.3940742313861847, -0.3002215325832367, -0.04418675974011421, 0.22759082913398743, 0.13728861510753632, -0.3651493489742279, -0.11532150208950043, 0.5033685564994812, -0.15650424361228943, 0.4605247676372528, -0.10297688096761703, 0.25614991784095764, -0.18861854076385498, -0.16845718026161194, 0.06239771470427513, -0.26182815432548523, 0.4735451936721802, 0.20836222171783447, 0.2653369903564453, 0.1711520552635193, 0.1909404993057251, 0.2940540611743927, 0.45529940724372864, 0.2638966143131256, 0.2448636144399643, 0.06997570395469666, 0.022651517763733864, 0.18037839233875275, -0.3712258040904999, -0.10636425763368607, 0.2323072850704193, 0.07448543608188629, -1.1090481281280518, 0.0007898311014287174, -0.09668850898742676, 0.4707534611225128, -0.183078333735466, -0.185586616396904, 0.135294109582901, -0.1164044588804245, 0.21516430377960205, 0.2672047019004822, 0.17023789882659912, 0.11648058146238327, 0.06775509566068649, -0.1922178566455841, 0.06892786175012589, 0.15309719741344452, 0.3096602261066437, -0.19180624186992645, 0.2188423126935959, -0.5401977300643921, -0.1461281031370163, 0.1992706060409546, 1.2229803800582886, 0.024044541642069817, 0.08033086359500885, 0.04520159959793091, -0.03381628543138504, -0.06744752079248428, -0.3986457288265228, -0.11061549186706543, 0.20584815740585327, -0.08716943114995956, 0.641609787940979, -0.049002643674612045, 0.028047144412994385, 0.42916005849838257, -0.15325498580932617, 0.04093470051884651, 0.10597876459360123, -0.09516564011573792, 0.16639354825019836, -0.060598429292440414, -0.27428701519966125, 0.05994923412799835, 0.717934250831604, 0.2591702938079834, 0.03375529870390892, -0.3580535650253296, -0.13867983222007751, 0.036067474633455276, -0.2009880393743515, 0.030672166496515274, -0.13138794898986816, -0.3030749559402466, 0.47568652033805847, 0.23235957324504852, -0.010975703597068787, 0.044276054948568344, 0.023052915930747986, -0.07182201743125916, -0.0075151678174734116, 0.2215399593114853, 0.10345767438411713, -0.045392196625471115, -0.22803226113319397], "sparse_embedding": null}, {"id": "7d59e73ee346bf20e448c32497594e56f348b1d4d0fc24e02843cdbb7834702c", "content": "Test collections play a vital role in evaluation of information retrieval\n(IR) systems. Obtaining a diverse set of user queries for test collection\nconstruction can be challenging, and acquiring relevance judgments, which\nindicate the appropriateness of retrieved documents to a query, is often costly\nand resource-intensive. Generating synthetic datasets using Large Language\nModels (LLMs) has recently gained significant attention in various\napplications. In IR, while previous work exploited the capabilities of LLMs to\ngenerate synthetic queries or documents to augment training data and improve\nthe performance of ranking models, using LLMs for constructing synthetic test\ncollections is relatively unexplored. Previous studies demonstrate that LLMs\nhave the potential to generate synthetic relevance judgments for use in the\nevaluation of IR systems. In this paper, we comprehensively investigate whether\nit is possible to use LLMs to construct fully synthetic test collections by\ngenerating not only synthetic judgments but also synthetic queries. In\nparticular, we analyse whether it is possible to construct reliable synthetic\ntest collections and the potential risks of bias such test collections may\nexhibit towards LLM-based models. Our experiments indicate that using LLMs it\nis possible to construct synthetic test collections that can reliably be used\nfor retrieval evaluation.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2405.07767v1", "title": "Synthetic Test Collections for Retrieval Evaluation", "content": "http://arxiv.org/pdf/2405.07767v1", "datetime": "2024-05-13 14:11:09", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting developments in the world of Information Retrieval (IR) systems! A recent study delves into the potential of using Large Language Models (LLMs) to construct synthetic test collections for evaluation purposes. \n\n\ud83d\udd0d Generating diverse user queries and relevance judgments for test collections can be a challenge, but leveraging LLMs offers a promising solution. By creating synthetic queries and judgments, researchers are exploring the possibility of enhancing the evaluation process without the traditional resource-intensive methods.\n\n\ud83d\udcca The study thoroughly investigates the feasibility of using LLMs to construct fully synthetic test collections and evaluates the reliability of such collections. The results suggest that synthetic test collections generated using LLMs can indeed be reliable for evaluating IR systems.\n\n\ud83d\udd17 Dive deeper into the details of this innovative research at: [Read more](http://arxiv.org/abs/2405.07767v1)\n\n#ArtificialIntelligence #NLP #LLMs #InformationRetrieval #Research #TechInnovation", "x": "\ud83d\ude80 Exciting research on using Large Language Models (LLMs) to construct synthetic test collections for Information Retrieval evaluation. Discover how LLMs can generate synthetic queries and relevance judgments efficiently! Check out the study here: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLMs #InformationRetrieval #TechResearch", "source_id": "fb6c55988e03568c9c1e1bdfed54e385db0e5737711ae9bb15730e00b716f968", "page_number": 1}, "score": null, "embedding": [-0.3488380014896393, 0.3338947594165802, 0.08413302153348923, 0.1096409410238266, 0.12859174609184265, 0.07224198430776596, -0.01973169483244419, 0.03952665254473686, 0.2615777254104614, -0.3726494610309601, -0.19557246565818787, -0.0658123642206192, 0.4689783751964569, 0.444282591342926, 0.10500134527683258, 0.18735061585903168, -0.19112159311771393, 0.16184337437152863, -0.11537201702594757, 0.21857215464115143, 0.3530536890029907, 0.015567806549370289, 0.09974405169487, -0.12686975300312042, -0.27004897594451904, -0.006609133444726467, -0.16978365182876587, -0.04895494133234024, -0.3716340959072113, -1.3569667339324951, -0.011670120060443878, -0.22214464843273163, 0.5758541226387024, 0.17441870272159576, -0.251838356256485, 0.026384949684143066, -0.3789442777633667, 0.09183529764413834, -0.12206558883190155, 0.222330242395401, 0.04671197384595871, 0.06959744542837143, -0.2695982754230499, -0.04594366252422333, -0.14235559105873108, -0.1636113077402115, -0.1882057934999466, -0.044204432517290115, -0.5235347151756287, 0.039605762809515, -0.2204168289899826, -0.22400116920471191, 0.09858459234237671, 0.04045602306723595, -0.0995456725358963, 0.10600141435861588, 0.41527771949768066, 0.19683600962162018, 0.043290261179208755, -0.024271467700600624, 0.4020750820636749, 0.11292839795351028, -0.9541236162185669, 0.05621834471821785, -0.20927223563194275, 0.19989798963069916, -0.17148450016975403, -0.0532916858792305, 0.3597913384437561, -0.02734001725912094, -0.22305770218372345, 0.0759091004729271, 0.0352083295583725, 0.4253425896167755, 0.3314042091369629, 0.2591039836406708, 0.04204750061035156, -0.2515597641468048, 0.2444983720779419, -0.04351162910461426, 0.11477458477020264, -0.11801591515541077, 0.05730168893933296, -0.335130900144577, 0.03875799477100372, -0.04902089014649391, -0.07474987953901291, -0.14868274331092834, 0.31677567958831787, 0.11726601421833038, 0.04883849620819092, -0.040614575147628784, -0.268070250749588, 0.5306349992752075, -0.1726122498512268, -0.22771373391151428, 0.01650707609951496, 0.27687954902648926, 0.0691196396946907, 0.9098931550979614, -0.2553708553314209, 0.22770008444786072, -0.017310015857219696, -0.21228384971618652, -0.06485331058502197, -0.11619209498167038, -0.05232660099864006, -0.38042834401130676, -0.31346723437309265, -0.3374009132385254, -0.12463778257369995, -0.0023636030964553356, 0.1440758854150772, -0.17506574094295502, -0.051845092326402664, 0.2244267463684082, 0.5068951845169067, 0.24543237686157227, -0.18637825548648834, -0.21255581080913544, -0.44904622435569763, 0.249570831656456, 0.02145908959209919, -0.13751260936260223, 0.048569805920124054, -0.08436280488967896, 0.1994674652814865, 0.30844539403915405, 0.04462207853794098, -0.067525714635849, -0.13817381858825684, -0.2127961814403534, -0.49517741799354553, -0.13792206346988678, 0.05293094739317894, -0.0586150586605072, 0.10374259203672409, -0.12497226893901825, 0.10083121806383133, 0.40914273262023926, 0.02487790957093239, 0.10437434911727905, 0.2873607277870178, 0.016661223024129868, -0.5302038788795471, 0.7812649607658386, -0.2387779802083969, 0.1460329294204712, -0.30603528022766113, -0.4594229757785797, 0.19407173991203308, 0.17000405490398407, 0.03666457533836365, -0.3552371859550476, 0.20928163826465607, 0.028148643672466278, 0.10844220221042633, 0.1534939855337143, -0.3364046812057495, -0.046777497977018356, -0.06744931638240814, 0.1636451929807663, -0.16535590589046478, 0.7299871444702148, -0.24239201843738556, -0.11601758748292923, -0.330045610666275, 0.194733127951622, 0.19232487678527832, -0.06628365814685822, 0.25151196122169495, -0.0875217542052269, -0.15758346021175385, 0.05937263369560242, -0.10137379169464111, -0.08342328667640686, -0.5459420084953308, 0.2391051948070526, -0.08339163661003113, 0.03975839540362358, -0.0826185941696167, -0.3775600492954254, -0.0666562169790268, 0.24842970073223114, 0.012023398652672768, -0.2054118812084198, -0.1986255794763565, -0.06462620943784714, 0.15881893038749695, 0.2238858938217163, 0.0011397537309676409, -0.016336433589458466, -0.1729901283979416, 0.05916108936071396, 0.10844114422798157, -0.11273942142724991, 0.005688630510121584, -0.006470251828432083, 0.031125973910093307, -0.09151701629161835, 0.3110295534133911, 0.04448584467172623, 0.07547428458929062, -0.10416844487190247, -0.12628719210624695, 0.10099843144416809, 0.0867394432425499, -0.21056310832500458, 0.15786145627498627, 0.15343661606311798, -0.27474913001060486, -0.05351004749536514, 0.11667660623788834, -0.09801750630140305, -0.2809230387210846, -0.19160744547843933, 0.4983278512954712, 0.058804407715797424, -0.18519049882888794, 0.1967386156320572, -0.10876180231571198, -0.17416787147521973, -0.07102493196725845, -1.251766324043274, 0.0028529127594083548, -0.10652938485145569, 0.23508292436599731, 0.28136420249938965, -0.43402138352394104, -0.09159284085035324, 0.13763858377933502, 0.05648757144808769, 0.33236488699913025, -0.06173934042453766, 0.127684086561203, -0.15982192754745483, 0.19222842156887054, 0.03488975018262863, -0.20652160048484802, -0.2601495087146759, 0.0795099213719368, -0.28513118624687195, 0.28005844354629517, -0.2195361703634262, 0.13771919906139374, 0.40953484177589417, -0.7189937829971313, 0.27436548471450806, -0.067650206387043, 0.7938412427902222, -0.1820748895406723, -0.1861400008201599, -0.0955522358417511, 0.3156414031982422, 0.015473554842174053, -0.14233043789863586, -0.470174103975296, 0.4236376881599426, 0.16063830256462097, 0.04865328595042229, 0.17975959181785583, -0.024292731657624245, -0.03048444166779518, -0.25205811858177185, -0.07079952210187912, 0.2704840898513794, -0.45156630873680115, -0.024688884615898132, -0.10143446922302246, -0.17335158586502075, -0.11156243830919266, -0.15943174064159393, 0.5787137746810913, -0.022822469472885132, 0.13022921979427338, 0.28511950373649597, -0.07216299325227737, -0.07907319813966751, -0.08812040090560913, -0.6732575297355652, -0.23568400740623474, -0.12576401233673096, -0.08591113984584808, 0.09363098442554474, -0.006973947864025831, 0.1932549923658371, -0.209942027926445, 0.3940742015838623, -0.3002214729785919, -0.0441867932677269, 0.227590873837471, 0.13728860020637512, -0.3651493489742279, -0.11532137542963028, 0.5033685564994812, -0.15650424361228943, 0.46052470803260803, -0.10297692567110062, 0.25614988803863525, -0.18861840665340424, -0.16845707595348358, 0.062397658824920654, -0.26182812452316284, 0.4735451936721802, 0.20836223661899567, 0.26533693075180054, 0.1711519956588745, 0.1909405142068863, 0.2940540611743927, 0.455299437046051, 0.26389655470848083, 0.24486373364925385, 0.06997570395469666, 0.02265148051083088, 0.18037837743759155, -0.37122565507888794, -0.10636427998542786, 0.23230724036693573, 0.07448536157608032, -1.1090481281280518, 0.0007897727191448212, -0.09668862819671631, 0.47075343132019043, -0.183078333735466, -0.185586616396904, 0.1352941393852234, -0.1164044439792633, 0.21516424417495728, 0.2672048509120941, 0.1702379435300827, 0.11648046970367432, 0.0677550733089447, -0.1922179013490677, 0.06892789155244827, 0.1530972272157669, 0.3096601963043213, -0.19180628657341003, 0.2188422679901123, -0.5401977300643921, -0.1461281180381775, 0.199270561337471, 1.2229803800582886, 0.024044545367360115, 0.08033087104558945, 0.0452016144990921, -0.033816296607255936, -0.06744752079248428, -0.3986457884311676, -0.11061543971300125, 0.2058483064174652, -0.08716937899589539, 0.6416098475456238, -0.04900269955396652, 0.0280471108853817, 0.42916005849838257, -0.15325504541397095, 0.04093474522233009, 0.10597873479127884, -0.09516562521457672, 0.16639360785484314, -0.060598500072956085, -0.27428698539733887, 0.059949297457933426, 0.717934250831604, 0.2591703534126282, 0.03375529497861862, -0.3580535352230072, -0.1386798620223999, 0.03606738895177841, -0.20098799467086792, 0.030672242864966393, -0.13138806819915771, -0.3030749261379242, 0.475686639547348, 0.23235949873924255, -0.01097577903419733, 0.0442761555314064, 0.02305295132100582, -0.07182201743125916, -0.007515151519328356, 0.2215399146080017, 0.10345771908760071, -0.045392174273729324, -0.2280322015285492], "sparse_embedding": null}, {"id": "df42bdc070f382ad8d804ff00c06958915aa09c1ba3033c1b62631ad377a15a8", "content": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.11764v1", "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs", "content": "http://arxiv.org/pdf/2402.11764v1", "datetime": "2024-02-19 01:28:48", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting advancements in the world of AI and debiasing LLMs! \ud83e\udd16 This groundbreaking work introduces a novel approach using ChatGPT to generate synthetic training data, enhancing the debiasing of Large Language Models. \n\nCheck out the full study here: http://arxiv.org/abs/2402.11764v1\n\nKey findings include:\n1\ufe0f\u20e3 Efficient production of high-quality training data for debiasing LLMs using ChatGPT.\n2\ufe0f\u20e3 Surpassing existing datasets in debiasing performance while preserving internal LLM knowledge.\n3\ufe0f\u20e3 Generalizability across categories, effectively mitigating various biases, including intersectional ones.\n\nThese results highlight the potential of synthetic data in promoting fairness in LLMs with minimal retraining costs. A must-read for all tech enthusiasts and AI professionals! \ud83c\udf10\ud83d\udca1 #AI #LLMs #Debiasing #ChatGPT #TechInnovation", "x": "\"Exciting research on using ChatGPT to enhance debiasing of Large Language Models (LLMs)! This innovative approach generates synthetic training data for efficient debiasing, surpassing existing datasets in performance. Learn more at: http://arxiv.org/abs/2402.11764v1 #AI #NLP #LLMs #Debiasing\"", "source_id": "34b57431f74c4136b0aa83972ca3e891431bc21be5ddcb028652bae40ae1aff8", "page_number": 1}, "score": null, "embedding": [-0.32588520646095276, 0.051456186920404434, 0.17917366325855255, -0.22951076924800873, 0.117042176425457, 0.14475078880786896, -0.34830811619758606, 0.06395481526851654, 0.18563805520534515, -0.21027077734470367, 0.11639576405286789, -0.3040435314178467, 0.2705221474170685, 0.2910913825035095, 0.30288541316986084, 0.12289630621671677, -0.09013062715530396, 0.11092410981655121, -0.04692119359970093, -0.02346149832010269, 0.04542367905378342, -0.2352202832698822, -0.013926483690738678, -0.07263044267892838, -0.01783851534128189, -0.0979059636592865, 0.012478243559598923, 0.04576390981674194, -0.36341941356658936, -1.368696689605713, 0.38847318291664124, 0.1386776864528656, 0.31665974855422974, -0.11464003473520279, -0.2713668942451477, -0.08210886269807816, -0.24885667860507965, 0.20234858989715576, -0.28503385186195374, 0.29566627740859985, -0.07431003451347351, 0.08717773109674454, -0.043008264154195786, -0.23199553787708282, -0.1902022361755371, -0.2423349916934967, -0.2833353579044342, 0.10521257668733597, -0.8590225577354431, -0.10191581398248672, -0.09175914525985718, -0.206577330827713, 0.17945407330989838, -0.04750678315758705, 0.09825583547353745, 0.13287247717380524, 0.16766521334648132, 0.21588070690631866, -0.0024898441042751074, 0.04818909242749214, 0.18430325388908386, 0.16828322410583496, -0.8503208160400391, 0.27962765097618103, -0.15636982023715973, 0.315446138381958, -0.07197972387075424, 0.09036200493574142, 0.10139772295951843, 0.08255224674940109, 0.0651211366057396, 0.07053831219673157, 0.38835835456848145, 0.14494937658309937, 0.297570139169693, 0.37333643436431885, 0.09044014662504196, 0.015277200378477573, 0.23833608627319336, -0.0803898423910141, -0.04617784544825554, -0.03669750690460205, -0.017210938036441803, -0.1693502813577652, -0.04569137841463089, -0.17417578399181366, 0.12867599725723267, -0.07445548474788666, 0.03797706216573715, -0.12783558666706085, -0.15560874342918396, -0.11343099176883698, 0.04201935604214668, 0.02218504250049591, -0.07609432935714722, 0.13019824028015137, -0.018224455416202545, 0.07655686885118484, -0.18733073770999908, 0.7223085165023804, -0.36116132140159607, -0.04461825266480446, -0.3337952196598053, 0.08075466752052307, 0.15521518886089325, -0.27638378739356995, -0.25792697072029114, -0.305022656917572, -0.09612582623958588, -0.033016227185726166, -0.11618635058403015, -0.07134752720594406, 0.11058133095502853, -0.2358003556728363, -0.08530714362859726, 0.16366371512413025, 0.40541815757751465, -0.12099892646074295, -0.16998596489429474, 0.2464878112077713, -0.4958495497703552, 0.34001290798187256, 0.043201252818107605, -0.10014329850673676, 0.1450165957212448, -0.12764357030391693, -0.08505313843488693, 0.6038832664489746, 0.2294897884130478, 0.02282373048365116, 0.15016239881515503, 0.05621354654431343, -0.49052053689956665, -0.0209647323936224, 0.24640455842018127, -0.05564427375793457, 0.1637762039899826, -0.05653715878725052, -0.018783770501613617, 0.1657085418701172, -0.24588564038276672, 0.07139962911605835, 0.15583844482898712, -0.039185017347335815, -0.4650760293006897, 0.4913884103298187, -0.10252974182367325, -0.06718135625123978, -0.1736762374639511, -0.2719017267227173, 0.025988630950450897, 0.20208734273910522, -0.04521575942635536, -0.33948007225990295, 0.33427974581718445, 0.3799770474433899, 0.09871029853820801, 0.281596302986145, -0.364315390586853, -0.05512901768088341, 0.01629631780087948, -0.05234672129154205, 0.03513065725564957, 0.6477655172348022, -0.16995391249656677, -0.3173379898071289, -0.3955928683280945, 0.15259429812431335, 0.23155353963375092, -0.11019016802310944, 0.18231409788131714, -0.10002958029508591, 0.08542238920927048, 0.0071565574035048485, -0.12326250970363617, 0.06384649872779846, -0.7125979661941528, 0.04697170853614807, -0.22725917398929596, 0.2478177845478058, 0.23641876876354218, -0.20152215659618378, -0.11491342633962631, 0.2465081363916397, -0.27203017473220825, -0.3202401399612427, 9.825779852690175e-05, -0.07429111003875732, 0.2089412659406662, 0.15946705639362335, -0.2262963205575943, 0.15104277431964874, 0.18394793570041656, -0.06205473095178604, -0.038176700472831726, -0.23064495623111725, -0.271366685628891, 0.11925449222326279, -0.1448030024766922, 0.00850458163768053, 0.15921244025230408, 0.15296316146850586, -0.25500693917274475, 0.15356482565402985, 0.09767599403858185, -0.10157442837953568, 0.20595204830169678, -0.12488822638988495, 0.48324933648109436, 0.2324099838733673, -0.20093435049057007, 0.026275798678398132, 0.27074408531188965, -0.13025091588497162, 0.0343756340444088, -0.08421745151281357, 0.25944578647613525, 0.4651089012622833, -0.06864844262599945, 0.14965377748012543, -0.07425045967102051, 0.005200178828090429, -0.17853988707065582, -1.261063814163208, -0.21308015286922455, 0.25569236278533936, 0.17137621343135834, -0.0209174994379282, -0.2616691589355469, 0.29086828231811523, 0.016468171030282974, 0.32817575335502625, 0.5816035270690918, 0.24921226501464844, 0.099449023604393, -0.07913340628147125, 0.09466169029474258, 0.19046801328659058, 0.06182221695780754, -0.08513641357421875, 0.17225027084350586, -0.25100672245025635, 0.2010205239057541, -0.12364061921834946, 0.23108893632888794, 0.4050254821777344, -0.4478190541267395, 0.2469176948070526, -0.08966434001922607, 0.7842197418212891, -0.32644128799438477, 0.19434809684753418, -0.03016890399158001, 0.2897610068321228, 0.30291748046875, 0.07076788693666458, -0.4925413131713867, 0.5288856625556946, 0.002056524623185396, 0.3463629186153412, -0.039753902703523636, 0.23143979907035828, -0.02279515564441681, -0.16801582276821136, 0.0040715839713811874, 0.010243145748972893, -0.7246937155723572, -0.2325892448425293, 0.05941680073738098, -0.03919592872262001, -0.26262640953063965, -0.3267791271209717, 0.1961350291967392, 0.03362526744604111, -0.12114184349775314, 0.4477417469024658, -0.013350648805499077, -0.1728479564189911, -0.020860224962234497, -0.9123855233192444, 0.052164457738399506, -0.3676987886428833, -0.13986194133758545, 0.20017562806606293, -0.1320909708738327, -0.06305575370788574, -0.2741776406764984, 0.142153799533844, -0.01292519923299551, 0.062065206468105316, -0.1049845814704895, 0.0463947057723999, 0.0026618249248713255, -0.18198612332344055, 0.6316737532615662, 0.07281547784805298, 0.17332004010677338, -0.008176534436643124, 0.25423893332481384, -0.23671236634254456, -0.2648526728153229, -0.26705268025398254, -0.21516770124435425, 0.7624513506889343, 0.2289218306541443, 0.15375131368637085, 0.23978441953659058, -0.03788452222943306, 0.0650193840265274, 0.2565004825592041, -0.15334200859069824, 0.33097317814826965, 0.169430211186409, -0.1426200270652771, 0.006278471555560827, -0.42821070551872253, -0.1717475950717926, 0.10583905875682831, -0.05688587948679924, -1.094693899154663, -0.11736875027418137, -0.2270958572626114, 0.16866111755371094, 0.049047473818063736, -0.03797833248972893, 0.06025581434369087, -0.07258311659097672, -0.12108274549245834, -0.05459333211183548, -0.12400738894939423, 0.4996959865093231, 0.28875473141670227, -0.10817933082580566, 0.12192212790250778, -0.14937031269073486, 0.3958459198474884, -0.11794828623533249, -0.020740481093525887, -0.3301783502101898, -0.0035616562236100435, -0.11372645199298859, 0.9268903136253357, 0.2042960524559021, 0.07458925247192383, 0.19828300178050995, -0.22934946417808533, 0.0853203758597374, -0.0003293656336609274, 0.07731377333402634, -0.2114633321762085, 0.25683823227882385, 0.6746576428413391, -0.10606779158115387, 0.05072480067610741, 0.4938308596611023, -0.1738308221101761, -0.11823868751525879, 0.3289543092250824, 0.0876530110836029, 0.1058540940284729, 0.19095753133296967, 0.24807460606098175, 0.03962714225053787, 0.5847651362419128, 0.10107679665088654, -0.15597569942474365, -0.34947410225868225, -0.065751813352108, 0.058750543743371964, -0.14361701905727386, 0.2033316045999527, -0.15663428604602814, -0.09334612637758255, 0.1470021903514862, -0.05129378288984299, 0.0700317993760109, -0.02478516660630703, 0.03505818545818329, -0.2259977161884308, 0.16214047372341156, -0.12357186526060104, -0.2121932953596115, -0.018310319632291794, -0.3299744129180908], "sparse_embedding": null}, {"id": "c54ef7e3de06d8aac798b4e257172f49abd7ceda66d65c78becdfeb7c1339c5d", "content": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.07849v2", "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations", "content": "http://arxiv.org/pdf/2310.07849v2", "datetime": "2023-10-13 01:31:59", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting new study alert! Researchers dive into the world of Large Language Models (LLMs) for synthetic data generation in text classification models. Check out the latest findings on the impact of subjectivity on model performance when using LLM-generated synthetic data. \n\nCurious to learn more? Dive into the full study here: \nhttp://arxiv.org/abs/2310.07849v2\n\n#AI #NLP #LLMs #TextClassification #Research #Tech #ArtificialIntelligence #MachineLearning", "x": "\ud83d\ude80 New research alert! How effective are large language models in generating synthetic data for text classification models? Check out the findings on the impact of subjectivity on model performance: http://arxiv.org/abs/2310.07849v2 #AI #NLP #LLMs #DataSynthesis", "source_id": "0a0ef655e96989d0b89c12d821dc8b2b1557ea55fa385dd4246d055b467d4575", "page_number": 1}, "score": null, "embedding": [-0.06270274519920349, 0.14129891991615295, 0.13971006870269775, 0.024773020297288895, 0.2409205138683319, 0.11751479655504227, -0.44341766834259033, 0.08317718654870987, 0.14975927770137787, -0.2747926414012909, -0.06805751472711563, -0.10084991902112961, 0.15312933921813965, 0.3222736418247223, 0.12334565818309784, 0.18766215443611145, -0.027098650112748146, -0.10255811363458633, -0.19528613984584808, -0.13954007625579834, 0.3490787148475647, -0.009035090915858746, -0.027498982846736908, -0.011638441123068333, 0.04519539326429367, -0.1254495233297348, -0.16642192006111145, -0.015260660089552402, -0.4111778140068054, -1.558991551399231, 0.027973683550953865, -0.02944852039217949, 0.4124736189842224, 0.07784028351306915, -0.15026003122329712, -0.004468750208616257, -0.2566208839416504, 0.3089997172355652, -0.10015732795000076, 0.1911439746618271, -0.23413525521755219, -0.005555801093578339, -0.23352089524269104, -0.15534308552742004, 0.14520986378192902, -0.24576517939567566, -0.15772554278373718, -0.15424954891204834, -0.5845125317573547, 0.036552995443344116, -0.16836108267307281, -0.11295165866613388, 0.04163862764835358, 0.3933827877044678, 0.02209611050784588, 0.20567168295383453, 0.2557460069656372, 0.013030116446316242, 0.19616667926311493, 0.005080816335976124, 0.17307545244693756, 0.4204321801662445, -1.0848811864852905, 0.10424838960170746, -0.10688749700784683, 0.21349108219146729, -0.20156753063201904, 0.13180671632289886, 0.20496520400047302, 0.16095541417598724, -0.16840411722660065, -0.054420728236436844, 0.2725929915904999, 0.1213286742568016, 0.3674931824207306, 0.2906312644481659, 0.1385955512523651, -0.059025056660175323, 0.2702150344848633, -0.06163667514920235, 0.12258514761924744, -0.17685210704803467, -0.03566912189126015, -0.43435290455818176, -0.10059576481580734, -0.18975915014743805, 0.09130404144525528, -0.2649790644645691, 0.10698853433132172, 0.08917167782783508, -0.11869445443153381, 0.005404900759458542, -0.31156331300735474, 0.2352982461452484, -0.20408327877521515, 0.08057409524917603, 0.03523573651909828, 0.2572343349456787, -0.2613707482814789, 0.6864578723907471, -0.3970365822315216, -0.11143763363361359, -0.13562722504138947, 0.029005950316786766, 0.19067825376987457, -0.15960034728050232, -0.2593405246734619, -0.06744968891143799, -0.21444113552570343, -0.202550008893013, -0.05531404912471771, 0.09342595189809799, -0.4425014853477478, -0.09803635627031326, -0.057978857308626175, -0.18051956593990326, 0.6934233903884888, 0.03646238520741463, -0.1754896342754364, 0.04308097064495087, -0.24119611084461212, 0.10025126487016678, 0.04093211889266968, -0.18864712119102478, 0.03617647662758827, -0.08814411610364914, 0.13664531707763672, 0.5600436329841614, 0.2054157555103302, -0.10364406555891037, 0.09997303783893585, 0.12214008718729019, -0.6969346404075623, -0.07216918468475342, 0.013333569280803204, -0.24294498562812805, 0.05052966997027397, -0.09051694720983505, 0.006758122239261866, 0.2126707136631012, 0.09143522381782532, 0.20153601467609406, 0.07060746103525162, 0.06496873497962952, -0.6665464639663696, 0.7664282321929932, -0.154449462890625, 0.11751388013362885, -0.26149559020996094, -0.11618376523256302, 0.05774417519569397, 0.2452881634235382, -0.06478971242904663, -0.3447389304637909, 0.274018257856369, 0.2055083066225052, 0.17793084681034088, 0.05935071036219597, -0.4773649573326111, -0.3113879859447479, -0.017750028520822525, 0.0029865822289139032, -0.10153037309646606, 0.8530542850494385, -0.16099411249160767, -0.12886382639408112, -0.25559329986572266, 0.022102240473031998, 0.2527458965778351, -0.07995100319385529, 0.32579505443573, 0.06195077300071716, -0.12004929780960083, 0.23027953505516052, -0.11419341713190079, 0.06435124576091766, -0.6637915968894958, -0.1608750969171524, -0.15968985855579376, 0.16130845248699188, 0.21251355111598969, -0.18767140805721283, -0.27775004506111145, 0.14970043301582336, 0.08295708894729614, -0.16411791741847992, -0.1369430273771286, -0.2506859004497528, 0.19211089611053467, 0.21453550457954407, 0.08355562388896942, 0.11340274661779404, 0.07057289779186249, -0.08084657043218613, -0.06701972335577011, -0.20852522552013397, -0.09483729302883148, 0.018609285354614258, 0.06894030421972275, -0.34860390424728394, 0.0005555838579311967, 0.4815407991409302, 0.19412128627300262, -0.07382447272539139, -0.026267796754837036, 0.02480335533618927, 0.023395109921693802, 0.05755120888352394, 0.4463590383529663, 0.32079362869262695, -0.4372434914112091, -0.06471089273691177, 0.30692580342292786, -0.051021918654441833, -0.06673339009284973, 0.032973144203424454, 0.33929312229156494, 0.2763749957084656, -0.3851875066757202, 0.2492460161447525, 0.01356489397585392, 0.23306594789028168, -0.14977146685123444, -1.3114259243011475, -0.1472245305776596, 0.10630930215120316, 0.1777215301990509, 0.10265380144119263, -0.2829647660255432, -0.04397294670343399, 0.21141701936721802, 0.07169230282306671, 0.48993781208992004, 0.21005931496620178, 0.1189265325665474, -0.2923693060874939, 0.04798387363553047, -0.048500221222639084, 0.04577048495411873, 0.010723423212766647, 0.07038149237632751, -0.26777321100234985, 0.1970251053571701, -0.2573358416557312, 0.02800014428794384, 0.21648457646369934, -0.7389047145843506, 0.10002664476633072, -0.04885248467326164, 0.7000568509101868, -0.36555933952331543, 0.14971166849136353, -0.07177667319774628, 0.09157342463731766, 0.3008616268634796, -0.18164938688278198, -0.44704940915107727, 0.5580542683601379, -0.25778478384017944, 0.21896669268608093, 0.0734545886516571, 0.007310864515602589, -0.13762494921684265, -0.1753910630941391, 0.0020382509101182222, 0.1679728478193283, -0.8076971173286438, -0.20298941433429718, 0.07002092897891998, -0.09104476869106293, -0.12311484664678574, -0.579216480255127, 0.2673088014125824, 0.05670662596821785, 0.23181433975696564, 0.5310488939285278, -0.02039857767522335, -0.15544643998146057, 0.027747297659516335, -0.7601108551025391, 0.21640828251838684, -0.23190926015377045, -0.13446076214313507, 0.35909923911094666, -0.14846716821193695, 0.15931269526481628, -0.33118197321891785, 0.2056354582309723, -0.2940506637096405, -0.24237239360809326, -0.07731153070926666, 0.2207004576921463, -0.047722555696964264, -0.15622638165950775, 0.6295284628868103, -0.008708780631422997, 0.01906733773648739, 0.23553505539894104, 0.07374893128871918, -0.36211323738098145, -0.1332859843969345, -0.18783055245876312, 0.0025451229885220528, 0.45124542713165283, 0.379975825548172, 0.34646540880203247, 0.17578069865703583, 0.08974354714155197, 0.023593194782733917, 0.4204519987106323, 0.10705135762691498, 0.17138563096523285, 0.31022849678993225, 0.07138791680335999, 0.2361060231924057, -0.34942787885665894, -0.024865267798304558, 0.2261020541191101, 0.00872072484344244, -1.1068063974380493, -0.11723178625106812, -0.08120182901620865, 0.5220834612846375, -0.025381125509738922, -0.09126028418540955, 0.15349337458610535, -0.1516149938106537, 0.21919190883636475, 0.2528747022151947, 0.03316116705536842, 0.22964490950107574, 0.30448147654533386, -0.016192035749554634, 0.052628159523010254, -0.06439393013715744, 0.40737292170524597, -0.3319091498851776, 0.16918030381202698, -0.3713148832321167, 0.041701540350914, -0.002915392629802227, 1.0541191101074219, 0.03656702861189842, -0.029520146548748016, 0.21160760521888733, -0.18855388462543488, 0.06872816383838654, -0.007942460477352142, -0.09040651470422745, 0.04540999233722687, 0.18835483491420746, 0.9050746560096741, 0.1542350798845291, 0.010140911675989628, 0.5730359554290771, -0.21247784793376923, -0.010911116376519203, 0.046662356704473495, 0.05336993932723999, 0.4335554838180542, 0.17546112835407257, -0.2795095145702362, -0.18809786438941956, 0.4931975305080414, -0.0016552845481783152, 0.132182776927948, -0.3899199366569519, -0.1828104853630066, -0.005727567709982395, -0.26025664806365967, 0.11296942830085754, -0.1726807802915573, 0.10460752248764038, 0.14781902730464935, 0.2634488642215729, 0.11884079873561859, -0.12111473828554153, 0.04975264519453049, -0.14756746590137482, 0.10776016861200333, -0.1963730901479721, -0.11634528636932373, 0.11153899878263474, -0.32534563541412354], "sparse_embedding": null}, {"id": "b4600927aee0adb0b52a1af5aa725844d05c870dff2122247f964bea6e23a536", "content": "As large language models (LLMs) demonstrate unparalleled performance and\ngeneralization ability, LLMs are widely used and integrated into various\napplications. When it comes to sensitive domains, as commonly described in\nfederated learning scenarios, directly using external LLMs on private data is\nstrictly prohibited by stringent data security and privacy regulations. For\nlocal clients, the utilization of LLMs to improve the domain-specific small\nlanguage models (SLMs), characterized by limited computational resources and\ndomain-specific data, has attracted considerable research attention. By\nobserving that LLMs can empower domain-specific SLMs, existing methods\npredominantly concentrate on leveraging the public data or LLMs to generate\nmore data to transfer knowledge from LLMs to SLMs. However, due to the\ndiscrepancies between LLMs' generated data and clients' domain-specific data,\nthese methods cannot yield substantial improvements in the domain-specific\ntasks. In this paper, we introduce a Federated Domain-specific Knowledge\nTransfer (FDKT) framework, which enables domain-specific knowledge transfer\nfrom LLMs to SLMs while preserving clients' data privacy. The core insight is\nto leverage LLMs to augment data based on domain-specific few-shot\ndemonstrations, which are synthesized from private domain data using\ndifferential privacy. Such synthetic samples share similar data distribution\nwith clients' private data and allow the server LLM to generate particular\nknowledge to improve clients' SLMs. The extensive experimental results\ndemonstrate that the proposed FDKT framework consistently and greatly improves\nSLMs' task performance by around 5\\% with a privacy budget of less than 10,\ncompared to local training on private data.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2405.14212v1", "title": "Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data", "content": "http://arxiv.org/pdf/2405.14212v1", "datetime": "2024-05-23 06:14:35", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting news in the field of AI and privacy protection! \ud83d\udee1\ufe0f Our latest research introduces the Federated Domain-specific Knowledge Transfer (FDKT) framework, enabling the transfer of domain-specific knowledge from large language models (LLMs) to small language models (SLMs) while safeguarding clients' data privacy. \n\n\ud83d\udcc8 The FDKT framework leverages LLMs to augment data based on domain-specific few-shot demonstrations synthesized from private domain data using differential privacy. This approach ensures that the synthetic samples share a similar data distribution with clients' private data, leading to significant improvements in SLMs' task performance by approximately 5% with a privacy budget of less than 10, compared to local training on private data.\n\n\ud83d\udd0d Dive deeper into the results and methodology by checking out the full research paper at: http://arxiv.org/abs/2405.14212v1\n\n#AI #PrivacyProtection #LLMs #FDKT #Research #DataPrivacy #TechInnovation", "x": "\ud83d\ude80 Exciting research in enhancing domain-specific small language models (SLMs) using Federated Domain-specific Knowledge Transfer (FDKT) framework! \ud83e\udd16\ud83d\udcda Learn how to transfer knowledge from LLMs to SLMs while preserving data privacy: http://arxiv.org/abs/2405.14212v1 #AI #NLP #LLMs #DataPrivacy #Research #FDKT \ud83d\udcca\ud83d\udd12", "source_id": "d8b1f57372d7dde3c7ebdd27d72d39a2a697cc72c963c66ec55138531066e6c3", "page_number": 1}, "score": null, "embedding": [-0.0018664882518351078, -0.28536126017570496, -0.03459811583161354, -0.39022448658943176, 0.039996881037950516, -0.03395596146583557, -0.24697910249233246, 0.008915548212826252, 0.3412875533103943, -0.3483099341392517, -0.036198023706674576, -0.035585202276706696, 0.34993329644203186, 0.37368881702423096, -0.007716996595263481, 0.27540600299835205, -0.060614340007305145, 0.14178428053855896, -0.196101576089859, 0.14143170416355133, 0.4379614591598511, -0.22246702015399933, -0.02283213660120964, -0.1030689924955368, -0.14378666877746582, 0.11701454967260361, -0.3223714828491211, -0.08251363039016724, -0.4621949791908264, -1.228664755821228, 0.06191379204392433, -0.3149498403072357, 0.03633798286318779, 0.014492516405880451, -0.03228970617055893, -0.03823177516460419, -0.33058273792266846, 0.19549334049224854, -0.18031227588653564, 0.2902936339378357, 0.1207011267542839, -0.018939055502414703, 0.07678063958883286, 0.10348554700613022, -0.021663827821612358, -0.44227465987205505, -0.151572123169899, -0.045327868312597275, -0.6268388628959656, -0.1609935760498047, -0.039731547236442566, -0.22318756580352783, -0.024956781417131424, 0.293062686920166, -0.01919306069612503, 0.24323870241641998, 0.20530082285404205, 0.114844910800457, 0.15324246883392334, 0.18319623172283173, 0.14639630913734436, 0.5754050612449646, -0.9253599047660828, 0.2997191846370697, -0.05385162681341171, 0.5582111477851868, 0.02194773592054844, -0.02624712511897087, 0.1246078684926033, 0.04185853525996208, -0.08274951577186584, 0.25589197874069214, 0.14958056807518005, 0.2758711874485016, 0.3111904561519623, 0.07408344000577927, -0.10494665801525116, 0.08061373978853226, 6.519559974549338e-05, 0.10008919984102249, 0.17323584854602814, -0.046710461378097534, 0.1558254063129425, -0.341070294380188, -0.2813379466533661, -0.008142131380736828, -0.05625360459089279, -0.3802940249443054, 0.1969297081232071, -0.08421307057142258, -0.31627002358436584, -0.07093087583780289, -0.12952670454978943, 0.08496925979852676, -0.06388124823570251, 0.06497722864151001, 0.13526834547519684, 0.12184558063745499, -0.2712373733520508, 0.5901142358779907, -0.16152429580688477, 0.22616752982139587, -0.18734164535999298, -0.08281099796295166, 0.19751018285751343, 0.10373687744140625, -0.08779538422822952, -0.08132192492485046, -0.01766217313706875, 0.07416125386953354, -0.13550080358982086, -0.019975952804088593, -0.19286103546619415, -0.12187563627958298, 0.026637403294444084, -0.08329260349273682, 0.433657169342041, 0.18755455315113068, -0.23190279304981232, -0.1089581698179245, -0.3183828890323639, 0.17969754338264465, 0.0712619423866272, -0.06242280825972557, 0.1997007131576538, -0.07848446816205978, 0.04688981547951698, 0.37436166405677795, 0.20216840505599976, 0.24755463004112244, 0.3010520339012146, -0.1831343024969101, -0.34971654415130615, -0.29911088943481445, 0.20459719002246857, -0.10300134122371674, -0.06593898683786392, -0.23976564407348633, -0.05938855558633804, 0.09369409084320068, -0.028247429057955742, -0.03039564937353134, 0.3474371135234833, -0.09208719432353973, -0.1865982711315155, 0.6941249370574951, 0.1644814908504486, 0.06874614208936691, -0.4402334988117218, 0.0573623962700367, 0.2692042589187622, 0.3419731855392456, -0.17056021094322205, -0.26807376742362976, 0.21926164627075195, -0.009745952673256397, 0.36353135108947754, 0.2585037350654602, -0.3672780990600586, -0.1603499799966812, 0.2321028709411621, -0.0066455816850066185, 0.21106761693954468, 0.8135752081871033, -0.1387784630060196, -0.504119873046875, -0.09140407294034958, 0.2535920739173889, 0.26875200867652893, -0.3861630856990814, 0.04775552824139595, 0.0358419232070446, 0.020295122638344765, -0.15400028228759766, -0.27913305163383484, 0.019445544108748436, -0.5610476136207581, -0.06975193321704865, -0.14008352160453796, 0.03782747685909271, -0.07923828065395355, -0.1679447740316391, 0.0781046599149704, 0.10776612907648087, -0.00943643320351839, -0.2856330871582031, -0.009048286825418472, -0.2589224874973297, -0.15271425247192383, 0.16019944846630096, -0.6073534488677979, 0.25818532705307007, -0.1859704852104187, -0.06440958380699158, -0.05282454565167427, -0.07639405131340027, -0.19144895672798157, 0.1256810426712036, 0.04189349338412285, -0.15607839822769165, 0.1702508181333542, 0.43651849031448364, -0.050146155059337616, 0.0030639697797596455, -0.06852306425571442, -0.08631126582622528, 0.09061672538518906, -0.07052945345640182, 0.008337709121406078, 0.22624611854553223, -0.2909858524799347, 0.16017583012580872, 0.06544452160596848, -0.08224746584892273, -0.1512220948934555, 0.0931476354598999, 0.2558528184890747, 0.23493321239948273, -0.08136893063783646, -0.11061280220746994, 0.16442252695560455, 0.3064942955970764, -0.2697668671607971, -1.081972360610962, -0.1622379869222641, -0.001165009685792029, 0.07061809301376343, 0.3798975348472595, -0.2837832272052765, 0.13291357457637787, 0.18338046967983246, 0.12114433199167252, 0.33206936717033386, 0.45937079191207886, -0.14575223624706268, -0.43504586815834045, 0.2519753873348236, -0.01987687312066555, -0.12025386840105057, 0.05041075870394707, 0.18905553221702576, -0.10690484195947647, 0.24657483398914337, -0.20637980103492737, 0.0640764981508255, 0.3054620325565338, -0.582862377166748, 0.15220873057842255, -0.1110413447022438, 0.777820885181427, -0.27181321382522583, 0.06413227319717407, -0.3847557306289673, 0.1366218477487564, 0.2241085022687912, 0.1346813589334488, -0.6637555956840515, 0.19877523183822632, -0.20870846509933472, 0.13897062838077545, 0.1448378562927246, 0.2022646814584732, -0.06444039195775986, -0.01814867928624153, -0.0800171047449112, 0.11319346725940704, -0.6580791473388672, -0.2843056619167328, -0.017341021448373795, -0.161077082157135, -0.18204215168952942, -0.1974419504404068, 0.41611814498901367, 0.0008652808028273284, 0.0004314785765018314, 0.431703120470047, 0.022824835032224655, -0.09337665885686874, -0.09430709481239319, -0.31699565052986145, -0.040385354310274124, -0.046786025166511536, 0.1944768875837326, 0.06855501979589462, -0.17938131093978882, 0.06755605340003967, -0.35940080881118774, 0.23009246587753296, -0.5200045108795166, -0.13341552019119263, 0.011629948392510414, 0.34154894948005676, -0.12603461742401123, -0.11578470468521118, 0.5252876281738281, 0.2854141592979431, 0.2990737855434418, -0.05542850121855736, 0.2619436979293823, -0.18392308056354523, 0.09554101526737213, -0.5701740384101868, 0.03828045353293419, 0.7851638197898865, 0.12759895622730255, 0.12613926827907562, 0.16988828778266907, 0.16533394157886505, 0.015648256987333298, 0.36821576952934265, 0.341086208820343, -0.007284143008291721, -0.028501441702246666, -0.1544509381055832, -0.1855214685201645, -0.27995654940605164, -0.10591987520456314, 0.31320393085479736, -0.12420165538787842, -1.187362551689148, -0.15998056530952454, -0.24923844635486603, 0.3358149230480194, -0.09949982166290283, 0.10111886262893677, 0.33313459157943726, -0.062284089624881744, 0.0318102203309536, 0.21185985207557678, -0.017580842599272728, 0.18982431292533875, 0.20914682745933533, -0.016342034563422203, 0.18711133301258087, -0.0972285196185112, 0.2706458866596222, -0.040741149336099625, -0.12210414558649063, -0.33142784237861633, -0.14289197325706482, 0.304036408662796, 1.063277244567871, 0.013670734129846096, -0.1105443462729454, 0.09344141185283661, -0.001898284419439733, 0.29410016536712646, 0.03462962433695793, 0.070943184196949, -0.06501179933547974, 0.14653079211711884, 0.3886248469352722, 0.014470587484538555, 0.007521524094045162, 0.46660947799682617, -0.07186967134475708, -0.14854401350021362, 0.10957928001880646, 0.10879562795162201, 0.3840491473674774, -0.37589457631111145, 0.015595493838191032, 0.33202502131462097, 0.5523245334625244, 0.1039910688996315, -0.0010345788905397058, -0.3257797658443451, -0.08004741370677948, 0.011420954018831253, -0.11562198400497437, -0.3104763925075531, 0.055321916937828064, -0.13842365145683289, 0.2566399574279785, -0.08961454778909683, 0.16727067530155182, 0.06798906624317169, -0.09915726631879807, -0.14215803146362305, 0.12013955414295197, -0.20487233996391296, -0.27604371309280396, 0.0023189750500023365, -0.2501605749130249], "sparse_embedding": null}, {"id": "3713969ed3f7ab6fd67b23404f5d0a1a80063500a2bf6c926288545d06721030", "content": "Text data has become extremely valuable due to the emergence of machine\nlearning algorithms that learn from it. A lot of high-quality text data\ngenerated in the real world is private and therefore cannot be shared or used\nfreely due to privacy concerns. Generating synthetic replicas of private text\ndata with a formal privacy guarantee, i.e., differential privacy (DP), offers a\npromising and scalable solution. However, existing methods necessitate DP\nfinetuning of large language models (LLMs) on private data to generate DP\nsynthetic data. This approach is not viable for proprietary LLMs (e.g.,\nGPT-3.5) and also demands considerable computational resources for open-source\nLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)\nalgorithm to generate DP synthetic images with only API access to diffusion\nmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, that\napplies to the complex setting of text. We use API access to an LLM and\ngenerate DP synthetic text without any model training. We conduct comprehensive\nexperiments on three benchmark datasets. Our results demonstrate that Aug-PE\nproduces DP synthetic text that yields competitive utility with the SOTA DP\nfinetuning baselines. This underscores the feasibility of relying solely on API\naccess of LLMs to produce high-quality DP synthetic texts, thereby facilitating\nmore accessible routes to privacy-preserving LLM applications. Our code and\ndata are available at https://github.com/AI-secure/aug-pe.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.01749v1", "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text", "content": "http://arxiv.org/pdf/2403.01749v1", "datetime": "2024-03-04 05:57:50", "query": "synthetic data generation llms", "linkedin": "\ud83c\udf1f Exciting News in the World of AI and Privacy-Preserving Technologies! \ud83c\udf1f\n\nText data privacy is a crucial concern in the age of machine learning. Check out the groundbreaking work by Lin et al. introducing the Aug-PE algorithm, designed to generate differentially private (DP) synthetic text data using API access to large language models (LLMs) without the need for model training.\n\nCurious to learn more about how Aug-PE can revolutionize privacy-preserving LLM applications? Dive into the details and explore the impressive results from comprehensive experiments on benchmark datasets. The study showcases that Aug-PE produces high-quality DP synthetic text comparable to state-of-the-art DP finetuning baselines.\n\nReady to explore the future of privacy-preserving AI applications? Access the code and data at: [https://github.com/AI-secure/aug-pe](http://arxiv.org/abs/2403.01749v1)\n\n#AI #PrivacyPreservation #MachineLearning #DifferentialPrivacy #AugmentedPE #TechInnovation", "x": "\ud83d\ude80 Exciting new research alert! Aug-PE algorithm enables generating high-quality differentially private synthetic text without model training. Check out the groundbreaking study by Lin et al. (2024) and explore the code and data at: \n\n\ud83d\udd17 http://arxiv.org/abs/2403.01749v1\n\n#AI #NLP #LLMs #PrivacyPreservation #MachineLearning", "source_id": "524aed14c0346e98f527a3dd7732e5a88b79652c0e80b51768334da3f0850d98", "page_number": 1}, "score": null, "embedding": [-0.4291870594024658, -0.10987675935029984, -0.10655142366886139, -0.2902047634124756, 0.06917479634284973, 0.10741641372442245, -0.3048669397830963, 0.13748416304588318, 0.3194192945957184, -0.20526398718357086, 0.11524054408073425, -0.21398039162158966, 0.26470956206321716, -0.03159294277429581, 0.10704893618822098, 0.29436033964157104, -0.07339855283498764, -0.1275624930858612, -0.2935941219329834, 0.023414863273501396, 0.467397540807724, -0.07375524193048477, 0.02591385506093502, -0.4178381562232971, -0.13314229249954224, 0.02522156946361065, -0.10566651821136475, 0.05321599915623665, -0.2521522641181946, -1.3572381734848022, 0.24597328901290894, -0.24317197501659393, 0.24765080213546753, -0.09379339218139648, -0.16382862627506256, 0.030122822150588036, -0.37700414657592773, 0.40784740447998047, -0.2829825282096863, 0.16309323906898499, -0.20305736362934113, -0.11165878921747208, -0.18599894642829895, 0.004911675583571196, 0.1675688922405243, -0.23902961611747742, -0.3163624405860901, -0.08138668537139893, -0.47089409828186035, -0.049021657556295395, 0.00911865383386612, 0.19452030956745148, -0.04176080599427223, 0.2863551378250122, 0.0995720699429512, 0.04512610286474228, 0.3116237223148346, 0.009745021350681782, 0.024864155799150467, 0.1554258018732071, 0.045125335454940796, 0.4537762999534607, -0.9542045593261719, 0.30536195635795593, -0.10309531539678574, 0.37561771273612976, -0.3148249089717865, 0.01182633824646473, 0.3182671070098877, -0.17746366560459137, -0.034210409969091415, 0.3457828462123871, 0.19274236261844635, 0.11701195687055588, 0.2967613935470581, 0.4526892900466919, 0.14556987583637238, -0.1896955519914627, 0.18537898361682892, -0.0026341222692281008, 0.10411114990711212, 0.06697925180196762, 0.03545455262064934, -0.14898960292339325, -0.12389160692691803, -0.044797539710998535, 0.13563811779022217, -0.30560970306396484, 0.2439599484205246, -0.20049236714839935, -0.21346507966518402, 0.003440586384385824, -0.023229064419865608, 0.1083725094795227, -0.17393572628498077, 0.14789240062236786, -0.1009591668844223, 0.014839082024991512, -0.1387665569782257, 0.5459895133972168, -0.3220401108264923, 0.08064356446266174, -0.10834489017724991, 0.06329837441444397, 0.17732690274715424, -0.16440263390541077, -0.2024739533662796, -0.13588008284568787, -0.30839791893959045, 0.04414628818631172, 0.09034716337919235, -0.0042667207308113575, -0.02077212929725647, 0.07876367121934891, -0.23202067613601685, -0.10184548795223236, 0.4014061391353607, -0.026573877781629562, -0.11782129853963852, -0.10641442239284515, -0.17336821556091309, 0.2785351872444153, -0.10698242485523224, 0.1374308466911316, 0.15030640363693237, -0.14662933349609375, 0.04933831840753555, 0.5299703478813171, 0.09269892424345016, 0.03547108918428421, 0.13977181911468506, 0.09054730087518692, -0.4407247006893158, -0.26264262199401855, 0.008573111146688461, -0.22195734083652496, -0.179942324757576, -0.21157827973365784, 0.1451796442270279, -0.07002151012420654, -0.16032463312149048, 0.1953897476196289, 0.18572649359703064, -0.009748974815011024, -0.3286805748939514, 0.6647255420684814, 0.24512174725532532, 0.20701999962329865, -0.3691357970237732, -0.3292267918586731, 0.1739971935749054, 0.10997285693883896, -0.1480848491191864, -0.15620344877243042, 0.41794437170028687, -0.129311665892601, 0.16523617506027222, 0.30370956659317017, -0.32480406761169434, 0.11734195798635483, 0.06160939857363701, 0.11047782003879547, -0.029638485983014107, 1.1148216724395752, -0.15124404430389404, -0.491580992937088, 0.02030581422150135, 0.09326091408729553, 0.24673138558864594, -0.1521129310131073, 0.2810158431529999, -0.014492124319076538, -0.0927080363035202, 0.2647082209587097, -0.14085876941680908, 0.04503662511706352, -0.6340011954307556, -0.11973420530557632, -0.20081526041030884, 0.23059935867786407, -0.2658504247665405, -0.2809734344482422, -0.2305474579334259, 0.14386354386806488, -0.004263830371201038, -0.26218387484550476, -0.08067622780799866, -0.21307089924812317, 0.0563577301800251, 0.3091602027416229, -0.04165572300553322, 0.3113554120063782, -0.2025000900030136, -0.0062160673551261425, -0.11918268352746964, -0.3166021406650543, -0.05704604089260101, 0.08463772386312485, 0.12192314118146896, -0.16213534772396088, -0.05256020277738571, 0.25733765959739685, 0.05651348829269409, -0.17579641938209534, -0.2555276155471802, -0.060671109706163406, 0.033597178757190704, 0.050303276628255844, 0.19239823520183563, 0.3228408396244049, -0.2648454010486603, -0.0837264284491539, 0.17422328889369965, -0.13085836172103882, -0.3300672173500061, 0.23089885711669922, 0.11121384799480438, 0.10290537774562836, -0.18665246665477753, 0.1876850426197052, -0.13844554126262665, 0.14772078394889832, -0.15804730355739594, -1.0019162893295288, -0.24304518103599548, -0.0430770106613636, -0.006932568736374378, 0.2990112900733948, -0.3786335289478302, 0.04134431481361389, -0.17035983502864838, 0.2794581353664398, 0.5465908050537109, 0.511677086353302, -0.01880510151386261, -0.21504449844360352, 0.2691400945186615, -0.06348750740289688, 0.10081207007169724, 0.08856707811355591, 0.32525166869163513, 0.008833813481032848, 0.05369657278060913, -0.3056578040122986, 0.04298987612128258, 0.20433880388736725, -0.6390594244003296, 0.2881365716457367, -0.11434875428676605, 0.8873940706253052, -0.07332039624452591, 0.18176116049289703, -0.027786798775196075, 0.09421129524707794, 0.2616358995437622, -0.05374443158507347, -0.7932963967323303, 0.1744554489850998, -0.12674029171466827, -0.14205028116703033, 0.13017183542251587, 0.15669944882392883, -0.24577219784259796, -0.008466826751828194, 0.1583413928747177, -0.08300764858722687, -0.7071980834007263, -0.14798232913017273, -0.25206419825553894, -0.14316387474536896, -0.1506551206111908, -0.27692902088165283, 0.4335053861141205, 0.4295061230659485, 0.18403953313827515, 0.5424576997756958, -0.10213746875524521, -0.13983695209026337, -0.33935508131980896, -0.4263472259044647, -0.06249784678220749, -0.2568284869194031, -0.0356982946395874, 0.10668236017227173, -0.04564942792057991, -0.02633473090827465, -0.3580663800239563, 0.084502212703228, -0.24960504472255707, -0.09052518010139465, -0.22946877777576447, 0.22510388493537903, -0.023604650050401688, -0.010549165308475494, 0.6323372721672058, 0.028115004301071167, 0.12440488487482071, 0.09227535128593445, 0.17903591692447662, -0.10355297476053238, 0.06712745130062103, -0.2281772345304489, -0.2730778455734253, 0.5558294057846069, 0.40309250354766846, 0.3375866115093231, 0.284316748380661, -0.017635103315114975, 0.35562512278556824, 0.35547178983688354, 0.19088782370090485, 0.05744222179055214, 0.11325077712535858, 0.09826018661260605, 0.1378936916589737, -0.39805611968040466, -0.1277625560760498, 0.3859047591686249, 0.08279023319482803, -1.173597812652588, -0.06245243176817894, -0.23295186460018158, 0.2977347671985626, -0.21436727046966553, 0.15064644813537598, 0.3592328727245331, 0.025720743462443352, -0.04417584463953972, 0.10748082399368286, -0.22466999292373657, 0.21504609286785126, 0.16626189649105072, -0.18065838515758514, 0.29660651087760925, -0.11958146840333939, 0.2998022139072418, -0.05589418485760689, -0.058314140886068344, -0.3450494408607483, 0.09343080222606659, -0.1260197013616562, 1.0226726531982422, -0.27794837951660156, -0.17982834577560425, 0.11474978178739548, -0.00018451843061484396, 0.27729955315589905, -0.20018908381462097, -0.06905008107423782, -0.24543032050132751, 0.1257123351097107, 0.47458240389823914, -0.0738857164978981, 0.18780289590358734, 0.4559614360332489, -0.1848403662443161, -0.05029934644699097, -0.05107022076845169, 0.08456479758024216, 0.24269047379493713, 0.050148479640483856, -0.08686771243810654, 0.0734742134809494, 0.23227143287658691, 0.29064884781837463, -0.01302128192037344, -0.14134006202220917, -0.0383564792573452, 0.08114472776651382, -0.24440282583236694, 0.25465887784957886, -0.2311541736125946, 0.003496558405458927, 0.33859071135520935, 0.37230122089385986, -0.06511722505092621, 0.09196515381336212, 0.029610037803649902, -0.07656556367874146, 0.1386222094297409, -0.13142940402030945, -0.11735451221466064, 0.4006313979625702, -0.19547995924949646], "sparse_embedding": null}, {"id": "b2eae6e6f459f03d6b118f4b9988653e23007b6380e0c3cdd7c9985cf08a437e", "content": "Large language models (LLMs) suffer from catastrophic forgetting during\ncontinual learning. Conventional rehearsal-based methods rely on previous\ntraining data to retain the model's ability, which may not be feasible in\nreal-world applications. When conducting continual learning based on a\npublicly-released LLM checkpoint, the availability of the original training\ndata may be non-existent. To address this challenge, we propose a framework\ncalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\ninstances for rehearsal. Concretely, we first employ the base LLM for\nin-context learning to generate synthetic instances. Subsequently, we utilize\nthe latest LLM to refine the instance outputs based on the synthetic inputs,\npreserving its acquired ability. Finally, we select diverse high-quality\nsynthetic instances for rehearsal in future stages. Experimental results\ndemonstrate that SSR achieves superior or comparable performance compared to\nconventional rehearsal-based approaches while being more data-efficient.\nBesides, SSR effectively preserves the generalization capabilities of LLMs in\ngeneral domains.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.01244v2", "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal", "content": "http://arxiv.org/pdf/2403.01244v2", "datetime": "2024-05-25 12:17:29", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting News in AI Research \ud83d\ude80\n\nLarge language models (LLMs) face a significant challenge in continual learning due to catastrophic forgetting. Conventional methods rely on previous training data for rehearsal, which may not be practical in real-world scenarios. Our latest research introduces a cutting-edge framework, Self-Synthesized Rehearsal (SSR), to tackle this issue.\n\n\ud83d\udd0d SSR leverages LLMs to generate synthetic instances for rehearsal, enabling continual learning without access to original training data. By utilizing the base LLM for in-context learning to create synthetic instances and refining them with the latest LLM, SSR maintains and enhances the model's acquired abilities. Moreover, diverse high-quality synthetic instances are selected for future rehearsal, ensuring data-efficient performance.\n\n\ud83d\udcca Experimental results showcase that SSR outperforms traditional rehearsal-based methods while preserving the generalization capabilities of LLMs in various domains.\n\nLearn more about our innovative SSR framework in our research paper: http://arxiv.org/abs/2403.01244v2\n\n#AI #LLM #ContinualLearning #Research #TechInnovation #NLP #SSR #ArtificialIntelligence #DataEfficiency", "x": "\ud83d\ude80 Exciting research alert! Addressing catastrophic forgetting in Large Language Models (LLMs) during continual learning, a new framework called Self-Synthesized Rehearsal (SSR) has been proposed. SSR generates synthetic instances for rehearsal, achieving superior performance while being more data-efficient. Check out the results here: http://arxiv.org/abs/2403.01244v2 #AI #NLP #LLM #Research #TechInnovation \ud83e\udd16\ud83d\udcda", "source_id": "7e656a036e346c7815e8fe235c8843c66073ddc34f0adf4f2a7ba7187b49d910", "page_number": 1}, "score": null, "embedding": [-0.3815949857234955, 0.06480429321527481, 0.19226329028606415, -0.19486968219280243, -0.1329120397567749, 0.2341940999031067, -0.300415962934494, -0.11995147168636322, 0.47389456629753113, -0.4108138084411621, 0.023740801960229874, -0.08459772169589996, 0.49150630831718445, 0.39763614535331726, 0.17734669148921967, 0.1548987179994583, -0.2075279802083969, 0.24429327249526978, 0.19359040260314941, -0.17367056012153625, 0.22768446803092957, -0.23611557483673096, -0.10102922469377518, 0.0876164510846138, 0.08585672080516815, -0.14963848888874054, -0.12291835993528366, -0.09154842048883438, -0.14427873492240906, -1.5243940353393555, 0.1439262479543686, -0.00945792905986309, 0.02101890742778778, 0.09695371240377426, -0.4352850914001465, -0.10674527287483215, -0.33754295110702515, 0.22151640057563782, -0.06996330618858337, 0.320064514875412, -0.1487892121076584, 0.28739333152770996, -0.20005619525909424, -0.14138883352279663, 0.0655912533402443, -0.34857189655303955, 0.06540517508983612, -0.062484871596097946, -0.4131771922111511, 0.039938535541296005, -0.09096451103687286, -0.1401703804731369, 0.32762956619262695, 0.16088616847991943, -0.041162822395563126, -0.07531983405351639, 0.27346256375312805, 0.508137583732605, 0.13399171829223633, -0.10701637715101242, 0.24123068153858185, 0.1818958967924118, -1.0153578519821167, -0.028511352837085724, -0.15274623036384583, 0.19014225900173187, 0.09700212627649307, -0.035608287900686264, 0.2627652883529663, 0.34113427996635437, -0.28046929836273193, 0.1639130413532257, 0.09119667857885361, 0.23368367552757263, 0.390934556722641, 0.06547007709741592, 0.1536460518836975, -0.027722228318452835, 0.271510511636734, 0.067971371114254, 0.029480233788490295, -0.09351667016744614, -0.03511686250567436, -0.2900681793689728, -0.306826651096344, 0.03291292116045952, 0.045743782073259354, -0.08976680040359497, 0.3472965359687805, 0.004233037121593952, -0.24111708998680115, -0.17586082220077515, -0.08951551467180252, 0.10079789906740189, -0.17688381671905518, 0.04064788669347763, 0.08218761533498764, 0.24861085414886475, -0.2315533608198166, 0.6913825869560242, -0.12264090776443481, -0.04035515710711479, 0.026806537061929703, 0.26600098609924316, 0.0565749891102314, -0.1753973364830017, -0.09875059872865677, -0.19543348252773285, -0.34337949752807617, 0.0001751295494614169, -0.043501585721969604, 0.03374633193016052, -0.12496698647737503, -0.12396170198917389, -0.19031009078025818, 0.07472354173660278, 0.5566895604133606, 0.15415926277637482, -0.09711437672376633, 0.06125941127538681, -0.1969841569662094, 0.12737131118774414, 0.0341527983546257, -0.3091490566730499, 0.07946854084730148, -0.19740581512451172, -0.31737035512924194, 0.5468471646308899, 0.30615848302841187, -0.17331373691558838, 0.17046408355236053, -0.16188128292560577, -0.4792076647281647, -0.2583213150501251, 0.03489493951201439, -0.3652550280094147, 0.03836121782660484, -0.20715701580047607, 0.21066975593566895, -0.26938167214393616, -0.0300836730748415, 0.18060874938964844, 0.37979012727737427, -0.12495606392621994, -0.4780987501144409, 0.7855582237243652, -0.005918177310377359, 0.20133517682552338, -0.2752525806427002, -0.06878572702407837, -0.17667530477046967, 0.06894530355930328, -0.15362440049648285, -0.449443519115448, 0.27874717116355896, 0.313068687915802, -0.03866118565201759, 0.34341299533843994, -0.1381978690624237, -0.019244801253080368, -0.19698505103588104, 0.0806795135140419, 0.15724073350429535, 0.6865642666816711, -0.1336449831724167, -0.3644867241382599, -0.34122323989868164, 0.20482033491134644, 0.3225537836551666, -0.003311082487925887, 0.08256680518388748, 0.15858447551727295, -0.09862834215164185, -0.24516849219799042, -0.1319543868303299, 0.19700437784194946, -0.506624698638916, -0.08033329993486404, 0.05316763371229172, 0.2715863287448883, -0.00831281766295433, -0.45575788617134094, -0.16596341133117676, 0.16241823136806488, 0.03593279421329498, -0.22044290602207184, 0.008257448673248291, -0.40797513723373413, 0.1915058046579361, 0.30494722723960876, -0.05316852405667305, 0.33204180002212524, 0.1417284458875656, 0.07425494492053986, -0.1890280544757843, -0.2339821606874466, -0.09361395239830017, -0.26228058338165283, 0.08047778159379959, -0.2114747315645218, -0.151285782456398, 0.21284961700439453, -0.10019262880086899, -0.037516772747039795, 0.5070717930793762, -0.07609246671199799, 0.11105524003505707, -0.021576933562755585, 0.40782904624938965, 0.46326079964637756, -0.4692530930042267, -0.031556643545627594, 0.3518272936344147, 0.17941616475582123, -0.16354985535144806, 0.06488178670406342, -0.08416160941123962, 0.2181725949048996, -0.14142845571041107, 0.1431400328874588, 0.2656033933162689, 0.30377933382987976, -0.3148035407066345, -1.189820647239685, -0.26587891578674316, 0.13522718846797943, 0.11034277081489563, 0.24921363592147827, -0.21323524415493011, 0.07379855215549469, 0.3569934666156769, 0.24745362997055054, 0.10661152005195618, 0.19020481407642365, 0.10960575193166733, -0.3891333043575287, 0.17518694698810577, -0.23019260168075562, -0.185659259557724, -0.0910206213593483, 0.05054658651351929, -0.1772468239068985, 0.23839685320854187, -0.05673041194677353, 0.30481311678886414, -0.001411384204402566, -0.7177629470825195, 0.2211897075176239, -0.12979069352149963, 0.7815149426460266, -0.3351130783557892, 0.44512268900871277, -0.03427871689200401, 0.05373283475637436, 0.32484540343284607, -0.06567975878715515, -0.5723230838775635, 0.031065288931131363, -0.08511289209127426, 0.1277192384004593, 0.2182278037071228, 0.2387673556804657, -0.12589794397354126, -0.08538012206554413, 0.005004719831049442, -0.05623563006520271, -0.6879569888114929, -0.3112315833568573, 0.0701124295592308, -0.1799256056547165, -0.002685467479750514, -0.31907257437705994, 0.2120063304901123, 0.14446358382701874, -0.16052353382110596, 0.31047794222831726, -0.04346247389912605, -0.19732795655727386, 0.039905846118927, -0.5399602651596069, -0.01085247378796339, -0.2836478352546692, -0.09062456339597702, 0.33152347803115845, 0.11335544288158417, -0.04822571948170662, -0.15194369852542877, 0.2877901494503021, -0.19033488631248474, 0.17662979662418365, -0.005756495054811239, 0.3739493191242218, -0.20545907318592072, -0.2630162239074707, 0.6633395552635193, 0.06610127538442612, -0.02944989502429962, -0.017443252727389336, 0.03427690640091896, -0.2782320976257324, -0.4957210421562195, -0.478443443775177, 0.09344061464071274, 0.32676950097084045, 0.08853519707918167, 0.12433360517024994, 0.06403296440839767, 0.07743700593709946, 0.4215780794620514, 0.4462019205093384, 0.31928783655166626, 0.2995237410068512, 0.20257875323295593, -0.041226089000701904, 0.08695374429225922, -0.2819094657897949, -0.009819935075938702, 0.21391455829143524, -0.15381920337677002, -1.2332829236984253, -0.0823952928185463, -0.3008321523666382, 0.5679198503494263, -0.22350525856018066, 0.03909822180867195, -0.0714196190237999, 0.05941230058670044, -0.2319229692220688, 0.41263094544410706, -0.16413334012031555, 0.09723145514726639, 0.22221292555332184, 0.20576012134552002, 0.16814863681793213, 0.01008895318955183, 0.5818696022033691, -0.16324689984321594, 0.23443400859832764, -0.7251099944114685, 0.1876894235610962, 0.08992055803537369, 0.9023405909538269, 0.09364891052246094, 0.12367826700210571, 0.22346776723861694, -0.04174236208200455, 0.06123462691903114, 0.04610196873545647, -0.0604214109480381, -0.15033093094825745, -0.029469387605786324, 0.6219382286071777, -0.03382406011223793, 0.18635229766368866, 0.05671006441116333, -0.2922412157058716, 0.1133187785744667, 0.14279380440711975, 0.054897673428058624, -0.040461424738168716, -0.0005501809646375477, -0.021324628964066505, -0.00017902693070936948, 0.8350028395652771, 0.08942637592554092, 0.24378006160259247, -0.4989364445209503, -0.1790555864572525, -0.08070351928472519, -0.26075732707977295, -0.06886997073888779, 0.035309530794620514, -0.10438535362482071, 0.026014454662799835, 0.10481622815132141, 0.14783349633216858, -0.05077042803168297, 0.041025999933481216, -0.07020047307014465, 0.1952841728925705, -0.1718677431344986, 0.26268866658210754, -0.027470173314213753, -0.1648205816745758], "sparse_embedding": null}, {"id": "4f8f5a6c2959929148959dfca527bf9ef1f928bcb3dc7cd0e483a70dbbc71ea6", "content": "Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.17860v2", "title": "Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications", "content": "http://arxiv.org/pdf/2403.17860v2", "datetime": "2024-04-02 12:25:57", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting news in the world of Natural Language Processing! \ud83c\udf1f\n\nAre you interested in cutting-edge research on mitigating errors in NLP models during classification tasks? Check out this groundbreaking study that explores the use of large language models (LLMs) for data augmentation to address high confidence misclassifications. \n\nThe research compares the effectiveness of synthetic data generated by LLMs versus human-provided data in reducing wrong predictions while maintaining accuracy levels. Results show that LLMs can significantly reduce high confidence misclassifications, offering a more scalable solution compared to human-provided data.\n\nRead more about this innovative approach and its implications for NLP models here: http://arxiv.org/abs/2403.17860v2\n\n#NLP #LLMs #DataAugmentation #AI #TechResearch #Innovation #ArtificialIntelligence #MachineLearning #TechTrends", "x": "Exciting research on leveraging large language models for data augmentation in NLP to reduce high confidence misclassifications! \ud83d\ude80\ud83e\udd16 Check out the study comparing human-generated vs. LLM-generated synthetic data for classification tasks. Results show promising effectiveness and scalability. Read more at: http://arxiv.org/abs/2403.17860v2 #AI #NLP #LLM #DataAugmentation #Research", "source_id": "4fccfa3f8234a42278df83e186b4f7516f78cd704f1f8d04351df9001db67be5", "page_number": 1}, "score": null, "embedding": [-0.21187737584114075, -0.144776850938797, 0.02318030782043934, -0.1063542366027832, 0.16847851872444153, 0.08783604949712753, -0.09416191279888153, -0.12090016156435013, 0.23620939254760742, -0.3144957721233368, 0.08201296627521515, -0.07078663259744644, 0.13169200718402863, 0.1960277110338211, 0.1584893763065338, 0.16704308986663818, -0.15740519762039185, 0.20012041926383972, -0.1747296005487442, 0.14268344640731812, 0.28332599997520447, -0.0909186378121376, 0.04555460438132286, -0.07405633479356766, -0.07345584779977798, -0.06807492673397064, -0.19900722801685333, -0.28271007537841797, -0.22304318845272064, -1.4243792295455933, 0.1305612176656723, -0.18254190683364868, 0.42051416635513306, -0.17403599619865417, -0.06704200804233551, -0.030339589342474937, -0.24088190495967865, 0.17687919735908508, -0.07381585240364075, 0.15252038836479187, -0.07996097207069397, -0.06129855290055275, -0.10296111553907394, -0.15178504586219788, 0.2709576487541199, -0.2973276376724243, -0.20988832414150238, -0.15812285244464874, -0.6306280493736267, 0.011844421736896038, -0.24917304515838623, -0.14581464231014252, 0.07274281233549118, 0.3053082525730133, 0.23612554371356964, 0.00031514879083260894, 0.4045030176639557, 0.1955689638853073, 0.06767717003822327, 0.14696885645389557, 0.13193745911121368, 0.3805449604988098, -1.0780584812164307, 0.2147429883480072, -0.07001551985740662, 0.22775086760520935, -0.11516852676868439, -0.3073274791240692, 0.003356871660798788, 0.011441130191087723, -0.15418614447116852, 0.15777409076690674, 0.3562445044517517, 0.2555287480354309, 0.23722761869430542, 0.36356601119041443, 0.14603900909423828, -0.3036997616291046, 0.1950850635766983, 0.08621452748775482, 0.34401535987854004, -0.02130407840013504, -0.11329497396945953, -0.5054540038108826, -0.038526006042957306, -0.09420305490493774, 0.11323251575231552, -0.1032792255282402, 0.0572981983423233, -0.2188667207956314, -0.05270480737090111, -0.022952787578105927, -0.10717914998531342, 0.34496110677719116, 0.08840445429086685, 0.1878531277179718, 0.08921486884355545, 0.19796156883239746, -0.2933236062526703, 0.726256787776947, -0.20031188428401947, 0.043196894228458405, -0.26799139380455017, -0.12360315769910812, 0.2176218032836914, -0.14848031103610992, -0.3150935769081116, -0.21988064050674438, -0.11432711035013199, -0.15134724974632263, 0.02031281404197216, 0.07088794559240341, -0.1553734987974167, -0.011636096984148026, -0.07580693066120148, -0.0841653123497963, 0.6719228029251099, -0.07716140896081924, -0.21578148007392883, 0.13091573119163513, -0.14394043385982513, 0.08007846027612686, -0.004262122325599194, -0.18036775290966034, -0.02081672102212906, -0.1072036549448967, 0.20465639233589172, 0.6119037866592407, 0.053024519234895706, -0.013823749497532845, 0.12015057355165482, -0.13832901418209076, -0.36415883898735046, -0.11552301049232483, 0.22135649621486664, -0.1730324923992157, -0.037171974778175354, -0.030536023899912834, 0.10737390071153641, 0.11964188516139984, -0.09913487732410431, 0.04185626283288002, -0.025674259290099144, 0.02245919778943062, -0.5081172585487366, 0.925348162651062, -0.20284079015254974, 0.05057017505168915, -0.21981023252010345, -0.2558691203594208, -0.1215970441699028, 0.32423093914985657, -0.2763233780860901, -0.1444149911403656, 0.27198678255081177, 0.02579585276544094, 0.1984017938375473, -0.08054925501346588, -0.44736847281455994, -0.25182294845581055, 0.04015631228685379, -0.05609462782740593, -0.24938388168811798, 0.7143455743789673, -0.07448387891054153, -0.1499778777360916, -0.19562023878097534, -0.020999737083911896, 0.39274999499320984, -0.06783446669578552, 0.2975768446922302, -0.03634040802717209, -0.035298559814691544, 0.05417142063379288, -0.3566654622554779, 0.2282739132642746, -0.5995250940322876, -0.19448070228099823, -0.05802714452147484, 0.19836513698101044, -0.026842376217246056, -0.13764485716819763, -0.21666671335697174, 0.09236080944538116, 0.05347706750035286, -0.26509368419647217, -0.0028201478999108076, -0.22163207828998566, 0.29775556921958923, 0.09561651200056076, -0.24258127808570862, 0.2171596735715866, 0.03412620350718498, -0.09654124081134796, -0.17414140701293945, -0.15997901558876038, -0.06536814570426941, -0.005913130473345518, 0.1693975180387497, -0.09061725437641144, -0.09163734316825867, 0.33966702222824097, 0.2794054448604584, -0.11812885850667953, 0.11085357517004013, 0.09007801860570908, -0.051891494542360306, 0.09680327773094177, 0.16770444810390472, 0.4339732825756073, -0.4052797257900238, -0.05067423731088638, 0.3570442497730255, -0.0035742996260523796, -0.00989274587482214, 0.1238502636551857, 0.3244912028312683, 0.35594889521598816, 0.040645208209753036, 0.26648035645484924, 0.05247640237212181, 0.3059855103492737, -0.19545020163059235, -1.207869291305542, -0.15305057168006897, 0.30066660046577454, -0.009440707042813301, 0.26874783635139465, -0.27053120732307434, 0.08391578495502472, 0.008087648078799248, 0.27173808217048645, 0.5558276772499084, 0.09210389107465744, -0.030784593895077705, -0.03378487378358841, 0.2249690294265747, 0.1297358274459839, 0.013203282840549946, -0.12446446716785431, 0.08560933917760849, -0.3177913725376129, 0.38713619112968445, -0.2271975874900818, 0.015711022540926933, 0.05609256401658058, -0.9378836154937744, -0.10444065183401108, -0.05590551719069481, 0.6909571290016174, -0.36877962946891785, 0.17137302458286285, -0.16081400215625763, -0.007771321572363377, 0.26790472865104675, -0.04415342956781387, -0.41224274039268494, 0.6454294323921204, 0.016565581783652306, 0.20038975775241852, -0.05813243240118027, 0.06471564620733261, -0.11078440397977829, -0.21080979704856873, 0.008327520452439785, 0.16697971522808075, -0.7546325922012329, -0.25387340784072876, 0.05554937943816185, -0.13235269486904144, -0.06423930078744888, -0.41052863001823425, 0.22854602336883545, 0.2077798694372177, 0.11376574635505676, 0.3104441165924072, -0.04038134962320328, -0.14119401574134827, 0.07565145194530487, -0.7082957625389099, 0.1578107476234436, -0.15533509850502014, 0.044834721833467484, 0.387326180934906, -0.13542751967906952, 0.04799919202923775, -0.23801934719085693, 0.34348830580711365, -0.31827446818351746, -0.055126119405031204, -0.03177319094538689, 0.2792668044567108, 0.0034211655147373676, -0.19095337390899658, 0.8353658318519592, 0.04404481500387192, 0.06863164156675339, 0.1751292496919632, 0.16087546944618225, -0.3104541301727295, -0.36477401852607727, -0.3042603135108948, 0.08043579757213593, 0.584552526473999, 0.23783722519874573, 0.35756269097328186, 0.04420514032244682, 0.28618499636650085, -0.02501070126891136, 0.47074365615844727, 0.03402162715792656, 0.031692326068878174, 0.18675576150417328, 0.02902974747121334, 0.01590788923203945, -0.2993856370449066, 0.0054594241082668304, 0.0865020602941513, 0.09123663604259491, -1.1194052696228027, -0.19072610139846802, -0.16591638326644897, 0.3546641767024994, 0.03792308643460274, -0.0987798348069191, 0.21231591701507568, -0.021063653752207756, 0.1611359715461731, 0.19442275166511536, -0.2751191258430481, 0.08208410441875458, 0.17468366026878357, 0.06320665031671524, 0.24652791023254395, -0.13451749086380005, 0.23420806229114532, -0.08830586820840836, 0.15416398644447327, -0.33981677889823914, 0.18325620889663696, 0.1231195330619812, 1.0888649225234985, 0.015018091537058353, -0.0027966538909822702, 0.3302270770072937, -0.11179791390895844, -0.07430104166269302, 0.014780785888433456, -0.24154993891716003, -0.17405319213867188, 0.21952563524246216, 0.6936688423156738, -0.06864948570728302, -0.04189971834421158, 0.6111531853675842, -0.27697136998176575, -0.13598038256168365, 0.17086286842823029, 0.0554693266749382, 0.324670672416687, 0.02227381058037281, 0.26057592034339905, -0.16602519154548645, 0.5693451166152954, -0.12065854668617249, 0.06256729364395142, -0.35365399718284607, 0.06750036776065826, 0.13972437381744385, -0.31208449602127075, 0.12949174642562866, -0.08631333708763123, -0.01840822771191597, 0.05651261284947395, 0.08395704627037048, 0.08561667054891586, -0.4159099757671356, -0.057430144399404526, -0.2760006785392761, 0.0031131806317716837, -0.22067326307296753, -0.058455146849155426, 0.057387810200452805, -0.43528279662132263], "sparse_embedding": null}, {"id": "f51dc20b212072ef6ae5b0ac84e06bd2df8b73464d6eae3b2af50f71c42a9db0", "content": "The rapid development of Large Language Models (LLMs) has led to great\nstrides in model capabilities like long-context understanding and reasoning.\nHowever, as LLMs are able to process longer contexts, it becomes more\nchallenging to evaluate whether they have acquired certain capabilities, since\nthe length of text (e.g., 200K tokens) they can process far exceeds what humans\ncan reliably assess in a reasonable duration. In this paper, we propose using\ncomplex synthetic tasks as a proxy evaluation method, and present S3Eval, a\nSynthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The\nsynthetic nature of S3Eval provides users full control over the dataset,\nallowing them to systematically probe LLM capabilities by scaling text length\nand varying task difficulty across diverse scenarios. The strong correlation\nbetween S3Eval and real-world benchmarks demonstrates the soundness of using\nS3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite\nlong-context data generation method. We have generated a comprehensive dataset\ncalled S3Eval-Standard, and experimental results have shown that it poses\nsignificant challenges for all existing LLMs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.15147v2", "title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models", "content": "http://arxiv.org/pdf/2310.15147v2", "datetime": "2024-04-06 15:20:18", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting developments in the field of Large Language Models (LLMs)! A recent paper introduces S3Eval, a Synthetic, Scalable, Systematic evaluation suite designed to assess LLM capabilities in processing long contexts. By utilizing complex synthetic tasks, S3Eval offers a flexible and controlled method to evaluate LLM performance across diverse scenarios. \n\nThe correlation between S3Eval and real-world benchmarks showcases its effectiveness in gauging LLM capabilities. Experimental results with the S3Eval-Standard dataset have revealed significant challenges for existing LLMs, highlighting the potential of this evaluation method.\n\nRead more about S3Eval and its implications for LLM assessment here: http://arxiv.org/abs/2310.15147v2\n\n#AI #NLP #LLMs #Technology #Research #Innovation", "x": "\ud83d\ude80 Exciting development in Large Language Models (LLMs) evaluation! Check out S3Eval, a Synthetic, Scalable, Systematic evaluation suite that challenges existing LLMs with complex synthetic tasks. Learn more at: http://arxiv.org/abs/2310.15147v2 #AI #NLP #LLMs #TechInnovation \ud83e\udde0\ud83d\udd0d", "source_id": "aa4c1b5dcd1138e0ced3fc41237db46b75204561bcfd8accce11f030c799d98a", "page_number": 1}, "score": null, "embedding": [-0.35848429799079895, 0.0015975757269188762, 0.10478750616312027, -0.04485936462879181, 0.03236818686127663, -0.04530768096446991, -0.36183643341064453, 0.02879466861486435, 0.18743020296096802, -0.3061543405056, -0.00938084814697504, -0.041489582508802414, 0.19368398189544678, 0.03167246654629707, 0.27485358715057373, 0.19041377305984497, -0.17045974731445312, 0.12452284246683121, -0.33135512471199036, -0.08223668485879898, 0.37406525015830994, -0.3209313452243805, 0.008382098749279976, 0.0352679006755352, 0.2671791613101959, 0.13905414938926697, -0.08844809234142303, -0.14477965235710144, -0.26733896136283875, -1.558762550354004, 0.23951765894889832, -0.14924439787864685, 0.3658835291862488, -0.03743365406990051, -0.1499825268983841, 0.12701919674873352, -0.2836548984050751, 0.15643103420734406, 0.04554998502135277, 0.20786502957344055, 0.09319771826267242, 0.3803851902484894, -0.08678082376718521, -0.10990755259990692, -0.29475969076156616, -0.4050459563732147, -0.3071819245815277, 0.1453532874584198, -0.5592555999755859, -0.0339779257774353, -0.19142179191112518, -0.2950328290462494, 0.2275005280971527, 0.16734729707241058, 0.014172518625855446, 0.1267213523387909, 0.15832951664924622, 0.15919561684131622, 0.1348644495010376, -0.1425934135913849, 0.28758156299591064, 0.28799834847450256, -1.014596939086914, 0.2760965824127197, -0.006536900531500578, 0.2432277798652649, 0.07114250212907791, 0.009081157855689526, 0.2705484926700592, 0.12523795664310455, -0.2046426385641098, 0.06773227453231812, 0.36536914110183716, 0.46742182970046997, 0.37637007236480713, 0.28620782494544983, 0.14732956886291504, -0.0958731397986412, 0.19212573766708374, 0.06041484326124191, 0.30982035398483276, -0.22156114876270294, -0.06994183361530304, -0.1404319852590561, -0.2884107828140259, -0.06747601181268692, -0.28887057304382324, 0.0029375539161264896, 0.04246702417731285, -0.02517695166170597, -0.10103089362382889, -0.23508398234844208, -0.13545939326286316, 0.2760877311229706, -0.2546563446521759, -0.01776576228439808, 0.024495406076312065, 0.0813588947057724, -0.2407192885875702, 0.6029932498931885, -0.09987611323595047, 0.21509747207164764, -0.07395809888839722, -0.14799749851226807, 0.34901049733161926, 0.015023541636765003, -0.19878095388412476, -0.1624128818511963, -0.10655558109283447, -0.11246078461408615, -0.0002865247952286154, 0.05580634996294975, -0.23010651767253876, -0.043920837342739105, 0.05029435083270073, -0.0424649678170681, 0.5266819596290588, -0.18885129690170288, -0.09248797595500946, -0.09128803014755249, -0.3690141439437866, 0.43355700373649597, 0.009309403598308563, -0.2469925582408905, 0.1655445545911789, -0.19444972276687622, 0.2932610511779785, 0.40570268034935, 0.13912366330623627, -0.0844905748963356, 0.15913920104503632, 0.06686001271009445, -0.4477471709251404, -0.138241708278656, -0.061343900859355927, -0.28314465284347534, -0.1618902087211609, -0.0983930453658104, 0.04405909776687622, 0.10711152106523514, -0.03409166261553764, 0.14152835309505463, 0.3366810083389282, -0.11621317267417908, -0.33420130610466003, 0.7228467464447021, 0.10956450551748276, 0.13364605605602264, -0.378818541765213, -0.16564056277275085, 0.20226337015628815, 0.23282606899738312, 0.06858892738819122, -0.3032062351703644, 0.2753695249557495, 0.13642780482769012, 0.17485882341861725, 0.18263626098632812, -0.46176788210868835, -0.051488351076841354, -0.020929502323269844, 0.25366970896720886, -0.1463349312543869, 0.7454001307487488, -0.1089019924402237, -0.4548723101615906, -0.06362710148096085, -0.013454850763082504, 0.08831803500652313, -0.0012239809148013592, 0.11935895681381226, 0.04527544602751732, -0.15761306881904602, 0.055649686604738235, -0.09183737635612488, 0.19080807268619537, -0.6268976926803589, -0.09857518970966339, 0.10528022050857544, 0.16188734769821167, -0.0063082510605454445, -0.20455855131149292, -0.09060220420360565, 0.2780018746852875, -0.057027511298656464, -0.26887640357017517, 0.02715299278497696, -0.12916573882102966, 0.0702575221657753, 0.07513020932674408, -0.12862548232078552, 0.38217779994010925, -0.017893170937895775, 0.010457301512360573, -0.01927649788558483, -0.3323343098163605, 0.11169102787971497, 0.04066379368305206, 0.1316136121749878, -0.1365717649459839, 0.24171008169651031, 0.20375174283981323, 0.0022997877094894648, -0.21001090109348297, 0.2476046085357666, -0.09222674369812012, -0.09866239875555038, 0.17675365507602692, 0.3513653576374054, 0.31428059935569763, -0.4720105528831482, -0.03685186803340912, 0.2938949465751648, -0.044259510934352875, -0.11742214858531952, -0.00656198849901557, 0.15421336889266968, 0.025236835703253746, -0.2426232397556305, 0.30958855152130127, 0.0587269589304924, 0.09140915423631668, -0.18000087141990662, -1.224050521850586, 0.0007699818233959377, 0.2621045708656311, -0.0013086776016280055, 0.22985722124576569, -0.28706490993499756, -0.06621386855840683, -0.08507630974054337, 0.22324460744857788, 0.03467332571744919, 0.2962604761123657, 0.08908036351203918, -0.26463988423347473, 0.030241573229432106, 0.10373026132583618, -0.0662844181060791, -0.32555273175239563, 0.04269104823470116, -0.3792259097099304, 0.2765428423881531, -0.2718571722507477, 0.13738900423049927, 0.06944972276687622, -0.5222581624984741, -0.10317353904247284, -0.3599128723144531, 0.9009602069854736, -0.32030075788497925, -0.09779156744480133, -0.26172780990600586, 0.11061755567789078, 0.09862162172794342, -0.016203241422772408, -0.4937795400619507, 0.4063761830329895, 0.12181771546602249, 0.19868694245815277, 0.1280672401189804, 0.11286643892526627, 0.06632617115974426, -0.23247578740119934, -0.10426025837659836, 0.1545167863368988, -0.6912596225738525, -0.2467132955789566, 0.12530116736888885, 0.2039048969745636, -0.14665354788303375, -0.15006765723228455, 0.059599436819553375, 0.22873151302337646, 0.028801845386624336, 0.1750662624835968, -0.09971769154071808, -0.13058269023895264, 0.020104965195059776, -0.6513299345970154, 0.21007002890110016, -0.14688940346240997, -0.1497368961572647, 0.1803847998380661, 0.0824158787727356, -0.1672765612602234, 0.025355743244290352, 0.252594918012619, -0.08089540153741837, -0.06233997642993927, -0.018530765548348427, -0.17110440135002136, -0.23285235464572906, -0.16725026071071625, 0.5003700852394104, -0.2569662928581238, 0.1675364077091217, 0.1208374947309494, 0.075835682451725, -0.0687863677740097, -0.23982562124729156, -0.14425255358219147, -0.11100603640079498, 0.5790972113609314, 0.11290065944194794, 0.02984621189534664, 0.1751919537782669, 0.08007261157035828, 0.04268860071897507, 0.25864213705062866, 0.21669240295886993, 0.06624661386013031, -0.047141700983047485, -0.0993492603302002, 0.04804876074194908, -0.25278884172439575, -0.15802861750125885, 0.18130339682102203, -0.10488761216402054, -1.0592572689056396, 0.031171267852187157, -0.08782222121953964, 0.22000738978385925, -0.07754570990800858, -0.07963177561759949, 0.21247051656246185, -0.13714154064655304, -0.061533503234386444, 0.04851701110601425, -0.061563823372125626, 0.2639119029045105, 0.1225975751876831, 0.12960949540138245, 0.23012375831604004, -0.12377127259969711, 0.42167457938194275, -0.14319711923599243, 0.30985888838768005, -0.5097527503967285, 0.10569097101688385, 0.2143355756998062, 1.0496634244918823, -0.10421421378850937, -0.0009752041078172624, 0.4330388903617859, 0.029114242643117905, -0.1154932752251625, -0.04759583994746208, -0.1742101013660431, -0.005389671307057142, -0.010057150386273861, 0.6417298913002014, -0.02090759016573429, 0.12018411606550217, 0.2704838216304779, -0.19913803040981293, 0.06805586814880371, 0.2656058371067047, 0.23985689878463745, 0.6060487031936646, 0.027039259672164917, 0.12355917692184448, -0.15121880173683167, 0.6927741765975952, -0.012215000577270985, -0.01072030607610941, -0.3562093675136566, -0.10581261664628983, 0.003069165861234069, -0.3170423209667206, 0.06651562452316284, -0.19205491244792938, 0.17192643880844116, 0.3926302194595337, 0.18584807217121124, 0.13374418020248413, -0.03559572249650955, -0.134162038564682, -0.17669358849525452, -0.09032878279685974, -0.39979878067970276, 0.08671089261770248, 0.2629711627960205, -0.07005271315574646], "sparse_embedding": null}, {"id": "3b778acbda18010571b2a242bc6999da6901d53ccd85463d54236ea987ab3a9d", "content": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of \"fact-checking\" are\nbased on verifying each piece of a model generation against potential evidence\nusing an LLM. However, this process can be very computationally expensive,\nrequiring many calls to LLMs to check a single response. In this work, we show\nhow to build small models that have GPT-4-level performance but for 400x lower\ncost. We do this by constructing synthetic training data with GPT-4, which\ninvolves creating realistic yet challenging instances of factual errors via a\nstructured generation procedure. Training on this data teaches models to check\neach fact in the claim and recognize synthesis of information across sentences.\nFor evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact,\ncollected from recent work on fact-checking and grounding LLM generations. Our\nbest system MiniCheck-FT5 (770M parameters) outperforms all systems of\ncomparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for\ndata synthesis, and models.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2404.10774v1", "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "content": "http://arxiv.org/pdf/2404.10774v1", "datetime": "2024-04-16 17:59:10", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting news in the world of NLP and LLMs! Researchers have developed a groundbreaking approach to fact-checking LLM outputs, significantly reducing computational costs while maintaining GPT-4-level performance. By training small models on synthetic data generated with GPT-4, they have successfully improved the efficiency of verifying facts in model generations. The newly introduced benchmark LLM-AggreFact, along with the MiniCheck-FT5 system, outperforms comparable models and achieves GPT-4 accuracy. Learn more about this innovative work at: http://arxiv.org/abs/2404.10774v1 #NLP #LLM #AI #FactChecking #Innovation \ud83d\udd0d\ud83d\udcca\ud83d\udd2c", "x": "\ud83d\ude80 Exciting new research on fact-checking in NLP! Learn how small models with GPT-4-level performance are built at 400x lower cost. Check out MiniCheck-FT5, outperforming others of comparable size. Find out more at: http://arxiv.org/abs/2404.10774v1 #AI #NLP #LLM #research #factchecking", "source_id": "b9cc3e96294ad553fbc4c6f6e413034b38772a5e09e7d5475479c3338b5d34f3", "page_number": 1}, "score": null, "embedding": [-0.24489767849445343, 0.024771815165877342, 0.11332601308822632, -0.08777137845754623, 0.299771785736084, 0.2517421841621399, -0.24166202545166016, -0.07232672721147537, 0.15615323185920715, -0.3335495591163635, 0.18184220790863037, -0.1765809804201126, 0.15131573379039764, 0.3290174603462219, -0.05932466685771942, 0.08044549822807312, -0.15943312644958496, 0.2986326813697815, -0.007857013493776321, 0.04201221466064453, 0.3500896394252777, -0.2946479320526123, -0.04459037631750107, -0.08301432430744171, 0.10804054141044617, 0.17545993626117706, -0.04238557815551758, -0.07000923901796341, -0.24876338243484497, -1.4215278625488281, -0.05915524438023567, -0.0507311150431633, 0.3764212727546692, 0.021718956530094147, 0.05341435223817825, 0.04948800429701805, -0.2780010998249054, 0.1587284803390503, -0.1735297292470932, 0.14896520972251892, 0.04588188976049423, 0.09605715423822403, 0.012238327413797379, -0.18247655034065247, -0.11035076528787613, -0.30142098665237427, -0.16304460167884827, 0.10826417803764343, -0.4953683614730835, -0.3309241533279419, -0.06409389525651932, -0.06090308353304863, -0.024076033383607864, 0.043340351432561874, -0.0239630825817585, 0.0004878451582044363, 0.27596110105514526, 0.12598669528961182, 0.2876015305519104, 0.017919354140758514, 0.27214646339416504, 0.5538138151168823, -1.1221377849578857, 0.08997482061386108, 0.031624890863895416, 0.1730901300907135, -0.04877348244190216, 0.02171856164932251, 0.16131755709648132, 0.15810653567314148, -0.17462432384490967, -0.06692488491535187, 0.12279929220676422, 0.2993437945842743, 0.1364125907421112, 0.24711859226226807, 0.12297190725803375, -0.18626195192337036, 0.023338397964835167, -0.06619830429553986, -0.10881505906581879, 0.08515167236328125, -0.13044750690460205, -0.009846288710832596, -0.2330990433692932, -0.1648934781551361, 0.017297882586717606, 0.020565830171108246, 0.24672353267669678, -0.04051751643419266, 0.017295056954026222, -0.03560780733823776, -0.14379297196865082, 0.26959484815597534, -0.11023427546024323, 0.16836407780647278, 0.18752743303775787, 0.06595935672521591, -0.043660953640937805, 0.5830998420715332, -0.0893031433224678, 0.12416823208332062, -0.22029384970664978, 0.12973982095718384, 0.08140739798545837, -0.032685261219739914, -0.20986363291740417, 0.06143441051244736, -0.14391300082206726, -0.1757129579782486, -0.0680936649441719, 0.2546316385269165, 0.10128585249185562, -0.1706414669752121, 0.04050520434975624, -0.12063415348529816, 0.3678762912750244, -0.08710464090108871, -0.20603802800178528, 0.025794576853513718, -0.24110108613967896, 0.12865565717220306, -0.2546003460884094, -0.12756046652793884, 0.2173849642276764, -0.22223961353302002, 0.13691702485084534, 0.39779287576675415, 0.12627151608467102, 0.17904803156852722, 0.12289884686470032, -0.11679986119270325, -0.4250882863998413, -0.2534197270870209, 0.23370927572250366, -0.16259920597076416, 0.39305999875068665, 0.05236067995429039, -0.003010575659573078, 0.4142020344734192, -0.08390432596206665, 0.08945050835609436, 0.04418230056762695, 0.03836431726813316, -0.5370074510574341, 0.7218722105026245, -0.2342934012413025, 0.03777607902884483, -0.16034385561943054, -0.4251599907875061, 0.1379646360874176, 0.33037203550338745, -0.21479882299900055, -0.23326388001441956, 0.3431151509284973, 0.014459947124123573, 0.13701659440994263, 0.016161467880010605, -0.47479069232940674, 0.04240143299102783, -0.07071799784898758, 0.014700457453727722, -0.2020588517189026, 0.6643903255462646, -0.08950944244861603, -0.3327251672744751, -0.3037571907043457, 0.11591409146785736, 0.3602806329727173, -0.40405961871147156, 0.2079903781414032, 0.20333224534988403, 0.015363877639174461, -0.32832589745521545, -0.22167229652404785, -0.28133270144462585, -0.5339145660400391, 0.09048773348331451, -0.11612208187580109, 0.27428746223449707, 0.15429341793060303, -0.29755669832229614, -0.17244884371757507, 0.2448420226573944, 0.014714025892317295, -0.08835753798484802, 0.02226327359676361, -0.32014715671539307, 0.2009958177804947, -0.03185264766216278, -0.3070489764213562, 0.0899343267083168, -0.03731667995452881, 0.17445632815361023, -0.17367838323116302, -0.2165086269378662, -0.12126510590314865, 0.12933677434921265, 0.18161866068840027, -0.16873714327812195, -0.012683156877756119, -0.15029706060886383, -0.03378137946128845, 0.026060916483402252, 0.0255748238414526, 0.01946772076189518, -0.1319957673549652, 0.0012222614604979753, 0.3552854061126709, 0.33242297172546387, -0.18218031525611877, -0.10219789296388626, 0.10524836927652359, -0.227272629737854, -0.19501036405563354, 0.03777482360601425, 0.21338653564453125, 0.38220471143722534, -0.37903764843940735, 0.1987294852733612, -0.013637064956128597, 0.06798097491264343, -0.11121775209903717, -1.0931999683380127, -0.29621756076812744, 0.2416556030511856, 0.31571054458618164, 0.4429347515106201, -0.3547252416610718, 0.1457219123840332, 0.1572628915309906, 0.009648572653532028, 0.2871696352958679, 0.06472210586071014, 0.20443099737167358, -0.16301411390304565, 0.16948699951171875, -0.13777372241020203, -0.042458295822143555, -0.15421634912490845, 0.03637031465768814, -0.19491547346115112, 0.17703351378440857, -0.26977595686912537, 0.04713036119937897, -0.07160937041044235, -0.6790460348129272, 0.05070190131664276, -0.228683203458786, 0.7717533111572266, -0.14339196681976318, 0.13298581540584564, -0.00216200714930892, 0.028044937178492546, 0.1332312673330307, -0.02834535948932171, -0.3456297218799591, 0.46392494440078735, 0.12698590755462646, -0.031646158546209335, 0.06882455199956894, 0.12078545987606049, -0.1517791450023651, -0.26919466257095337, -0.002273108810186386, 0.12513086199760437, -0.6115936040878296, -0.34110167622566223, 0.011349090375006199, -0.13350418210029602, 0.054359689354896545, -0.20811180770397186, 0.22758954763412476, 0.19610600173473358, -0.07233989238739014, 0.43616706132888794, -0.033773377537727356, -0.2001662254333496, -0.15650933980941772, -0.5496731996536255, 0.0358540304005146, -0.2616675794124603, -0.05091353505849838, 0.21378283202648163, -0.07442694902420044, -0.06679287552833557, -0.13865940272808075, 0.4080488979816437, -0.14169704914093018, -0.1427837461233139, -0.055166929960250854, 0.3003440499305725, -0.20694716274738312, 0.2816014289855957, 0.5293822288513184, 0.07673567533493042, 0.2671310007572174, 0.13177059590816498, 0.15076719224452972, -0.13289514183998108, -0.36360904574394226, -0.32486164569854736, -0.21365748345851898, 0.38791465759277344, 0.21772171556949615, 0.1332206279039383, 0.07762634754180908, -0.05076819285750389, 0.2716550827026367, 0.4362708330154419, 0.14381133019924164, 0.2032375931739807, -0.05095309764146805, 0.048725225031375885, 0.16479922831058502, -0.19194296002388, -0.08129335939884186, 0.04370344802737236, 0.08618157356977463, -1.101419448852539, -0.04725700616836548, -0.16149529814720154, 0.4304172396659851, -0.30680814385414124, -0.11177344620227814, 0.11419408023357391, -0.051363732665777206, 0.03146618604660034, 0.178850919008255, -0.05726664513349533, 0.06810370832681656, 0.30377626419067383, -0.2853488326072693, 0.1249655932188034, 0.18471422791481018, 0.24745529890060425, -0.3797200322151184, -0.15747228264808655, -0.3124513626098633, 0.16753464937210083, 0.3032509386539459, 0.900011420249939, 0.0019178339280188084, 0.057243216782808304, 0.2980411648750305, -0.06485901772975922, 0.12073211371898651, 0.16721299290657043, -0.06735079735517502, -0.15898939967155457, -0.06173669546842575, 0.8125963807106018, 0.08518106490373611, 0.13361400365829468, 0.5482099056243896, -0.21665409207344055, 0.06824369728565216, 0.1817592978477478, -0.018296638503670692, 0.3229610025882721, -0.15540948510169983, 0.09110085666179657, -0.02094460465013981, 0.4381689727306366, -0.1592206358909607, 0.26027947664260864, -0.30951517820358276, -0.16486817598342896, 0.0057158088311553, -0.04218393564224243, 0.035610973834991455, 0.06748049706220627, -0.16500402987003326, 0.3984247148036957, 0.3229164779186249, 0.039306432008743286, -0.05396384745836258, 0.10627150535583496, -0.27749499678611755, -0.05454543977975845, -0.041989073157310486, 0.23855853080749512, 0.11828915774822235, -0.2584190368652344], "sparse_embedding": null}, {"id": "4b9cf588260072c8784faf77ea3cf248de31ac491102dcd37aec1eae19fdd159", "content": "Large language models (LLMs) are remarkable data annotators. They can be used\nto generate high-fidelity supervised training data, as well as survey and\nexperimental data. With the widespread adoption of LLMs, human gold--standard\nannotations are key to understanding the capabilities of LLMs and the validity\nof their results. However, crowdsourcing, an important, inexpensive way to\nobtain human annotations, may itself be impacted by LLMs, as crowd workers have\nfinancial incentives to use LLMs to increase their productivity and income. To\ninvestigate this concern, we conducted a case study on the prevalence of LLM\nusage by crowd workers. We reran an abstract summarization task from the\nliterature on Amazon Mechanical Turk and, through a combination of keystroke\ndetection and synthetic text classification, estimate that 33-46% of crowd\nworkers used LLMs when completing the task. Although generalization to other,\nless LLM-friendly tasks is unclear, our results call for platforms,\nresearchers, and crowd workers to find new ways to ensure that human data\nremain human, perhaps using the methodology proposed here as a stepping stone.\nCode/data: https://github.com/epfl-dlab/GPTurk", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2306.07899v1", "title": "Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks", "content": "http://arxiv.org/pdf/2306.07899v1", "datetime": "2023-06-13 16:46:24", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting insights into the impact of Large Language Models (LLMs) on crowdsourcing! \ud83e\udd16\ud83d\udcac\n\nA recent study delved into the prevalence of LLM usage by crowd workers in data annotation tasks. The findings showed that 33-46% of crowd workers leveraged LLMs to enhance their productivity and earnings. This raises important considerations for ensuring the integrity of human-generated data in the era of advanced AI technologies.\n\nFor a detailed overview of the study and its implications, check out the full paper here: http://arxiv.org/abs/2306.07899v1\n\n#LLMs #Crowdsourcing #AI #DataAnnotation #TechResearch #ArtificialIntelligence #NLP\n\nCode and data from the study are available at: https://github.com/epfl-dlab/GPTurk\n\nLet's keep exploring the intersection of human intelligence and AI advancements! \ud83c\udf10\ud83d\udca1\n\n#TechInnovation #Research #DataScience #MachineLearning #SocialMediaExpert #LinkedInPost", "x": "\ud83d\ude80 New research alert! Learn how Large Language Models impact crowd workers and human annotations in AI tasks. A case study found that 33-46% of workers used LLMs on Amazon Mechanical Turk. Check out the study here: http://arxiv.org/abs/2306.07899v1 #AI #LLMs #NLP #Research\n\nCode/data available at: https://github.com/epfl-dlab/GPTurk", "source_id": "4f042a7419d34c4e6c4062b8c5a5a576aa51c7925b8518cda9064fcfed77ea8b", "page_number": 1}, "score": null, "embedding": [-0.3446113169193268, -0.23002922534942627, 0.012291175313293934, -0.01784185878932476, 0.061887066811323166, -0.14213670790195465, -0.03900010883808136, -0.09666896611452103, 0.204078808426857, -0.35064199566841125, 0.11318489909172058, -0.02541397325694561, 0.0545683316886425, 0.31988197565078735, 0.16668209433555603, 0.15436159074306488, -0.11548511683940887, -0.027915509417653084, -0.37181356549263, -0.146237313747406, 0.27379998564720154, -0.1946173757314682, 0.0374549962580204, -0.04342114180326462, -0.16745761036872864, -0.06730791181325912, -0.3916100561618805, -0.15132220089435577, -0.3853778839111328, -1.3043313026428223, 0.27065369486808777, -0.12418132275342941, 0.5888561010360718, 0.11538673937320709, -0.13940002024173737, -0.013710364699363708, 0.045651379972696304, 0.2279273420572281, -0.018917083740234375, 0.3402582108974457, -0.1864681839942932, -0.017929010093212128, 0.11295180767774582, -0.0599459633231163, 0.00028282098355703056, -0.4557562470436096, -0.22024847567081451, -0.13784955441951752, -0.6061627864837646, 0.20582087337970734, -0.05660214275121689, -0.34433886408805847, -0.01999448984861374, 0.3556288182735443, 0.21798790991306305, -0.13541825115680695, 0.20605537295341492, 0.12039349228143692, 0.123565174639225, 0.07941567152738571, 0.315268874168396, 0.2741556763648987, -0.9707534313201904, 0.23759199678897858, -0.2039649337530136, 0.31148678064346313, -0.013288678601384163, 0.05708992853760719, 0.08236779272556305, -0.08451370894908905, 0.003992629237473011, 0.08718644082546234, 0.16205599904060364, 0.21125516295433044, 0.3102879524230957, 0.19895881414413452, 0.08869162201881409, -0.13901132345199585, 0.24206268787384033, -0.12711074948310852, -0.11480497568845749, 0.11646515130996704, 0.05158771947026253, -0.20510786771774292, -0.12209118157625198, 0.044071611016988754, -0.01291557028889656, -0.1612834632396698, 0.22012588381767273, -0.28385576605796814, -0.03398514539003372, -0.0861731767654419, -0.16862259805202484, 0.3380797207355499, -0.037908509373664856, -0.08080446720123291, 0.18850991129875183, 0.1919415444135666, -0.22425305843353271, 0.7163200378417969, -0.1149432510137558, 0.10783172398805618, -0.17472682893276215, -0.1329052597284317, 0.308031290769577, -0.0915079191327095, -0.21907028555870056, -0.17933039367198944, -0.09606579691171646, 0.036141157150268555, 0.03383379429578781, 0.30642542243003845, -0.13253401219844818, -0.06104642525315285, 0.08475696295499802, -0.04546848312020302, 0.4191812574863434, 0.11115454882383347, 0.026502294465899467, 0.2688315808773041, -0.3014450967311859, 0.013860564678907394, 0.020568417385220528, -0.05433191731572151, 0.07844673097133636, 0.02098606340587139, 0.13993684947490692, 0.5672350525856018, 0.4722268879413605, 0.08146361261606216, 0.1919202357530594, 0.03643380478024483, -0.7635665535926819, -0.1835721731185913, 0.0977984219789505, -0.03435052931308746, 0.10357706993818283, -0.15332208573818207, 0.11179787665605545, 0.2688734829425812, 0.05400729924440384, 0.07168533653020859, 0.024660231545567513, -0.0865032896399498, -0.2551039159297943, 0.8387208580970764, 0.04140797257423401, 0.020255066454410553, -0.17023198306560516, -0.18892879784107208, 0.17772434651851654, 0.24832379817962646, -0.08806158602237701, -0.17273738980293274, 0.2926671802997589, 0.1819872111082077, 0.5225852727890015, 0.3579050302505493, -0.44086697697639465, -0.10605181753635406, 0.030162660405039787, -0.053751736879348755, -0.07230647653341293, 0.6261501908302307, -0.13790802657604218, -0.5416116118431091, -0.15161949396133423, -0.024927765130996704, 0.21627604961395264, -0.09526491910219193, 0.1964091807603836, 0.00013336833217181265, -0.2156393975019455, 0.21392865478992462, -0.08830055594444275, 0.15449506044387817, -0.6373661160469055, 0.031450651586055756, -0.09324116259813309, 0.3534698188304901, 0.2086373269557953, -0.20442792773246765, -0.32716450095176697, 0.2307945340871811, -0.18746072053909302, -0.33870649337768555, -0.061574261635541916, -0.18094831705093384, 0.1485944539308548, 0.23110593855381012, 0.11420425772666931, 0.37108272314071655, -0.1445281058549881, -0.15269583463668823, -0.008897148072719574, -0.2471712976694107, -0.2421712726354599, -0.036871060729026794, 0.12302844971418381, -0.2222633957862854, -0.21543444693088531, 0.37343740463256836, 0.09557485580444336, -0.057721517980098724, 0.20669040083885193, 0.05785973370075226, -0.16121800243854523, -0.12434031814336777, 0.40018540620803833, 0.21995654702186584, -0.22510984539985657, -0.04440970718860626, 0.16626648604869843, 0.08152946829795837, -0.22386084496974945, 0.10832910239696503, 0.3946278989315033, 0.29169556498527527, -0.04633978754281998, 0.06783760339021683, 0.1826183795928955, 0.3645966947078705, -0.3138371706008911, -1.2466096878051758, -0.26902568340301514, 0.2769414782524109, 0.06367775797843933, 0.09581676870584488, -0.1892765909433365, 0.03386731445789337, 0.10688621550798416, 0.11048483103513718, 0.2757313549518585, 0.22650489211082458, -0.11429097503423691, -0.27830180525779724, 0.17769014835357666, 0.11043863743543625, 0.18866638839244843, -0.47292187809944153, 0.14301909506320953, -0.13999922573566437, 0.28443530201911926, 0.004606835078448057, -0.08025228977203369, -0.060262635350227356, -0.7217048406600952, 0.17185458540916443, -0.10595270246267319, 0.824509859085083, -0.03957429900765419, -0.26710689067840576, -0.24749481678009033, -0.14620287716388702, 0.38115906715393066, -0.21698446571826935, -0.5825871825218201, 0.4817289710044861, -0.05110285431146622, 0.06775125861167908, -0.20331363379955292, -0.015293385833501816, 0.024064958095550537, -0.32959845662117004, -0.039835549890995026, 0.03348394110798836, -0.5377172231674194, -0.29482850432395935, 0.12174350023269653, -0.06783381849527359, -0.2106010615825653, -0.21927011013031006, 0.11607799679040909, 0.11197817325592041, 0.13411954045295715, 0.4458104968070984, 0.03581525757908821, -0.2481595128774643, -0.1820584088563919, -0.43875783681869507, 0.224492609500885, -0.3073238134384155, 0.11632010340690613, 0.25180986523628235, -0.17363770306110382, -0.08099134266376495, -0.14047937095165253, 0.2826072871685028, -0.337566077709198, -0.26975899934768677, 0.15374855697155, 0.12164565920829773, -0.2099367380142212, -0.20941589772701263, 0.7656469345092773, 0.1086663082242012, 0.17246019840240479, 0.20673975348472595, 0.1393754929304123, -0.037760864943265915, -0.3548777103424072, -0.3242456018924713, 0.005870177410542965, 0.43839791417121887, 0.23729443550109863, -0.06799564510583878, 0.15405678749084473, 0.07291820645332336, 0.2339274287223816, 0.22316308319568634, 0.016170352697372437, 0.31690514087677, -0.06498516350984573, -0.05473890155553818, -0.0306851789355278, -0.4871050715446472, -0.013238859362900257, 0.009911393746733665, 0.04785574600100517, -1.0801700353622437, 0.12449586391448975, -0.10298073291778564, 0.3489472270011902, 0.018372396007180214, 0.04267866536974907, 0.11181516945362091, -0.09761021286249161, 0.1458185613155365, 0.22724418342113495, 0.015566214919090271, 0.1942850798368454, 0.17394177615642548, -0.2557942867279053, 0.1849406510591507, -0.0171201154589653, 0.4604552686214447, -0.04159868136048317, 0.040563032031059265, -0.32808029651641846, -0.07523130625486374, 0.14361122250556946, 1.07389497756958, -0.055578358471393585, -0.093827024102211, 0.26424551010131836, -0.1438254863023758, -0.03456764668226242, 0.33080288767814636, -0.041027627885341644, -0.15815074741840363, -0.09926497936248779, 0.7188711166381836, -0.08635369688272476, -0.09303339570760727, 0.20675408840179443, -0.26541489362716675, -0.19106633961200714, 0.1422611027956009, 0.15751662850379944, 0.24388337135314941, 0.05367119610309601, -0.06600884348154068, -0.30467137694358826, 0.5640103220939636, 0.11038137227296829, 0.12550662457942963, -0.6761223673820496, -0.008328828029334545, 0.06931068748235703, -0.19806279242038727, -0.32931771874427795, -0.23597756028175354, 0.18238139152526855, 0.1516280323266983, 0.18672633171081543, 0.018266892060637474, 0.06081622466444969, -0.09945856779813766, -0.1482730656862259, -0.039544858038425446, -0.1800912767648697, -0.09300395101308823, 0.07241924852132797, -0.13295519351959229], "sparse_embedding": null}, {"id": "395d9979720c3ff49e0dd480e6111855e74b76bf614421992f08252d2758bf35", "content": "Test collections play a vital role in evaluation of information retrieval\n(IR) systems. Obtaining a diverse set of user queries for test collection\nconstruction can be challenging, and acquiring relevance judgments, which\nindicate the appropriateness of retrieved documents to a query, is often costly\nand resource-intensive. Generating synthetic datasets using Large Language\nModels (LLMs) has recently gained significant attention in various\napplications. In IR, while previous work exploited the capabilities of LLMs to\ngenerate synthetic queries or documents to augment training data and improve\nthe performance of ranking models, using LLMs for constructing synthetic test\ncollections is relatively unexplored. Previous studies demonstrate that LLMs\nhave the potential to generate synthetic relevance judgments for use in the\nevaluation of IR systems. In this paper, we comprehensively investigate whether\nit is possible to use LLMs to construct fully synthetic test collections by\ngenerating not only synthetic judgments but also synthetic queries. In\nparticular, we analyse whether it is possible to construct reliable synthetic\ntest collections and the potential risks of bias such test collections may\nexhibit towards LLM-based models. Our experiments indicate that using LLMs it\nis possible to construct synthetic test collections that can reliably be used\nfor retrieval evaluation.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2405.07767v1", "title": "Synthetic Test Collections for Retrieval Evaluation", "content": "http://arxiv.org/pdf/2405.07767v1", "datetime": "2024-05-13 14:11:09", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting development in the field of Information Retrieval! \ud83c\udf10\n\nConstructing test collections for evaluating Information Retrieval (IR) systems can be challenging and resource-intensive. However, a recent study has shown promising results in using Large Language Models (LLMs) to generate fully synthetic test collections, including queries and relevance judgments.\n\nCheck out the research paper to learn more about how LLMs can be leveraged to construct synthetic test collections for IR evaluation: http://arxiv.org/abs/2405.07767v1\n\n#AI #NLP #LLMs #InformationRetrieval #TechResearch #Innovation\n\nLet's continue pushing the boundaries of what is possible in the world of technology! \ud83c\udf1f", "x": "\ud83d\ude80 Exciting research alert! Can Large Language Models (LLMs) be used to construct synthetic test collections for information retrieval systems? Find out more in this comprehensive investigation: http://arxiv.org/abs/2405.07767v1 #AI #NLP #LLMs #TechResearch #InformationRetrieval \ud83e\udd16\ud83d\udcda", "source_id": "7d12be8359cf21dd5c201609fbcf3eb28256e09e53c087e86b609bd458cb926d", "page_number": 1}, "score": null, "embedding": [-0.3488380014896393, 0.3338947594165802, 0.08413302153348923, 0.1096409410238266, 0.12859174609184265, 0.07224198430776596, -0.01973169483244419, 0.03952665254473686, 0.2615777254104614, -0.3726494610309601, -0.19557246565818787, -0.0658123642206192, 0.4689783751964569, 0.444282591342926, 0.10500134527683258, 0.18735061585903168, -0.19112159311771393, 0.16184337437152863, -0.11537201702594757, 0.21857215464115143, 0.3530536890029907, 0.015567806549370289, 0.09974405169487, -0.12686975300312042, -0.27004897594451904, -0.006609133444726467, -0.16978365182876587, -0.04895494133234024, -0.3716340959072113, -1.3569667339324951, -0.011670120060443878, -0.22214464843273163, 0.5758541226387024, 0.17441870272159576, -0.251838356256485, 0.026384949684143066, -0.3789442777633667, 0.09183529764413834, -0.12206558883190155, 0.222330242395401, 0.04671197384595871, 0.06959744542837143, -0.2695982754230499, -0.04594366252422333, -0.14235559105873108, -0.1636113077402115, -0.1882057934999466, -0.044204432517290115, -0.5235347151756287, 0.039605762809515, -0.2204168289899826, -0.22400116920471191, 0.09858459234237671, 0.04045602306723595, -0.0995456725358963, 0.10600141435861588, 0.41527771949768066, 0.19683600962162018, 0.043290261179208755, -0.024271467700600624, 0.4020750820636749, 0.11292839795351028, -0.9541236162185669, 0.05621834471821785, -0.20927223563194275, 0.19989798963069916, -0.17148450016975403, -0.0532916858792305, 0.3597913384437561, -0.02734001725912094, -0.22305770218372345, 0.0759091004729271, 0.0352083295583725, 0.4253425896167755, 0.3314042091369629, 0.2591039836406708, 0.04204750061035156, -0.2515597641468048, 0.2444983720779419, -0.04351162910461426, 0.11477458477020264, -0.11801591515541077, 0.05730168893933296, -0.335130900144577, 0.03875799477100372, -0.04902089014649391, -0.07474987953901291, -0.14868274331092834, 0.31677567958831787, 0.11726601421833038, 0.04883849620819092, -0.040614575147628784, -0.268070250749588, 0.5306349992752075, -0.1726122498512268, -0.22771373391151428, 0.01650707609951496, 0.27687954902648926, 0.0691196396946907, 0.9098931550979614, -0.2553708553314209, 0.22770008444786072, -0.017310015857219696, -0.21228384971618652, -0.06485331058502197, -0.11619209498167038, -0.05232660099864006, -0.38042834401130676, -0.31346723437309265, -0.3374009132385254, -0.12463778257369995, -0.0023636030964553356, 0.1440758854150772, -0.17506574094295502, -0.051845092326402664, 0.2244267463684082, 0.5068951845169067, 0.24543237686157227, -0.18637825548648834, -0.21255581080913544, -0.44904622435569763, 0.249570831656456, 0.02145908959209919, -0.13751260936260223, 0.048569805920124054, -0.08436280488967896, 0.1994674652814865, 0.30844539403915405, 0.04462207853794098, -0.067525714635849, -0.13817381858825684, -0.2127961814403534, -0.49517741799354553, -0.13792206346988678, 0.05293094739317894, -0.0586150586605072, 0.10374259203672409, -0.12497226893901825, 0.10083121806383133, 0.40914273262023926, 0.02487790957093239, 0.10437434911727905, 0.2873607277870178, 0.016661223024129868, -0.5302038788795471, 0.7812649607658386, -0.2387779802083969, 0.1460329294204712, -0.30603528022766113, -0.4594229757785797, 0.19407173991203308, 0.17000405490398407, 0.03666457533836365, -0.3552371859550476, 0.20928163826465607, 0.028148643672466278, 0.10844220221042633, 0.1534939855337143, -0.3364046812057495, -0.046777497977018356, -0.06744931638240814, 0.1636451929807663, -0.16535590589046478, 0.7299871444702148, -0.24239201843738556, -0.11601758748292923, -0.330045610666275, 0.194733127951622, 0.19232487678527832, -0.06628365814685822, 0.25151196122169495, -0.0875217542052269, -0.15758346021175385, 0.05937263369560242, -0.10137379169464111, -0.08342328667640686, -0.5459420084953308, 0.2391051948070526, -0.08339163661003113, 0.03975839540362358, -0.0826185941696167, -0.3775600492954254, -0.0666562169790268, 0.24842970073223114, 0.012023398652672768, -0.2054118812084198, -0.1986255794763565, -0.06462620943784714, 0.15881893038749695, 0.2238858938217163, 0.0011397537309676409, -0.016336433589458466, -0.1729901283979416, 0.05916108936071396, 0.10844114422798157, -0.11273942142724991, 0.005688630510121584, -0.006470251828432083, 0.031125973910093307, -0.09151701629161835, 0.3110295534133911, 0.04448584467172623, 0.07547428458929062, -0.10416844487190247, -0.12628719210624695, 0.10099843144416809, 0.0867394432425499, -0.21056310832500458, 0.15786145627498627, 0.15343661606311798, -0.27474913001060486, -0.05351004749536514, 0.11667660623788834, -0.09801750630140305, -0.2809230387210846, -0.19160744547843933, 0.4983278512954712, 0.058804407715797424, -0.18519049882888794, 0.1967386156320572, -0.10876180231571198, -0.17416787147521973, -0.07102493196725845, -1.251766324043274, 0.0028529127594083548, -0.10652938485145569, 0.23508292436599731, 0.28136420249938965, -0.43402138352394104, -0.09159284085035324, 0.13763858377933502, 0.05648757144808769, 0.33236488699913025, -0.06173934042453766, 0.127684086561203, -0.15982192754745483, 0.19222842156887054, 0.03488975018262863, -0.20652160048484802, -0.2601495087146759, 0.0795099213719368, -0.28513118624687195, 0.28005844354629517, -0.2195361703634262, 0.13771919906139374, 0.40953484177589417, -0.7189937829971313, 0.27436548471450806, -0.067650206387043, 0.7938412427902222, -0.1820748895406723, -0.1861400008201599, -0.0955522358417511, 0.3156414031982422, 0.015473554842174053, -0.14233043789863586, -0.470174103975296, 0.4236376881599426, 0.16063830256462097, 0.04865328595042229, 0.17975959181785583, -0.024292731657624245, -0.03048444166779518, -0.25205811858177185, -0.07079952210187912, 0.2704840898513794, -0.45156630873680115, -0.024688884615898132, -0.10143446922302246, -0.17335158586502075, -0.11156243830919266, -0.15943174064159393, 0.5787137746810913, -0.022822469472885132, 0.13022921979427338, 0.28511950373649597, -0.07216299325227737, -0.07907319813966751, -0.08812040090560913, -0.6732575297355652, -0.23568400740623474, -0.12576401233673096, -0.08591113984584808, 0.09363098442554474, -0.006973947864025831, 0.1932549923658371, -0.209942027926445, 0.3940742015838623, -0.3002214729785919, -0.0441867932677269, 0.227590873837471, 0.13728860020637512, -0.3651493489742279, -0.11532137542963028, 0.5033685564994812, -0.15650424361228943, 0.46052470803260803, -0.10297692567110062, 0.25614988803863525, -0.18861840665340424, -0.16845707595348358, 0.062397658824920654, -0.26182812452316284, 0.4735451936721802, 0.20836223661899567, 0.26533693075180054, 0.1711519956588745, 0.1909405142068863, 0.2940540611743927, 0.455299437046051, 0.26389655470848083, 0.24486373364925385, 0.06997570395469666, 0.02265148051083088, 0.18037837743759155, -0.37122565507888794, -0.10636427998542786, 0.23230724036693573, 0.07448536157608032, -1.1090481281280518, 0.0007897727191448212, -0.09668862819671631, 0.47075343132019043, -0.183078333735466, -0.185586616396904, 0.1352941393852234, -0.1164044439792633, 0.21516424417495728, 0.2672048509120941, 0.1702379435300827, 0.11648046970367432, 0.0677550733089447, -0.1922179013490677, 0.06892789155244827, 0.1530972272157669, 0.3096601963043213, -0.19180628657341003, 0.2188422679901123, -0.5401977300643921, -0.1461281180381775, 0.199270561337471, 1.2229803800582886, 0.024044545367360115, 0.08033087104558945, 0.0452016144990921, -0.033816296607255936, -0.06744752079248428, -0.3986457884311676, -0.11061543971300125, 0.2058483064174652, -0.08716937899589539, 0.6416098475456238, -0.04900269955396652, 0.0280471108853817, 0.42916005849838257, -0.15325504541397095, 0.04093474522233009, 0.10597873479127884, -0.09516562521457672, 0.16639360785484314, -0.060598500072956085, -0.27428698539733887, 0.059949297457933426, 0.717934250831604, 0.2591703534126282, 0.03375529497861862, -0.3580535352230072, -0.1386798620223999, 0.03606738895177841, -0.20098799467086792, 0.030672242864966393, -0.13138806819915771, -0.3030749261379242, 0.475686639547348, 0.23235949873924255, -0.01097577903419733, 0.0442761555314064, 0.02305295132100582, -0.07182201743125916, -0.007515151519328356, 0.2215399146080017, 0.10345771908760071, -0.045392174273729324, -0.2280322015285492], "sparse_embedding": null}, {"id": "0506d5fc2b418d0ae02fd47db31d957a14c15b84dbaf545a1d2536a4da0d47c4", "content": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.11764v1", "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs", "content": "http://arxiv.org/pdf/2402.11764v1", "datetime": "2024-02-19 01:28:48", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting new research alert! \ud83d\ude80\n\nDebiasing Large Language models (LLMs) just got a major boost with a novel approach using ChatGPT to generate synthetic training data. This innovative method aims to enhance debiasing while maintaining multi-task language capabilities. \n\nLearn more about this cutting-edge research on leveraging synthetic data for LLM debiasing and its impressive results here: http://arxiv.org/abs/2402.11764v1\n\n#AI #NLP #LLMs #Debiasing #Research #TechInnovation", "x": "\ud83d\ude80 Exciting new research on debiasing Large Language Models (LLMs) using ChatGPT synthetic data! Learn how this innovative approach enhances debiasing while preserving multi-task capabilities. Find out more at: http://arxiv.org/abs/2402.11764v1 #AI #NLP #LLMs #Debiasing #ChatGPT", "source_id": "fd724ba31967035811fdebbf2096c3575f9048dcd76d0b7761d043291392e054", "page_number": 1}, "score": null, "embedding": [-0.32588520646095276, 0.051456186920404434, 0.17917366325855255, -0.22951076924800873, 0.117042176425457, 0.14475078880786896, -0.34830811619758606, 0.06395481526851654, 0.18563805520534515, -0.21027077734470367, 0.11639576405286789, -0.3040435314178467, 0.2705221474170685, 0.2910913825035095, 0.30288541316986084, 0.12289630621671677, -0.09013062715530396, 0.11092410981655121, -0.04692119359970093, -0.02346149832010269, 0.04542367905378342, -0.2352202832698822, -0.013926483690738678, -0.07263044267892838, -0.01783851534128189, -0.0979059636592865, 0.012478243559598923, 0.04576390981674194, -0.36341941356658936, -1.368696689605713, 0.38847318291664124, 0.1386776864528656, 0.31665974855422974, -0.11464003473520279, -0.2713668942451477, -0.08210886269807816, -0.24885667860507965, 0.20234858989715576, -0.28503385186195374, 0.29566627740859985, -0.07431003451347351, 0.08717773109674454, -0.043008264154195786, -0.23199553787708282, -0.1902022361755371, -0.2423349916934967, -0.2833353579044342, 0.10521257668733597, -0.8590225577354431, -0.10191581398248672, -0.09175914525985718, -0.206577330827713, 0.17945407330989838, -0.04750678315758705, 0.09825583547353745, 0.13287247717380524, 0.16766521334648132, 0.21588070690631866, -0.0024898441042751074, 0.04818909242749214, 0.18430325388908386, 0.16828322410583496, -0.8503208160400391, 0.27962765097618103, -0.15636982023715973, 0.315446138381958, -0.07197972387075424, 0.09036200493574142, 0.10139772295951843, 0.08255224674940109, 0.0651211366057396, 0.07053831219673157, 0.38835835456848145, 0.14494937658309937, 0.297570139169693, 0.37333643436431885, 0.09044014662504196, 0.015277200378477573, 0.23833608627319336, -0.0803898423910141, -0.04617784544825554, -0.03669750690460205, -0.017210938036441803, -0.1693502813577652, -0.04569137841463089, -0.17417578399181366, 0.12867599725723267, -0.07445548474788666, 0.03797706216573715, -0.12783558666706085, -0.15560874342918396, -0.11343099176883698, 0.04201935604214668, 0.02218504250049591, -0.07609432935714722, 0.13019824028015137, -0.018224455416202545, 0.07655686885118484, -0.18733073770999908, 0.7223085165023804, -0.36116132140159607, -0.04461825266480446, -0.3337952196598053, 0.08075466752052307, 0.15521518886089325, -0.27638378739356995, -0.25792697072029114, -0.305022656917572, -0.09612582623958588, -0.033016227185726166, -0.11618635058403015, -0.07134752720594406, 0.11058133095502853, -0.2358003556728363, -0.08530714362859726, 0.16366371512413025, 0.40541815757751465, -0.12099892646074295, -0.16998596489429474, 0.2464878112077713, -0.4958495497703552, 0.34001290798187256, 0.043201252818107605, -0.10014329850673676, 0.1450165957212448, -0.12764357030391693, -0.08505313843488693, 0.6038832664489746, 0.2294897884130478, 0.02282373048365116, 0.15016239881515503, 0.05621354654431343, -0.49052053689956665, -0.0209647323936224, 0.24640455842018127, -0.05564427375793457, 0.1637762039899826, -0.05653715878725052, -0.018783770501613617, 0.1657085418701172, -0.24588564038276672, 0.07139962911605835, 0.15583844482898712, -0.039185017347335815, -0.4650760293006897, 0.4913884103298187, -0.10252974182367325, -0.06718135625123978, -0.1736762374639511, -0.2719017267227173, 0.025988630950450897, 0.20208734273910522, -0.04521575942635536, -0.33948007225990295, 0.33427974581718445, 0.3799770474433899, 0.09871029853820801, 0.281596302986145, -0.364315390586853, -0.05512901768088341, 0.01629631780087948, -0.05234672129154205, 0.03513065725564957, 0.6477655172348022, -0.16995391249656677, -0.3173379898071289, -0.3955928683280945, 0.15259429812431335, 0.23155353963375092, -0.11019016802310944, 0.18231409788131714, -0.10002958029508591, 0.08542238920927048, 0.0071565574035048485, -0.12326250970363617, 0.06384649872779846, -0.7125979661941528, 0.04697170853614807, -0.22725917398929596, 0.2478177845478058, 0.23641876876354218, -0.20152215659618378, -0.11491342633962631, 0.2465081363916397, -0.27203017473220825, -0.3202401399612427, 9.825779852690175e-05, -0.07429111003875732, 0.2089412659406662, 0.15946705639362335, -0.2262963205575943, 0.15104277431964874, 0.18394793570041656, -0.06205473095178604, -0.038176700472831726, -0.23064495623111725, -0.271366685628891, 0.11925449222326279, -0.1448030024766922, 0.00850458163768053, 0.15921244025230408, 0.15296316146850586, -0.25500693917274475, 0.15356482565402985, 0.09767599403858185, -0.10157442837953568, 0.20595204830169678, -0.12488822638988495, 0.48324933648109436, 0.2324099838733673, -0.20093435049057007, 0.026275798678398132, 0.27074408531188965, -0.13025091588497162, 0.0343756340444088, -0.08421745151281357, 0.25944578647613525, 0.4651089012622833, -0.06864844262599945, 0.14965377748012543, -0.07425045967102051, 0.005200178828090429, -0.17853988707065582, -1.261063814163208, -0.21308015286922455, 0.25569236278533936, 0.17137621343135834, -0.0209174994379282, -0.2616691589355469, 0.29086828231811523, 0.016468171030282974, 0.32817575335502625, 0.5816035270690918, 0.24921226501464844, 0.099449023604393, -0.07913340628147125, 0.09466169029474258, 0.19046801328659058, 0.06182221695780754, -0.08513641357421875, 0.17225027084350586, -0.25100672245025635, 0.2010205239057541, -0.12364061921834946, 0.23108893632888794, 0.4050254821777344, -0.4478190541267395, 0.2469176948070526, -0.08966434001922607, 0.7842197418212891, -0.32644128799438477, 0.19434809684753418, -0.03016890399158001, 0.2897610068321228, 0.30291748046875, 0.07076788693666458, -0.4925413131713867, 0.5288856625556946, 0.002056524623185396, 0.3463629186153412, -0.039753902703523636, 0.23143979907035828, -0.02279515564441681, -0.16801582276821136, 0.0040715839713811874, 0.010243145748972893, -0.7246937155723572, -0.2325892448425293, 0.05941680073738098, -0.03919592872262001, -0.26262640953063965, -0.3267791271209717, 0.1961350291967392, 0.03362526744604111, -0.12114184349775314, 0.4477417469024658, -0.013350648805499077, -0.1728479564189911, -0.020860224962234497, -0.9123855233192444, 0.052164457738399506, -0.3676987886428833, -0.13986194133758545, 0.20017562806606293, -0.1320909708738327, -0.06305575370788574, -0.2741776406764984, 0.142153799533844, -0.01292519923299551, 0.062065206468105316, -0.1049845814704895, 0.0463947057723999, 0.0026618249248713255, -0.18198612332344055, 0.6316737532615662, 0.07281547784805298, 0.17332004010677338, -0.008176534436643124, 0.25423893332481384, -0.23671236634254456, -0.2648526728153229, -0.26705268025398254, -0.21516770124435425, 0.7624513506889343, 0.2289218306541443, 0.15375131368637085, 0.23978441953659058, -0.03788452222943306, 0.0650193840265274, 0.2565004825592041, -0.15334200859069824, 0.33097317814826965, 0.169430211186409, -0.1426200270652771, 0.006278471555560827, -0.42821070551872253, -0.1717475950717926, 0.10583905875682831, -0.05688587948679924, -1.094693899154663, -0.11736875027418137, -0.2270958572626114, 0.16866111755371094, 0.049047473818063736, -0.03797833248972893, 0.06025581434369087, -0.07258311659097672, -0.12108274549245834, -0.05459333211183548, -0.12400738894939423, 0.4996959865093231, 0.28875473141670227, -0.10817933082580566, 0.12192212790250778, -0.14937031269073486, 0.3958459198474884, -0.11794828623533249, -0.020740481093525887, -0.3301783502101898, -0.0035616562236100435, -0.11372645199298859, 0.9268903136253357, 0.2042960524559021, 0.07458925247192383, 0.19828300178050995, -0.22934946417808533, 0.0853203758597374, -0.0003293656336609274, 0.07731377333402634, -0.2114633321762085, 0.25683823227882385, 0.6746576428413391, -0.10606779158115387, 0.05072480067610741, 0.4938308596611023, -0.1738308221101761, -0.11823868751525879, 0.3289543092250824, 0.0876530110836029, 0.1058540940284729, 0.19095753133296967, 0.24807460606098175, 0.03962714225053787, 0.5847651362419128, 0.10107679665088654, -0.15597569942474365, -0.34947410225868225, -0.065751813352108, 0.058750543743371964, -0.14361701905727386, 0.2033316045999527, -0.15663428604602814, -0.09334612637758255, 0.1470021903514862, -0.05129378288984299, 0.0700317993760109, -0.02478516660630703, 0.03505818545818329, -0.2259977161884308, 0.16214047372341156, -0.12357186526060104, -0.2121932953596115, -0.018310319632291794, -0.3299744129180908], "sparse_embedding": null}, {"id": "67e9a3baed5e4395e79f512ed4f675c0522d5ed7f2d1bc0e83f6946617293298", "content": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.07849v2", "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations", "content": "http://arxiv.org/pdf/2310.07849v2", "datetime": "2023-10-13 01:31:59", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting insights in AI and NLP research! \ud83e\udd16\n\nCurating high-quality training data is crucial for text classification models, but can be costly and time-consuming. Researchers are now exploring the use of Large Language Models (LLMs) to generate synthetic datasets as a cost-effective alternative. However, the effectiveness of LLM-generated synthetic data varies across different tasks.\n\nIn a recent study, we delved into the impact of subjectivity on model performance when trained on synthetic data. Our findings reveal a negative association between subjectivity levels and model performance. This sheds light on the factors moderating the effectiveness of LLM-generated synthetic data.\n\nFor a deep dive into our study and its implications on leveraging LLMs for synthetic data generation, check out the full article here: http://arxiv.org/abs/2310.07849v2\n\n#AI #NLP #LLMs #Research #Tech #TextClassification #DataScience \n\nLet's stay ahead in the world of AI and NLP together! \ud83c\udf1f\ud83d\udd0d\ud83d\udd2c", "x": "\ud83d\ude80 New research alert! Discover how subjectivity impacts the effectiveness of large language models in generating synthetic data for text classification tasks. \ud83d\udcca Check out the study here: http://arxiv.org/abs/2310.07849v2 #AI #NLP #LLMs #TechResearch", "source_id": "9d03ed4d5bdb93076ba329e3fcda884f6e636fb19dee11e680c4c93da90c8328", "page_number": 1}, "score": null, "embedding": [-0.06270274519920349, 0.14129891991615295, 0.13971006870269775, 0.024773020297288895, 0.2409205138683319, 0.11751479655504227, -0.44341766834259033, 0.08317718654870987, 0.14975927770137787, -0.2747926414012909, -0.06805751472711563, -0.10084991902112961, 0.15312933921813965, 0.3222736418247223, 0.12334565818309784, 0.18766215443611145, -0.027098650112748146, -0.10255811363458633, -0.19528613984584808, -0.13954007625579834, 0.3490787148475647, -0.009035090915858746, -0.027498982846736908, -0.011638441123068333, 0.04519539326429367, -0.1254495233297348, -0.16642192006111145, -0.015260660089552402, -0.4111778140068054, -1.558991551399231, 0.027973683550953865, -0.02944852039217949, 0.4124736189842224, 0.07784028351306915, -0.15026003122329712, -0.004468750208616257, -0.2566208839416504, 0.3089997172355652, -0.10015732795000076, 0.1911439746618271, -0.23413525521755219, -0.005555801093578339, -0.23352089524269104, -0.15534308552742004, 0.14520986378192902, -0.24576517939567566, -0.15772554278373718, -0.15424954891204834, -0.5845125317573547, 0.036552995443344116, -0.16836108267307281, -0.11295165866613388, 0.04163862764835358, 0.3933827877044678, 0.02209611050784588, 0.20567168295383453, 0.2557460069656372, 0.013030116446316242, 0.19616667926311493, 0.005080816335976124, 0.17307545244693756, 0.4204321801662445, -1.0848811864852905, 0.10424838960170746, -0.10688749700784683, 0.21349108219146729, -0.20156753063201904, 0.13180671632289886, 0.20496520400047302, 0.16095541417598724, -0.16840411722660065, -0.054420728236436844, 0.2725929915904999, 0.1213286742568016, 0.3674931824207306, 0.2906312644481659, 0.1385955512523651, -0.059025056660175323, 0.2702150344848633, -0.06163667514920235, 0.12258514761924744, -0.17685210704803467, -0.03566912189126015, -0.43435290455818176, -0.10059576481580734, -0.18975915014743805, 0.09130404144525528, -0.2649790644645691, 0.10698853433132172, 0.08917167782783508, -0.11869445443153381, 0.005404900759458542, -0.31156331300735474, 0.2352982461452484, -0.20408327877521515, 0.08057409524917603, 0.03523573651909828, 0.2572343349456787, -0.2613707482814789, 0.6864578723907471, -0.3970365822315216, -0.11143763363361359, -0.13562722504138947, 0.029005950316786766, 0.19067825376987457, -0.15960034728050232, -0.2593405246734619, -0.06744968891143799, -0.21444113552570343, -0.202550008893013, -0.05531404912471771, 0.09342595189809799, -0.4425014853477478, -0.09803635627031326, -0.057978857308626175, -0.18051956593990326, 0.6934233903884888, 0.03646238520741463, -0.1754896342754364, 0.04308097064495087, -0.24119611084461212, 0.10025126487016678, 0.04093211889266968, -0.18864712119102478, 0.03617647662758827, -0.08814411610364914, 0.13664531707763672, 0.5600436329841614, 0.2054157555103302, -0.10364406555891037, 0.09997303783893585, 0.12214008718729019, -0.6969346404075623, -0.07216918468475342, 0.013333569280803204, -0.24294498562812805, 0.05052966997027397, -0.09051694720983505, 0.006758122239261866, 0.2126707136631012, 0.09143522381782532, 0.20153601467609406, 0.07060746103525162, 0.06496873497962952, -0.6665464639663696, 0.7664282321929932, -0.154449462890625, 0.11751388013362885, -0.26149559020996094, -0.11618376523256302, 0.05774417519569397, 0.2452881634235382, -0.06478971242904663, -0.3447389304637909, 0.274018257856369, 0.2055083066225052, 0.17793084681034088, 0.05935071036219597, -0.4773649573326111, -0.3113879859447479, -0.017750028520822525, 0.0029865822289139032, -0.10153037309646606, 0.8530542850494385, -0.16099411249160767, -0.12886382639408112, -0.25559329986572266, 0.022102240473031998, 0.2527458965778351, -0.07995100319385529, 0.32579505443573, 0.06195077300071716, -0.12004929780960083, 0.23027953505516052, -0.11419341713190079, 0.06435124576091766, -0.6637915968894958, -0.1608750969171524, -0.15968985855579376, 0.16130845248699188, 0.21251355111598969, -0.18767140805721283, -0.27775004506111145, 0.14970043301582336, 0.08295708894729614, -0.16411791741847992, -0.1369430273771286, -0.2506859004497528, 0.19211089611053467, 0.21453550457954407, 0.08355562388896942, 0.11340274661779404, 0.07057289779186249, -0.08084657043218613, -0.06701972335577011, -0.20852522552013397, -0.09483729302883148, 0.018609285354614258, 0.06894030421972275, -0.34860390424728394, 0.0005555838579311967, 0.4815407991409302, 0.19412128627300262, -0.07382447272539139, -0.026267796754837036, 0.02480335533618927, 0.023395109921693802, 0.05755120888352394, 0.4463590383529663, 0.32079362869262695, -0.4372434914112091, -0.06471089273691177, 0.30692580342292786, -0.051021918654441833, -0.06673339009284973, 0.032973144203424454, 0.33929312229156494, 0.2763749957084656, -0.3851875066757202, 0.2492460161447525, 0.01356489397585392, 0.23306594789028168, -0.14977146685123444, -1.3114259243011475, -0.1472245305776596, 0.10630930215120316, 0.1777215301990509, 0.10265380144119263, -0.2829647660255432, -0.04397294670343399, 0.21141701936721802, 0.07169230282306671, 0.48993781208992004, 0.21005931496620178, 0.1189265325665474, -0.2923693060874939, 0.04798387363553047, -0.048500221222639084, 0.04577048495411873, 0.010723423212766647, 0.07038149237632751, -0.26777321100234985, 0.1970251053571701, -0.2573358416557312, 0.02800014428794384, 0.21648457646369934, -0.7389047145843506, 0.10002664476633072, -0.04885248467326164, 0.7000568509101868, -0.36555933952331543, 0.14971166849136353, -0.07177667319774628, 0.09157342463731766, 0.3008616268634796, -0.18164938688278198, -0.44704940915107727, 0.5580542683601379, -0.25778478384017944, 0.21896669268608093, 0.0734545886516571, 0.007310864515602589, -0.13762494921684265, -0.1753910630941391, 0.0020382509101182222, 0.1679728478193283, -0.8076971173286438, -0.20298941433429718, 0.07002092897891998, -0.09104476869106293, -0.12311484664678574, -0.579216480255127, 0.2673088014125824, 0.05670662596821785, 0.23181433975696564, 0.5310488939285278, -0.02039857767522335, -0.15544643998146057, 0.027747297659516335, -0.7601108551025391, 0.21640828251838684, -0.23190926015377045, -0.13446076214313507, 0.35909923911094666, -0.14846716821193695, 0.15931269526481628, -0.33118197321891785, 0.2056354582309723, -0.2940506637096405, -0.24237239360809326, -0.07731153070926666, 0.2207004576921463, -0.047722555696964264, -0.15622638165950775, 0.6295284628868103, -0.008708780631422997, 0.01906733773648739, 0.23553505539894104, 0.07374893128871918, -0.36211323738098145, -0.1332859843969345, -0.18783055245876312, 0.0025451229885220528, 0.45124542713165283, 0.379975825548172, 0.34646540880203247, 0.17578069865703583, 0.08974354714155197, 0.023593194782733917, 0.4204519987106323, 0.10705135762691498, 0.17138563096523285, 0.31022849678993225, 0.07138791680335999, 0.2361060231924057, -0.34942787885665894, -0.024865267798304558, 0.2261020541191101, 0.00872072484344244, -1.1068063974380493, -0.11723178625106812, -0.08120182901620865, 0.5220834612846375, -0.025381125509738922, -0.09126028418540955, 0.15349337458610535, -0.1516149938106537, 0.21919190883636475, 0.2528747022151947, 0.03316116705536842, 0.22964490950107574, 0.30448147654533386, -0.016192035749554634, 0.052628159523010254, -0.06439393013715744, 0.40737292170524597, -0.3319091498851776, 0.16918030381202698, -0.3713148832321167, 0.041701540350914, -0.002915392629802227, 1.0541191101074219, 0.03656702861189842, -0.029520146548748016, 0.21160760521888733, -0.18855388462543488, 0.06872816383838654, -0.007942460477352142, -0.09040651470422745, 0.04540999233722687, 0.18835483491420746, 0.9050746560096741, 0.1542350798845291, 0.010140911675989628, 0.5730359554290771, -0.21247784793376923, -0.010911116376519203, 0.046662356704473495, 0.05336993932723999, 0.4335554838180542, 0.17546112835407257, -0.2795095145702362, -0.18809786438941956, 0.4931975305080414, -0.0016552845481783152, 0.132182776927948, -0.3899199366569519, -0.1828104853630066, -0.005727567709982395, -0.26025664806365967, 0.11296942830085754, -0.1726807802915573, 0.10460752248764038, 0.14781902730464935, 0.2634488642215729, 0.11884079873561859, -0.12111473828554153, 0.04975264519453049, -0.14756746590137482, 0.10776016861200333, -0.1963730901479721, -0.11634528636932373, 0.11153899878263474, -0.32534563541412354], "sparse_embedding": null}, {"id": "0cae8c50bbf22ac6458d849ff0838a346db2433301d97f0d064956c433eb81dc", "content": "As large language models (LLMs) demonstrate unparalleled performance and\ngeneralization ability, LLMs are widely used and integrated into various\napplications. When it comes to sensitive domains, as commonly described in\nfederated learning scenarios, directly using external LLMs on private data is\nstrictly prohibited by stringent data security and privacy regulations. For\nlocal clients, the utilization of LLMs to improve the domain-specific small\nlanguage models (SLMs), characterized by limited computational resources and\ndomain-specific data, has attracted considerable research attention. By\nobserving that LLMs can empower domain-specific SLMs, existing methods\npredominantly concentrate on leveraging the public data or LLMs to generate\nmore data to transfer knowledge from LLMs to SLMs. However, due to the\ndiscrepancies between LLMs' generated data and clients' domain-specific data,\nthese methods cannot yield substantial improvements in the domain-specific\ntasks. In this paper, we introduce a Federated Domain-specific Knowledge\nTransfer (FDKT) framework, which enables domain-specific knowledge transfer\nfrom LLMs to SLMs while preserving clients' data privacy. The core insight is\nto leverage LLMs to augment data based on domain-specific few-shot\ndemonstrations, which are synthesized from private domain data using\ndifferential privacy. Such synthetic samples share similar data distribution\nwith clients' private data and allow the server LLM to generate particular\nknowledge to improve clients' SLMs. The extensive experimental results\ndemonstrate that the proposed FDKT framework consistently and greatly improves\nSLMs' task performance by around 5\\% with a privacy budget of less than 10,\ncompared to local training on private data.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2405.14212v1", "title": "Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data", "content": "http://arxiv.org/pdf/2405.14212v1", "datetime": "2024-05-23 06:14:35", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting developments in AI and privacy protection! \ud83d\udee1\ufe0f Our latest research introduces the Federated Domain-specific Knowledge Transfer (FDKT) framework, enabling secure knowledge transfer from large language models (LLMs) to small language models (SLMs) while safeguarding clients' data privacy. By leveraging LLMs to augment data based on domain-specific few-shot demonstrations synthesized from private domain data using differential privacy, FDKT significantly boosts SLMs' task performance by approximately 5% with a privacy budget of less than 10. Learn more about this cutting-edge approach here: http://arxiv.org/abs/2405.14212v1 #AI #PrivacyProtection #FDKT #LLMs #SLMs #Innovation \ud83c\udf1f", "x": "\ud83d\ude80 Exciting new research on Federated Domain-specific Knowledge Transfer (FDKT) framework for enhancing small language models (SLMs) using large language models (LLMs) while ensuring data privacy. The FDKT framework yields significant performance improvements in domain-specific tasks. Check out the full paper here: http://arxiv.org/abs/2405.14212v1 #AI #NLP #LLMs #PrivacyPreservation", "source_id": "d1eec830ea96e6175d5daf0eaeea1dc6387afac3f7853b84c6a1d44aaf21dbd9", "page_number": 1}, "score": null, "embedding": [-0.0018664882518351078, -0.28536126017570496, -0.03459811583161354, -0.39022448658943176, 0.039996881037950516, -0.03395596146583557, -0.24697910249233246, 0.008915548212826252, 0.3412875533103943, -0.3483099341392517, -0.036198023706674576, -0.035585202276706696, 0.34993329644203186, 0.37368881702423096, -0.007716996595263481, 0.27540600299835205, -0.060614340007305145, 0.14178428053855896, -0.196101576089859, 0.14143170416355133, 0.4379614591598511, -0.22246702015399933, -0.02283213660120964, -0.1030689924955368, -0.14378666877746582, 0.11701454967260361, -0.3223714828491211, -0.08251363039016724, -0.4621949791908264, -1.228664755821228, 0.06191379204392433, -0.3149498403072357, 0.03633798286318779, 0.014492516405880451, -0.03228970617055893, -0.03823177516460419, -0.33058273792266846, 0.19549334049224854, -0.18031227588653564, 0.2902936339378357, 0.1207011267542839, -0.018939055502414703, 0.07678063958883286, 0.10348554700613022, -0.021663827821612358, -0.44227465987205505, -0.151572123169899, -0.045327868312597275, -0.6268388628959656, -0.1609935760498047, -0.039731547236442566, -0.22318756580352783, -0.024956781417131424, 0.293062686920166, -0.01919306069612503, 0.24323870241641998, 0.20530082285404205, 0.114844910800457, 0.15324246883392334, 0.18319623172283173, 0.14639630913734436, 0.5754050612449646, -0.9253599047660828, 0.2997191846370697, -0.05385162681341171, 0.5582111477851868, 0.02194773592054844, -0.02624712511897087, 0.1246078684926033, 0.04185853525996208, -0.08274951577186584, 0.25589197874069214, 0.14958056807518005, 0.2758711874485016, 0.3111904561519623, 0.07408344000577927, -0.10494665801525116, 0.08061373978853226, 6.519559974549338e-05, 0.10008919984102249, 0.17323584854602814, -0.046710461378097534, 0.1558254063129425, -0.341070294380188, -0.2813379466533661, -0.008142131380736828, -0.05625360459089279, -0.3802940249443054, 0.1969297081232071, -0.08421307057142258, -0.31627002358436584, -0.07093087583780289, -0.12952670454978943, 0.08496925979852676, -0.06388124823570251, 0.06497722864151001, 0.13526834547519684, 0.12184558063745499, -0.2712373733520508, 0.5901142358779907, -0.16152429580688477, 0.22616752982139587, -0.18734164535999298, -0.08281099796295166, 0.19751018285751343, 0.10373687744140625, -0.08779538422822952, -0.08132192492485046, -0.01766217313706875, 0.07416125386953354, -0.13550080358982086, -0.019975952804088593, -0.19286103546619415, -0.12187563627958298, 0.026637403294444084, -0.08329260349273682, 0.433657169342041, 0.18755455315113068, -0.23190279304981232, -0.1089581698179245, -0.3183828890323639, 0.17969754338264465, 0.0712619423866272, -0.06242280825972557, 0.1997007131576538, -0.07848446816205978, 0.04688981547951698, 0.37436166405677795, 0.20216840505599976, 0.24755463004112244, 0.3010520339012146, -0.1831343024969101, -0.34971654415130615, -0.29911088943481445, 0.20459719002246857, -0.10300134122371674, -0.06593898683786392, -0.23976564407348633, -0.05938855558633804, 0.09369409084320068, -0.028247429057955742, -0.03039564937353134, 0.3474371135234833, -0.09208719432353973, -0.1865982711315155, 0.6941249370574951, 0.1644814908504486, 0.06874614208936691, -0.4402334988117218, 0.0573623962700367, 0.2692042589187622, 0.3419731855392456, -0.17056021094322205, -0.26807376742362976, 0.21926164627075195, -0.009745952673256397, 0.36353135108947754, 0.2585037350654602, -0.3672780990600586, -0.1603499799966812, 0.2321028709411621, -0.0066455816850066185, 0.21106761693954468, 0.8135752081871033, -0.1387784630060196, -0.504119873046875, -0.09140407294034958, 0.2535920739173889, 0.26875200867652893, -0.3861630856990814, 0.04775552824139595, 0.0358419232070446, 0.020295122638344765, -0.15400028228759766, -0.27913305163383484, 0.019445544108748436, -0.5610476136207581, -0.06975193321704865, -0.14008352160453796, 0.03782747685909271, -0.07923828065395355, -0.1679447740316391, 0.0781046599149704, 0.10776612907648087, -0.00943643320351839, -0.2856330871582031, -0.009048286825418472, -0.2589224874973297, -0.15271425247192383, 0.16019944846630096, -0.6073534488677979, 0.25818532705307007, -0.1859704852104187, -0.06440958380699158, -0.05282454565167427, -0.07639405131340027, -0.19144895672798157, 0.1256810426712036, 0.04189349338412285, -0.15607839822769165, 0.1702508181333542, 0.43651849031448364, -0.050146155059337616, 0.0030639697797596455, -0.06852306425571442, -0.08631126582622528, 0.09061672538518906, -0.07052945345640182, 0.008337709121406078, 0.22624611854553223, -0.2909858524799347, 0.16017583012580872, 0.06544452160596848, -0.08224746584892273, -0.1512220948934555, 0.0931476354598999, 0.2558528184890747, 0.23493321239948273, -0.08136893063783646, -0.11061280220746994, 0.16442252695560455, 0.3064942955970764, -0.2697668671607971, -1.081972360610962, -0.1622379869222641, -0.001165009685792029, 0.07061809301376343, 0.3798975348472595, -0.2837832272052765, 0.13291357457637787, 0.18338046967983246, 0.12114433199167252, 0.33206936717033386, 0.45937079191207886, -0.14575223624706268, -0.43504586815834045, 0.2519753873348236, -0.01987687312066555, -0.12025386840105057, 0.05041075870394707, 0.18905553221702576, -0.10690484195947647, 0.24657483398914337, -0.20637980103492737, 0.0640764981508255, 0.3054620325565338, -0.582862377166748, 0.15220873057842255, -0.1110413447022438, 0.777820885181427, -0.27181321382522583, 0.06413227319717407, -0.3847557306289673, 0.1366218477487564, 0.2241085022687912, 0.1346813589334488, -0.6637555956840515, 0.19877523183822632, -0.20870846509933472, 0.13897062838077545, 0.1448378562927246, 0.2022646814584732, -0.06444039195775986, -0.01814867928624153, -0.0800171047449112, 0.11319346725940704, -0.6580791473388672, -0.2843056619167328, -0.017341021448373795, -0.161077082157135, -0.18204215168952942, -0.1974419504404068, 0.41611814498901367, 0.0008652808028273284, 0.0004314785765018314, 0.431703120470047, 0.022824835032224655, -0.09337665885686874, -0.09430709481239319, -0.31699565052986145, -0.040385354310274124, -0.046786025166511536, 0.1944768875837326, 0.06855501979589462, -0.17938131093978882, 0.06755605340003967, -0.35940080881118774, 0.23009246587753296, -0.5200045108795166, -0.13341552019119263, 0.011629948392510414, 0.34154894948005676, -0.12603461742401123, -0.11578470468521118, 0.5252876281738281, 0.2854141592979431, 0.2990737855434418, -0.05542850121855736, 0.2619436979293823, -0.18392308056354523, 0.09554101526737213, -0.5701740384101868, 0.03828045353293419, 0.7851638197898865, 0.12759895622730255, 0.12613926827907562, 0.16988828778266907, 0.16533394157886505, 0.015648256987333298, 0.36821576952934265, 0.341086208820343, -0.007284143008291721, -0.028501441702246666, -0.1544509381055832, -0.1855214685201645, -0.27995654940605164, -0.10591987520456314, 0.31320393085479736, -0.12420165538787842, -1.187362551689148, -0.15998056530952454, -0.24923844635486603, 0.3358149230480194, -0.09949982166290283, 0.10111886262893677, 0.33313459157943726, -0.062284089624881744, 0.0318102203309536, 0.21185985207557678, -0.017580842599272728, 0.18982431292533875, 0.20914682745933533, -0.016342034563422203, 0.18711133301258087, -0.0972285196185112, 0.2706458866596222, -0.040741149336099625, -0.12210414558649063, -0.33142784237861633, -0.14289197325706482, 0.304036408662796, 1.063277244567871, 0.013670734129846096, -0.1105443462729454, 0.09344141185283661, -0.001898284419439733, 0.29410016536712646, 0.03462962433695793, 0.070943184196949, -0.06501179933547974, 0.14653079211711884, 0.3886248469352722, 0.014470587484538555, 0.007521524094045162, 0.46660947799682617, -0.07186967134475708, -0.14854401350021362, 0.10957928001880646, 0.10879562795162201, 0.3840491473674774, -0.37589457631111145, 0.015595493838191032, 0.33202502131462097, 0.5523245334625244, 0.1039910688996315, -0.0010345788905397058, -0.3257797658443451, -0.08004741370677948, 0.011420954018831253, -0.11562198400497437, -0.3104763925075531, 0.055321916937828064, -0.13842365145683289, 0.2566399574279785, -0.08961454778909683, 0.16727067530155182, 0.06798906624317169, -0.09915726631879807, -0.14215803146362305, 0.12013955414295197, -0.20487233996391296, -0.27604371309280396, 0.0023189750500023365, -0.2501605749130249], "sparse_embedding": null}, {"id": "f5dcb530892c4205aa99ce4227281fd4b1f09854f05c6bcebadbdcb19ba78827", "content": "Text data has become extremely valuable due to the emergence of machine\nlearning algorithms that learn from it. A lot of high-quality text data\ngenerated in the real world is private and therefore cannot be shared or used\nfreely due to privacy concerns. Generating synthetic replicas of private text\ndata with a formal privacy guarantee, i.e., differential privacy (DP), offers a\npromising and scalable solution. However, existing methods necessitate DP\nfinetuning of large language models (LLMs) on private data to generate DP\nsynthetic data. This approach is not viable for proprietary LLMs (e.g.,\nGPT-3.5) and also demands considerable computational resources for open-source\nLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)\nalgorithm to generate DP synthetic images with only API access to diffusion\nmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, that\napplies to the complex setting of text. We use API access to an LLM and\ngenerate DP synthetic text without any model training. We conduct comprehensive\nexperiments on three benchmark datasets. Our results demonstrate that Aug-PE\nproduces DP synthetic text that yields competitive utility with the SOTA DP\nfinetuning baselines. This underscores the feasibility of relying solely on API\naccess of LLMs to produce high-quality DP synthetic texts, thereby facilitating\nmore accessible routes to privacy-preserving LLM applications. Our code and\ndata are available at https://github.com/AI-secure/aug-pe.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.01749v1", "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text", "content": "http://arxiv.org/pdf/2403.01749v1", "datetime": "2024-03-04 05:57:50", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting developments in the realm of privacy-preserving text data generation! Researchers have introduced the Aug-PE algorithm, a novel approach that leverages API access to large language models (LLMs) to create differential privacy (DP) synthetic text without the need for model training. This breakthrough offers a scalable solution for generating high-quality synthetic text data while ensuring formal privacy guarantees.\n\nThe study conducted by Lin et al. (2024) presents compelling results, showcasing the efficacy of Aug-PE in producing DP synthetic text that rivals state-of-the-art DP finetuning methods. By making use of API access to LLMs, this innovative algorithm paves the way for more accessible and efficient privacy-preserving LLM applications.\n\nFor those interested in delving deeper into the research and exploring the code and data, check out the full paper at: http://arxiv.org/abs/2403.01749v1\n\n#AI #NLP #LLMs #PrivacyPreservation #TextGeneration #TechInnovation", "x": "\ud83d\ude80 Exciting innovation in privacy-preserving text generation! Check out the Aug-PE algorithm, allowing for the generation of DP synthetic text without model training. Results show competitive utility with SOTA methods. Learn more at: http://arxiv.org/abs/2403.01749v1 #AI #NLP #LLMs #PrivacyPreservation", "source_id": "bf028c6b16923f1bcb25f610c51e24a03d4874eef86fc47f76547e762001971a", "page_number": 1}, "score": null, "embedding": [-0.4291870594024658, -0.10987675935029984, -0.10655142366886139, -0.2902047634124756, 0.06917479634284973, 0.10741641372442245, -0.3048669397830963, 0.13748416304588318, 0.3194192945957184, -0.20526398718357086, 0.11524054408073425, -0.21398039162158966, 0.26470956206321716, -0.03159294277429581, 0.10704893618822098, 0.29436033964157104, -0.07339855283498764, -0.1275624930858612, -0.2935941219329834, 0.023414863273501396, 0.467397540807724, -0.07375524193048477, 0.02591385506093502, -0.4178381562232971, -0.13314229249954224, 0.02522156946361065, -0.10566651821136475, 0.05321599915623665, -0.2521522641181946, -1.3572381734848022, 0.24597328901290894, -0.24317197501659393, 0.24765080213546753, -0.09379339218139648, -0.16382862627506256, 0.030122822150588036, -0.37700414657592773, 0.40784740447998047, -0.2829825282096863, 0.16309323906898499, -0.20305736362934113, -0.11165878921747208, -0.18599894642829895, 0.004911675583571196, 0.1675688922405243, -0.23902961611747742, -0.3163624405860901, -0.08138668537139893, -0.47089409828186035, -0.049021657556295395, 0.00911865383386612, 0.19452030956745148, -0.04176080599427223, 0.2863551378250122, 0.0995720699429512, 0.04512610286474228, 0.3116237223148346, 0.009745021350681782, 0.024864155799150467, 0.1554258018732071, 0.045125335454940796, 0.4537762999534607, -0.9542045593261719, 0.30536195635795593, -0.10309531539678574, 0.37561771273612976, -0.3148249089717865, 0.01182633824646473, 0.3182671070098877, -0.17746366560459137, -0.034210409969091415, 0.3457828462123871, 0.19274236261844635, 0.11701195687055588, 0.2967613935470581, 0.4526892900466919, 0.14556987583637238, -0.1896955519914627, 0.18537898361682892, -0.0026341222692281008, 0.10411114990711212, 0.06697925180196762, 0.03545455262064934, -0.14898960292339325, -0.12389160692691803, -0.044797539710998535, 0.13563811779022217, -0.30560970306396484, 0.2439599484205246, -0.20049236714839935, -0.21346507966518402, 0.003440586384385824, -0.023229064419865608, 0.1083725094795227, -0.17393572628498077, 0.14789240062236786, -0.1009591668844223, 0.014839082024991512, -0.1387665569782257, 0.5459895133972168, -0.3220401108264923, 0.08064356446266174, -0.10834489017724991, 0.06329837441444397, 0.17732690274715424, -0.16440263390541077, -0.2024739533662796, -0.13588008284568787, -0.30839791893959045, 0.04414628818631172, 0.09034716337919235, -0.0042667207308113575, -0.02077212929725647, 0.07876367121934891, -0.23202067613601685, -0.10184548795223236, 0.4014061391353607, -0.026573877781629562, -0.11782129853963852, -0.10641442239284515, -0.17336821556091309, 0.2785351872444153, -0.10698242485523224, 0.1374308466911316, 0.15030640363693237, -0.14662933349609375, 0.04933831840753555, 0.5299703478813171, 0.09269892424345016, 0.03547108918428421, 0.13977181911468506, 0.09054730087518692, -0.4407247006893158, -0.26264262199401855, 0.008573111146688461, -0.22195734083652496, -0.179942324757576, -0.21157827973365784, 0.1451796442270279, -0.07002151012420654, -0.16032463312149048, 0.1953897476196289, 0.18572649359703064, -0.009748974815011024, -0.3286805748939514, 0.6647255420684814, 0.24512174725532532, 0.20701999962329865, -0.3691357970237732, -0.3292267918586731, 0.1739971935749054, 0.10997285693883896, -0.1480848491191864, -0.15620344877243042, 0.41794437170028687, -0.129311665892601, 0.16523617506027222, 0.30370956659317017, -0.32480406761169434, 0.11734195798635483, 0.06160939857363701, 0.11047782003879547, -0.029638485983014107, 1.1148216724395752, -0.15124404430389404, -0.491580992937088, 0.02030581422150135, 0.09326091408729553, 0.24673138558864594, -0.1521129310131073, 0.2810158431529999, -0.014492124319076538, -0.0927080363035202, 0.2647082209587097, -0.14085876941680908, 0.04503662511706352, -0.6340011954307556, -0.11973420530557632, -0.20081526041030884, 0.23059935867786407, -0.2658504247665405, -0.2809734344482422, -0.2305474579334259, 0.14386354386806488, -0.004263830371201038, -0.26218387484550476, -0.08067622780799866, -0.21307089924812317, 0.0563577301800251, 0.3091602027416229, -0.04165572300553322, 0.3113554120063782, -0.2025000900030136, -0.0062160673551261425, -0.11918268352746964, -0.3166021406650543, -0.05704604089260101, 0.08463772386312485, 0.12192314118146896, -0.16213534772396088, -0.05256020277738571, 0.25733765959739685, 0.05651348829269409, -0.17579641938209534, -0.2555276155471802, -0.060671109706163406, 0.033597178757190704, 0.050303276628255844, 0.19239823520183563, 0.3228408396244049, -0.2648454010486603, -0.0837264284491539, 0.17422328889369965, -0.13085836172103882, -0.3300672173500061, 0.23089885711669922, 0.11121384799480438, 0.10290537774562836, -0.18665246665477753, 0.1876850426197052, -0.13844554126262665, 0.14772078394889832, -0.15804730355739594, -1.0019162893295288, -0.24304518103599548, -0.0430770106613636, -0.006932568736374378, 0.2990112900733948, -0.3786335289478302, 0.04134431481361389, -0.17035983502864838, 0.2794581353664398, 0.5465908050537109, 0.511677086353302, -0.01880510151386261, -0.21504449844360352, 0.2691400945186615, -0.06348750740289688, 0.10081207007169724, 0.08856707811355591, 0.32525166869163513, 0.008833813481032848, 0.05369657278060913, -0.3056578040122986, 0.04298987612128258, 0.20433880388736725, -0.6390594244003296, 0.2881365716457367, -0.11434875428676605, 0.8873940706253052, -0.07332039624452591, 0.18176116049289703, -0.027786798775196075, 0.09421129524707794, 0.2616358995437622, -0.05374443158507347, -0.7932963967323303, 0.1744554489850998, -0.12674029171466827, -0.14205028116703033, 0.13017183542251587, 0.15669944882392883, -0.24577219784259796, -0.008466826751828194, 0.1583413928747177, -0.08300764858722687, -0.7071980834007263, -0.14798232913017273, -0.25206419825553894, -0.14316387474536896, -0.1506551206111908, -0.27692902088165283, 0.4335053861141205, 0.4295061230659485, 0.18403953313827515, 0.5424576997756958, -0.10213746875524521, -0.13983695209026337, -0.33935508131980896, -0.4263472259044647, -0.06249784678220749, -0.2568284869194031, -0.0356982946395874, 0.10668236017227173, -0.04564942792057991, -0.02633473090827465, -0.3580663800239563, 0.084502212703228, -0.24960504472255707, -0.09052518010139465, -0.22946877777576447, 0.22510388493537903, -0.023604650050401688, -0.010549165308475494, 0.6323372721672058, 0.028115004301071167, 0.12440488487482071, 0.09227535128593445, 0.17903591692447662, -0.10355297476053238, 0.06712745130062103, -0.2281772345304489, -0.2730778455734253, 0.5558294057846069, 0.40309250354766846, 0.3375866115093231, 0.284316748380661, -0.017635103315114975, 0.35562512278556824, 0.35547178983688354, 0.19088782370090485, 0.05744222179055214, 0.11325077712535858, 0.09826018661260605, 0.1378936916589737, -0.39805611968040466, -0.1277625560760498, 0.3859047591686249, 0.08279023319482803, -1.173597812652588, -0.06245243176817894, -0.23295186460018158, 0.2977347671985626, -0.21436727046966553, 0.15064644813537598, 0.3592328727245331, 0.025720743462443352, -0.04417584463953972, 0.10748082399368286, -0.22466999292373657, 0.21504609286785126, 0.16626189649105072, -0.18065838515758514, 0.29660651087760925, -0.11958146840333939, 0.2998022139072418, -0.05589418485760689, -0.058314140886068344, -0.3450494408607483, 0.09343080222606659, -0.1260197013616562, 1.0226726531982422, -0.27794837951660156, -0.17982834577560425, 0.11474978178739548, -0.00018451843061484396, 0.27729955315589905, -0.20018908381462097, -0.06905008107423782, -0.24543032050132751, 0.1257123351097107, 0.47458240389823914, -0.0738857164978981, 0.18780289590358734, 0.4559614360332489, -0.1848403662443161, -0.05029934644699097, -0.05107022076845169, 0.08456479758024216, 0.24269047379493713, 0.050148479640483856, -0.08686771243810654, 0.0734742134809494, 0.23227143287658691, 0.29064884781837463, -0.01302128192037344, -0.14134006202220917, -0.0383564792573452, 0.08114472776651382, -0.24440282583236694, 0.25465887784957886, -0.2311541736125946, 0.003496558405458927, 0.33859071135520935, 0.37230122089385986, -0.06511722505092621, 0.09196515381336212, 0.029610037803649902, -0.07656556367874146, 0.1386222094297409, -0.13142940402030945, -0.11735451221466064, 0.4006313979625702, -0.19547995924949646], "sparse_embedding": null}, {"id": "8ed56cc0d9b21ca0436d41766433160218bdb454042088643fe04a91668ae8ef", "content": "Large language models (LLMs) suffer from catastrophic forgetting during\ncontinual learning. Conventional rehearsal-based methods rely on previous\ntraining data to retain the model's ability, which may not be feasible in\nreal-world applications. When conducting continual learning based on a\npublicly-released LLM checkpoint, the availability of the original training\ndata may be non-existent. To address this challenge, we propose a framework\ncalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\ninstances for rehearsal. Concretely, we first employ the base LLM for\nin-context learning to generate synthetic instances. Subsequently, we utilize\nthe latest LLM to refine the instance outputs based on the synthetic inputs,\npreserving its acquired ability. Finally, we select diverse high-quality\nsynthetic instances for rehearsal in future stages. Experimental results\ndemonstrate that SSR achieves superior or comparable performance compared to\nconventional rehearsal-based approaches while being more data-efficient.\nBesides, SSR effectively preserves the generalization capabilities of LLMs in\ngeneral domains.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.01244v2", "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal", "content": "http://arxiv.org/pdf/2403.01244v2", "datetime": "2024-05-25 12:17:29", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting Breakthrough in AI Research! \ud83d\ude80\n\nContinual learning with Large Language Models (LLMs) just got a major boost! Conventional methods face challenges with catastrophic forgetting, but a new framework called Self-Synthesized Rehearsal (SSR) is changing the game.\n\n\ud83d\udd0d SSR leverages LLMs to generate synthetic instances for rehearsal, overcoming the need for previous training data. By refining instance outputs based on synthetic inputs, SSR maintains and even enhances the model's abilities. Experimental results show SSR outperforms traditional methods while being more data-efficient.\n\nRead more about this groundbreaking research at: http://arxiv.org/abs/2403.01244v2\n\n#AI #LLM #ContinualLearning #TechInnovation #ArtificialIntelligence #NLP", "x": "\ud83d\ude80 New research in continual learning for Large Language Models (LLMs)! Introducing Self-Synthesized Rehearsal (SSR) framework that generates synthetic instances for rehearsal to address catastrophic forgetting. \ud83e\udde0\ud83d\udca1 Superior performance while being data-efficient! Check out the details at: http://arxiv.org/abs/2403.01244v2 #AI #NLP #LLMs #Research #Tech #Innovation \ud83e\udd16\ud83d\udcda", "source_id": "717173ed3ceacb92f57fe1d8ebc62114141b38000530f44835da0d40212008de", "page_number": 1}, "score": null, "embedding": [-0.3815949857234955, 0.06480429321527481, 0.19226329028606415, -0.19486968219280243, -0.1329120397567749, 0.2341940999031067, -0.300415962934494, -0.11995147168636322, 0.47389456629753113, -0.4108138084411621, 0.023740801960229874, -0.08459772169589996, 0.49150630831718445, 0.39763614535331726, 0.17734669148921967, 0.1548987179994583, -0.2075279802083969, 0.24429327249526978, 0.19359040260314941, -0.17367056012153625, 0.22768446803092957, -0.23611557483673096, -0.10102922469377518, 0.0876164510846138, 0.08585672080516815, -0.14963848888874054, -0.12291835993528366, -0.09154842048883438, -0.14427873492240906, -1.5243940353393555, 0.1439262479543686, -0.00945792905986309, 0.02101890742778778, 0.09695371240377426, -0.4352850914001465, -0.10674527287483215, -0.33754295110702515, 0.22151640057563782, -0.06996330618858337, 0.320064514875412, -0.1487892121076584, 0.28739333152770996, -0.20005619525909424, -0.14138883352279663, 0.0655912533402443, -0.34857189655303955, 0.06540517508983612, -0.062484871596097946, -0.4131771922111511, 0.039938535541296005, -0.09096451103687286, -0.1401703804731369, 0.32762956619262695, 0.16088616847991943, -0.041162822395563126, -0.07531983405351639, 0.27346256375312805, 0.508137583732605, 0.13399171829223633, -0.10701637715101242, 0.24123068153858185, 0.1818958967924118, -1.0153578519821167, -0.028511352837085724, -0.15274623036384583, 0.19014225900173187, 0.09700212627649307, -0.035608287900686264, 0.2627652883529663, 0.34113427996635437, -0.28046929836273193, 0.1639130413532257, 0.09119667857885361, 0.23368367552757263, 0.390934556722641, 0.06547007709741592, 0.1536460518836975, -0.027722228318452835, 0.271510511636734, 0.067971371114254, 0.029480233788490295, -0.09351667016744614, -0.03511686250567436, -0.2900681793689728, -0.306826651096344, 0.03291292116045952, 0.045743782073259354, -0.08976680040359497, 0.3472965359687805, 0.004233037121593952, -0.24111708998680115, -0.17586082220077515, -0.08951551467180252, 0.10079789906740189, -0.17688381671905518, 0.04064788669347763, 0.08218761533498764, 0.24861085414886475, -0.2315533608198166, 0.6913825869560242, -0.12264090776443481, -0.04035515710711479, 0.026806537061929703, 0.26600098609924316, 0.0565749891102314, -0.1753973364830017, -0.09875059872865677, -0.19543348252773285, -0.34337949752807617, 0.0001751295494614169, -0.043501585721969604, 0.03374633193016052, -0.12496698647737503, -0.12396170198917389, -0.19031009078025818, 0.07472354173660278, 0.5566895604133606, 0.15415926277637482, -0.09711437672376633, 0.06125941127538681, -0.1969841569662094, 0.12737131118774414, 0.0341527983546257, -0.3091490566730499, 0.07946854084730148, -0.19740581512451172, -0.31737035512924194, 0.5468471646308899, 0.30615848302841187, -0.17331373691558838, 0.17046408355236053, -0.16188128292560577, -0.4792076647281647, -0.2583213150501251, 0.03489493951201439, -0.3652550280094147, 0.03836121782660484, -0.20715701580047607, 0.21066975593566895, -0.26938167214393616, -0.0300836730748415, 0.18060874938964844, 0.37979012727737427, -0.12495606392621994, -0.4780987501144409, 0.7855582237243652, -0.005918177310377359, 0.20133517682552338, -0.2752525806427002, -0.06878572702407837, -0.17667530477046967, 0.06894530355930328, -0.15362440049648285, -0.449443519115448, 0.27874717116355896, 0.313068687915802, -0.03866118565201759, 0.34341299533843994, -0.1381978690624237, -0.019244801253080368, -0.19698505103588104, 0.0806795135140419, 0.15724073350429535, 0.6865642666816711, -0.1336449831724167, -0.3644867241382599, -0.34122323989868164, 0.20482033491134644, 0.3225537836551666, -0.003311082487925887, 0.08256680518388748, 0.15858447551727295, -0.09862834215164185, -0.24516849219799042, -0.1319543868303299, 0.19700437784194946, -0.506624698638916, -0.08033329993486404, 0.05316763371229172, 0.2715863287448883, -0.00831281766295433, -0.45575788617134094, -0.16596341133117676, 0.16241823136806488, 0.03593279421329498, -0.22044290602207184, 0.008257448673248291, -0.40797513723373413, 0.1915058046579361, 0.30494722723960876, -0.05316852405667305, 0.33204180002212524, 0.1417284458875656, 0.07425494492053986, -0.1890280544757843, -0.2339821606874466, -0.09361395239830017, -0.26228058338165283, 0.08047778159379959, -0.2114747315645218, -0.151285782456398, 0.21284961700439453, -0.10019262880086899, -0.037516772747039795, 0.5070717930793762, -0.07609246671199799, 0.11105524003505707, -0.021576933562755585, 0.40782904624938965, 0.46326079964637756, -0.4692530930042267, -0.031556643545627594, 0.3518272936344147, 0.17941616475582123, -0.16354985535144806, 0.06488178670406342, -0.08416160941123962, 0.2181725949048996, -0.14142845571041107, 0.1431400328874588, 0.2656033933162689, 0.30377933382987976, -0.3148035407066345, -1.189820647239685, -0.26587891578674316, 0.13522718846797943, 0.11034277081489563, 0.24921363592147827, -0.21323524415493011, 0.07379855215549469, 0.3569934666156769, 0.24745362997055054, 0.10661152005195618, 0.19020481407642365, 0.10960575193166733, -0.3891333043575287, 0.17518694698810577, -0.23019260168075562, -0.185659259557724, -0.0910206213593483, 0.05054658651351929, -0.1772468239068985, 0.23839685320854187, -0.05673041194677353, 0.30481311678886414, -0.001411384204402566, -0.7177629470825195, 0.2211897075176239, -0.12979069352149963, 0.7815149426460266, -0.3351130783557892, 0.44512268900871277, -0.03427871689200401, 0.05373283475637436, 0.32484540343284607, -0.06567975878715515, -0.5723230838775635, 0.031065288931131363, -0.08511289209127426, 0.1277192384004593, 0.2182278037071228, 0.2387673556804657, -0.12589794397354126, -0.08538012206554413, 0.005004719831049442, -0.05623563006520271, -0.6879569888114929, -0.3112315833568573, 0.0701124295592308, -0.1799256056547165, -0.002685467479750514, -0.31907257437705994, 0.2120063304901123, 0.14446358382701874, -0.16052353382110596, 0.31047794222831726, -0.04346247389912605, -0.19732795655727386, 0.039905846118927, -0.5399602651596069, -0.01085247378796339, -0.2836478352546692, -0.09062456339597702, 0.33152347803115845, 0.11335544288158417, -0.04822571948170662, -0.15194369852542877, 0.2877901494503021, -0.19033488631248474, 0.17662979662418365, -0.005756495054811239, 0.3739493191242218, -0.20545907318592072, -0.2630162239074707, 0.6633395552635193, 0.06610127538442612, -0.02944989502429962, -0.017443252727389336, 0.03427690640091896, -0.2782320976257324, -0.4957210421562195, -0.478443443775177, 0.09344061464071274, 0.32676950097084045, 0.08853519707918167, 0.12433360517024994, 0.06403296440839767, 0.07743700593709946, 0.4215780794620514, 0.4462019205093384, 0.31928783655166626, 0.2995237410068512, 0.20257875323295593, -0.041226089000701904, 0.08695374429225922, -0.2819094657897949, -0.009819935075938702, 0.21391455829143524, -0.15381920337677002, -1.2332829236984253, -0.0823952928185463, -0.3008321523666382, 0.5679198503494263, -0.22350525856018066, 0.03909822180867195, -0.0714196190237999, 0.05941230058670044, -0.2319229692220688, 0.41263094544410706, -0.16413334012031555, 0.09723145514726639, 0.22221292555332184, 0.20576012134552002, 0.16814863681793213, 0.01008895318955183, 0.5818696022033691, -0.16324689984321594, 0.23443400859832764, -0.7251099944114685, 0.1876894235610962, 0.08992055803537369, 0.9023405909538269, 0.09364891052246094, 0.12367826700210571, 0.22346776723861694, -0.04174236208200455, 0.06123462691903114, 0.04610196873545647, -0.0604214109480381, -0.15033093094825745, -0.029469387605786324, 0.6219382286071777, -0.03382406011223793, 0.18635229766368866, 0.05671006441116333, -0.2922412157058716, 0.1133187785744667, 0.14279380440711975, 0.054897673428058624, -0.040461424738168716, -0.0005501809646375477, -0.021324628964066505, -0.00017902693070936948, 0.8350028395652771, 0.08942637592554092, 0.24378006160259247, -0.4989364445209503, -0.1790555864572525, -0.08070351928472519, -0.26075732707977295, -0.06886997073888779, 0.035309530794620514, -0.10438535362482071, 0.026014454662799835, 0.10481622815132141, 0.14783349633216858, -0.05077042803168297, 0.041025999933481216, -0.07020047307014465, 0.1952841728925705, -0.1718677431344986, 0.26268866658210754, -0.027470173314213753, -0.1648205816745758], "sparse_embedding": null}, {"id": "eca4f4d2c710dc7a6914f4a5bad442dbfa5291da3e9432a945b3429952cd743e", "content": "Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.17860v2", "title": "Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications", "content": "http://arxiv.org/pdf/2403.17860v2", "datetime": "2024-04-02 12:25:57", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting new study alert! Researchers explore the use of Large Language Models (LLMs) for data augmentation to tackle high confidence errors in NLP models during classification tasks. Find out how synthetic data generated by LLMs compares to human data in mitigating misclassifications. Results show a significant reduction in high confidence errors while maintaining accuracy levels. Plus, LLMs prove to be a more cost-effective and scalable solution. Check out the full study here: http://arxiv.org/abs/2403.17860v2 #AI #NLP #LLMs #DataAugmentation #ResearchStudy #TechInnovation \ud83c\udf1f", "x": "\ud83d\ude80 Exciting research alert! Can Large Language Models (#LLMs) help improve NLP model predictions? A recent study explores using LLMs for data augmentation to reduce high confidence misclassifications. Check out the results here: http://arxiv.org/abs/2403.17860v2 #AI #NLP #TechResearch \ud83e\udd16\ud83d\udcca", "source_id": "c77f91fcb01ebd1aee5ed26f23e49b52c9d9fdb24317cb6e30c8290c2716d817", "page_number": 1}, "score": null, "embedding": [-0.21187737584114075, -0.144776850938797, 0.02318030782043934, -0.1063542366027832, 0.16847851872444153, 0.08783604949712753, -0.09416191279888153, -0.12090016156435013, 0.23620939254760742, -0.3144957721233368, 0.08201296627521515, -0.07078663259744644, 0.13169200718402863, 0.1960277110338211, 0.1584893763065338, 0.16704308986663818, -0.15740519762039185, 0.20012041926383972, -0.1747296005487442, 0.14268344640731812, 0.28332599997520447, -0.0909186378121376, 0.04555460438132286, -0.07405633479356766, -0.07345584779977798, -0.06807492673397064, -0.19900722801685333, -0.28271007537841797, -0.22304318845272064, -1.4243792295455933, 0.1305612176656723, -0.18254190683364868, 0.42051416635513306, -0.17403599619865417, -0.06704200804233551, -0.030339589342474937, -0.24088190495967865, 0.17687919735908508, -0.07381585240364075, 0.15252038836479187, -0.07996097207069397, -0.06129855290055275, -0.10296111553907394, -0.15178504586219788, 0.2709576487541199, -0.2973276376724243, -0.20988832414150238, -0.15812285244464874, -0.6306280493736267, 0.011844421736896038, -0.24917304515838623, -0.14581464231014252, 0.07274281233549118, 0.3053082525730133, 0.23612554371356964, 0.00031514879083260894, 0.4045030176639557, 0.1955689638853073, 0.06767717003822327, 0.14696885645389557, 0.13193745911121368, 0.3805449604988098, -1.0780584812164307, 0.2147429883480072, -0.07001551985740662, 0.22775086760520935, -0.11516852676868439, -0.3073274791240692, 0.003356871660798788, 0.011441130191087723, -0.15418614447116852, 0.15777409076690674, 0.3562445044517517, 0.2555287480354309, 0.23722761869430542, 0.36356601119041443, 0.14603900909423828, -0.3036997616291046, 0.1950850635766983, 0.08621452748775482, 0.34401535987854004, -0.02130407840013504, -0.11329497396945953, -0.5054540038108826, -0.038526006042957306, -0.09420305490493774, 0.11323251575231552, -0.1032792255282402, 0.0572981983423233, -0.2188667207956314, -0.05270480737090111, -0.022952787578105927, -0.10717914998531342, 0.34496110677719116, 0.08840445429086685, 0.1878531277179718, 0.08921486884355545, 0.19796156883239746, -0.2933236062526703, 0.726256787776947, -0.20031188428401947, 0.043196894228458405, -0.26799139380455017, -0.12360315769910812, 0.2176218032836914, -0.14848031103610992, -0.3150935769081116, -0.21988064050674438, -0.11432711035013199, -0.15134724974632263, 0.02031281404197216, 0.07088794559240341, -0.1553734987974167, -0.011636096984148026, -0.07580693066120148, -0.0841653123497963, 0.6719228029251099, -0.07716140896081924, -0.21578148007392883, 0.13091573119163513, -0.14394043385982513, 0.08007846027612686, -0.004262122325599194, -0.18036775290966034, -0.02081672102212906, -0.1072036549448967, 0.20465639233589172, 0.6119037866592407, 0.053024519234895706, -0.013823749497532845, 0.12015057355165482, -0.13832901418209076, -0.36415883898735046, -0.11552301049232483, 0.22135649621486664, -0.1730324923992157, -0.037171974778175354, -0.030536023899912834, 0.10737390071153641, 0.11964188516139984, -0.09913487732410431, 0.04185626283288002, -0.025674259290099144, 0.02245919778943062, -0.5081172585487366, 0.925348162651062, -0.20284079015254974, 0.05057017505168915, -0.21981023252010345, -0.2558691203594208, -0.1215970441699028, 0.32423093914985657, -0.2763233780860901, -0.1444149911403656, 0.27198678255081177, 0.02579585276544094, 0.1984017938375473, -0.08054925501346588, -0.44736847281455994, -0.25182294845581055, 0.04015631228685379, -0.05609462782740593, -0.24938388168811798, 0.7143455743789673, -0.07448387891054153, -0.1499778777360916, -0.19562023878097534, -0.020999737083911896, 0.39274999499320984, -0.06783446669578552, 0.2975768446922302, -0.03634040802717209, -0.035298559814691544, 0.05417142063379288, -0.3566654622554779, 0.2282739132642746, -0.5995250940322876, -0.19448070228099823, -0.05802714452147484, 0.19836513698101044, -0.026842376217246056, -0.13764485716819763, -0.21666671335697174, 0.09236080944538116, 0.05347706750035286, -0.26509368419647217, -0.0028201478999108076, -0.22163207828998566, 0.29775556921958923, 0.09561651200056076, -0.24258127808570862, 0.2171596735715866, 0.03412620350718498, -0.09654124081134796, -0.17414140701293945, -0.15997901558876038, -0.06536814570426941, -0.005913130473345518, 0.1693975180387497, -0.09061725437641144, -0.09163734316825867, 0.33966702222824097, 0.2794054448604584, -0.11812885850667953, 0.11085357517004013, 0.09007801860570908, -0.051891494542360306, 0.09680327773094177, 0.16770444810390472, 0.4339732825756073, -0.4052797257900238, -0.05067423731088638, 0.3570442497730255, -0.0035742996260523796, -0.00989274587482214, 0.1238502636551857, 0.3244912028312683, 0.35594889521598816, 0.040645208209753036, 0.26648035645484924, 0.05247640237212181, 0.3059855103492737, -0.19545020163059235, -1.207869291305542, -0.15305057168006897, 0.30066660046577454, -0.009440707042813301, 0.26874783635139465, -0.27053120732307434, 0.08391578495502472, 0.008087648078799248, 0.27173808217048645, 0.5558276772499084, 0.09210389107465744, -0.030784593895077705, -0.03378487378358841, 0.2249690294265747, 0.1297358274459839, 0.013203282840549946, -0.12446446716785431, 0.08560933917760849, -0.3177913725376129, 0.38713619112968445, -0.2271975874900818, 0.015711022540926933, 0.05609256401658058, -0.9378836154937744, -0.10444065183401108, -0.05590551719069481, 0.6909571290016174, -0.36877962946891785, 0.17137302458286285, -0.16081400215625763, -0.007771321572363377, 0.26790472865104675, -0.04415342956781387, -0.41224274039268494, 0.6454294323921204, 0.016565581783652306, 0.20038975775241852, -0.05813243240118027, 0.06471564620733261, -0.11078440397977829, -0.21080979704856873, 0.008327520452439785, 0.16697971522808075, -0.7546325922012329, -0.25387340784072876, 0.05554937943816185, -0.13235269486904144, -0.06423930078744888, -0.41052863001823425, 0.22854602336883545, 0.2077798694372177, 0.11376574635505676, 0.3104441165924072, -0.04038134962320328, -0.14119401574134827, 0.07565145194530487, -0.7082957625389099, 0.1578107476234436, -0.15533509850502014, 0.044834721833467484, 0.387326180934906, -0.13542751967906952, 0.04799919202923775, -0.23801934719085693, 0.34348830580711365, -0.31827446818351746, -0.055126119405031204, -0.03177319094538689, 0.2792668044567108, 0.0034211655147373676, -0.19095337390899658, 0.8353658318519592, 0.04404481500387192, 0.06863164156675339, 0.1751292496919632, 0.16087546944618225, -0.3104541301727295, -0.36477401852607727, -0.3042603135108948, 0.08043579757213593, 0.584552526473999, 0.23783722519874573, 0.35756269097328186, 0.04420514032244682, 0.28618499636650085, -0.02501070126891136, 0.47074365615844727, 0.03402162715792656, 0.031692326068878174, 0.18675576150417328, 0.02902974747121334, 0.01590788923203945, -0.2993856370449066, 0.0054594241082668304, 0.0865020602941513, 0.09123663604259491, -1.1194052696228027, -0.19072610139846802, -0.16591638326644897, 0.3546641767024994, 0.03792308643460274, -0.0987798348069191, 0.21231591701507568, -0.021063653752207756, 0.1611359715461731, 0.19442275166511536, -0.2751191258430481, 0.08208410441875458, 0.17468366026878357, 0.06320665031671524, 0.24652791023254395, -0.13451749086380005, 0.23420806229114532, -0.08830586820840836, 0.15416398644447327, -0.33981677889823914, 0.18325620889663696, 0.1231195330619812, 1.0888649225234985, 0.015018091537058353, -0.0027966538909822702, 0.3302270770072937, -0.11179791390895844, -0.07430104166269302, 0.014780785888433456, -0.24154993891716003, -0.17405319213867188, 0.21952563524246216, 0.6936688423156738, -0.06864948570728302, -0.04189971834421158, 0.6111531853675842, -0.27697136998176575, -0.13598038256168365, 0.17086286842823029, 0.0554693266749382, 0.324670672416687, 0.02227381058037281, 0.26057592034339905, -0.16602519154548645, 0.5693451166152954, -0.12065854668617249, 0.06256729364395142, -0.35365399718284607, 0.06750036776065826, 0.13972437381744385, -0.31208449602127075, 0.12949174642562866, -0.08631333708763123, -0.01840822771191597, 0.05651261284947395, 0.08395704627037048, 0.08561667054891586, -0.4159099757671356, -0.057430144399404526, -0.2760006785392761, 0.0031131806317716837, -0.22067326307296753, -0.058455146849155426, 0.057387810200452805, -0.43528279662132263], "sparse_embedding": null}, {"id": "7ab0aedbde44a5f71fa90cf8e8f657700caf859cdbe2ede7b3c5c931bc3c49d9", "content": "The rapid development of Large Language Models (LLMs) has led to great\nstrides in model capabilities like long-context understanding and reasoning.\nHowever, as LLMs are able to process longer contexts, it becomes more\nchallenging to evaluate whether they have acquired certain capabilities, since\nthe length of text (e.g., 200K tokens) they can process far exceeds what humans\ncan reliably assess in a reasonable duration. In this paper, we propose using\ncomplex synthetic tasks as a proxy evaluation method, and present S3Eval, a\nSynthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The\nsynthetic nature of S3Eval provides users full control over the dataset,\nallowing them to systematically probe LLM capabilities by scaling text length\nand varying task difficulty across diverse scenarios. The strong correlation\nbetween S3Eval and real-world benchmarks demonstrates the soundness of using\nS3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite\nlong-context data generation method. We have generated a comprehensive dataset\ncalled S3Eval-Standard, and experimental results have shown that it poses\nsignificant challenges for all existing LLMs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.15147v2", "title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models", "content": "http://arxiv.org/pdf/2310.15147v2", "datetime": "2024-04-06 15:20:18", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting developments in the field of Large Language Models (LLMs)! \ud83e\udde0\ud83d\udca1\n\nAs LLMs advance in their capabilities for long-context understanding and reasoning, evaluating their performance accurately becomes increasingly challenging due to their ability to process text far beyond human assessment limits. \n\nIn a recent paper, researchers propose a groundbreaking solution - S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs. By leveraging complex synthetic tasks, S3Eval allows for the systematic probing of LLM capabilities by adjusting text length and task difficulty. The correlation between S3Eval and real-world benchmarks showcases its reliability for evaluating LLMs.\n\nCurious to learn more? Dive into the details here: [Read the full paper](http://arxiv.org/abs/2310.15147v2) \ud83d\udcda #AI #NLP #LLMs #TechInnovation #ResearchPublication", "x": "\ud83d\ude80 Exciting advancements in Large Language Models (LLMs)! A new evaluation method, S3Eval, offers a synthetic, scalable, systematic approach to assessing LLM capabilities. Learn more about this innovative evaluation suite and its impact on LLM development at: http://arxiv.org/abs/2310.15147v2 #AI #NLP #LLMs #TechInnovation \ud83e\udd16\ud83d\udcda", "source_id": "06f91adc4b43775d4b3ab1b5bea155e0280ea3b243e4a2964efc946c53248086", "page_number": 1}, "score": null, "embedding": [-0.35848429799079895, 0.0015975757269188762, 0.10478750616312027, -0.04485936462879181, 0.03236818686127663, -0.04530768096446991, -0.36183643341064453, 0.02879466861486435, 0.18743020296096802, -0.3061543405056, -0.00938084814697504, -0.041489582508802414, 0.19368398189544678, 0.03167246654629707, 0.27485358715057373, 0.19041377305984497, -0.17045974731445312, 0.12452284246683121, -0.33135512471199036, -0.08223668485879898, 0.37406525015830994, -0.3209313452243805, 0.008382098749279976, 0.0352679006755352, 0.2671791613101959, 0.13905414938926697, -0.08844809234142303, -0.14477965235710144, -0.26733896136283875, -1.558762550354004, 0.23951765894889832, -0.14924439787864685, 0.3658835291862488, -0.03743365406990051, -0.1499825268983841, 0.12701919674873352, -0.2836548984050751, 0.15643103420734406, 0.04554998502135277, 0.20786502957344055, 0.09319771826267242, 0.3803851902484894, -0.08678082376718521, -0.10990755259990692, -0.29475969076156616, -0.4050459563732147, -0.3071819245815277, 0.1453532874584198, -0.5592555999755859, -0.0339779257774353, -0.19142179191112518, -0.2950328290462494, 0.2275005280971527, 0.16734729707241058, 0.014172518625855446, 0.1267213523387909, 0.15832951664924622, 0.15919561684131622, 0.1348644495010376, -0.1425934135913849, 0.28758156299591064, 0.28799834847450256, -1.014596939086914, 0.2760965824127197, -0.006536900531500578, 0.2432277798652649, 0.07114250212907791, 0.009081157855689526, 0.2705484926700592, 0.12523795664310455, -0.2046426385641098, 0.06773227453231812, 0.36536914110183716, 0.46742182970046997, 0.37637007236480713, 0.28620782494544983, 0.14732956886291504, -0.0958731397986412, 0.19212573766708374, 0.06041484326124191, 0.30982035398483276, -0.22156114876270294, -0.06994183361530304, -0.1404319852590561, -0.2884107828140259, -0.06747601181268692, -0.28887057304382324, 0.0029375539161264896, 0.04246702417731285, -0.02517695166170597, -0.10103089362382889, -0.23508398234844208, -0.13545939326286316, 0.2760877311229706, -0.2546563446521759, -0.01776576228439808, 0.024495406076312065, 0.0813588947057724, -0.2407192885875702, 0.6029932498931885, -0.09987611323595047, 0.21509747207164764, -0.07395809888839722, -0.14799749851226807, 0.34901049733161926, 0.015023541636765003, -0.19878095388412476, -0.1624128818511963, -0.10655558109283447, -0.11246078461408615, -0.0002865247952286154, 0.05580634996294975, -0.23010651767253876, -0.043920837342739105, 0.05029435083270073, -0.0424649678170681, 0.5266819596290588, -0.18885129690170288, -0.09248797595500946, -0.09128803014755249, -0.3690141439437866, 0.43355700373649597, 0.009309403598308563, -0.2469925582408905, 0.1655445545911789, -0.19444972276687622, 0.2932610511779785, 0.40570268034935, 0.13912366330623627, -0.0844905748963356, 0.15913920104503632, 0.06686001271009445, -0.4477471709251404, -0.138241708278656, -0.061343900859355927, -0.28314465284347534, -0.1618902087211609, -0.0983930453658104, 0.04405909776687622, 0.10711152106523514, -0.03409166261553764, 0.14152835309505463, 0.3366810083389282, -0.11621317267417908, -0.33420130610466003, 0.7228467464447021, 0.10956450551748276, 0.13364605605602264, -0.378818541765213, -0.16564056277275085, 0.20226337015628815, 0.23282606899738312, 0.06858892738819122, -0.3032062351703644, 0.2753695249557495, 0.13642780482769012, 0.17485882341861725, 0.18263626098632812, -0.46176788210868835, -0.051488351076841354, -0.020929502323269844, 0.25366970896720886, -0.1463349312543869, 0.7454001307487488, -0.1089019924402237, -0.4548723101615906, -0.06362710148096085, -0.013454850763082504, 0.08831803500652313, -0.0012239809148013592, 0.11935895681381226, 0.04527544602751732, -0.15761306881904602, 0.055649686604738235, -0.09183737635612488, 0.19080807268619537, -0.6268976926803589, -0.09857518970966339, 0.10528022050857544, 0.16188734769821167, -0.0063082510605454445, -0.20455855131149292, -0.09060220420360565, 0.2780018746852875, -0.057027511298656464, -0.26887640357017517, 0.02715299278497696, -0.12916573882102966, 0.0702575221657753, 0.07513020932674408, -0.12862548232078552, 0.38217779994010925, -0.017893170937895775, 0.010457301512360573, -0.01927649788558483, -0.3323343098163605, 0.11169102787971497, 0.04066379368305206, 0.1316136121749878, -0.1365717649459839, 0.24171008169651031, 0.20375174283981323, 0.0022997877094894648, -0.21001090109348297, 0.2476046085357666, -0.09222674369812012, -0.09866239875555038, 0.17675365507602692, 0.3513653576374054, 0.31428059935569763, -0.4720105528831482, -0.03685186803340912, 0.2938949465751648, -0.044259510934352875, -0.11742214858531952, -0.00656198849901557, 0.15421336889266968, 0.025236835703253746, -0.2426232397556305, 0.30958855152130127, 0.0587269589304924, 0.09140915423631668, -0.18000087141990662, -1.224050521850586, 0.0007699818233959377, 0.2621045708656311, -0.0013086776016280055, 0.22985722124576569, -0.28706490993499756, -0.06621386855840683, -0.08507630974054337, 0.22324460744857788, 0.03467332571744919, 0.2962604761123657, 0.08908036351203918, -0.26463988423347473, 0.030241573229432106, 0.10373026132583618, -0.0662844181060791, -0.32555273175239563, 0.04269104823470116, -0.3792259097099304, 0.2765428423881531, -0.2718571722507477, 0.13738900423049927, 0.06944972276687622, -0.5222581624984741, -0.10317353904247284, -0.3599128723144531, 0.9009602069854736, -0.32030075788497925, -0.09779156744480133, -0.26172780990600586, 0.11061755567789078, 0.09862162172794342, -0.016203241422772408, -0.4937795400619507, 0.4063761830329895, 0.12181771546602249, 0.19868694245815277, 0.1280672401189804, 0.11286643892526627, 0.06632617115974426, -0.23247578740119934, -0.10426025837659836, 0.1545167863368988, -0.6912596225738525, -0.2467132955789566, 0.12530116736888885, 0.2039048969745636, -0.14665354788303375, -0.15006765723228455, 0.059599436819553375, 0.22873151302337646, 0.028801845386624336, 0.1750662624835968, -0.09971769154071808, -0.13058269023895264, 0.020104965195059776, -0.6513299345970154, 0.21007002890110016, -0.14688940346240997, -0.1497368961572647, 0.1803847998380661, 0.0824158787727356, -0.1672765612602234, 0.025355743244290352, 0.252594918012619, -0.08089540153741837, -0.06233997642993927, -0.018530765548348427, -0.17110440135002136, -0.23285235464572906, -0.16725026071071625, 0.5003700852394104, -0.2569662928581238, 0.1675364077091217, 0.1208374947309494, 0.075835682451725, -0.0687863677740097, -0.23982562124729156, -0.14425255358219147, -0.11100603640079498, 0.5790972113609314, 0.11290065944194794, 0.02984621189534664, 0.1751919537782669, 0.08007261157035828, 0.04268860071897507, 0.25864213705062866, 0.21669240295886993, 0.06624661386013031, -0.047141700983047485, -0.0993492603302002, 0.04804876074194908, -0.25278884172439575, -0.15802861750125885, 0.18130339682102203, -0.10488761216402054, -1.0592572689056396, 0.031171267852187157, -0.08782222121953964, 0.22000738978385925, -0.07754570990800858, -0.07963177561759949, 0.21247051656246185, -0.13714154064655304, -0.061533503234386444, 0.04851701110601425, -0.061563823372125626, 0.2639119029045105, 0.1225975751876831, 0.12960949540138245, 0.23012375831604004, -0.12377127259969711, 0.42167457938194275, -0.14319711923599243, 0.30985888838768005, -0.5097527503967285, 0.10569097101688385, 0.2143355756998062, 1.0496634244918823, -0.10421421378850937, -0.0009752041078172624, 0.4330388903617859, 0.029114242643117905, -0.1154932752251625, -0.04759583994746208, -0.1742101013660431, -0.005389671307057142, -0.010057150386273861, 0.6417298913002014, -0.02090759016573429, 0.12018411606550217, 0.2704838216304779, -0.19913803040981293, 0.06805586814880371, 0.2656058371067047, 0.23985689878463745, 0.6060487031936646, 0.027039259672164917, 0.12355917692184448, -0.15121880173683167, 0.6927741765975952, -0.012215000577270985, -0.01072030607610941, -0.3562093675136566, -0.10581261664628983, 0.003069165861234069, -0.3170423209667206, 0.06651562452316284, -0.19205491244792938, 0.17192643880844116, 0.3926302194595337, 0.18584807217121124, 0.13374418020248413, -0.03559572249650955, -0.134162038564682, -0.17669358849525452, -0.09032878279685974, -0.39979878067970276, 0.08671089261770248, 0.2629711627960205, -0.07005271315574646], "sparse_embedding": null}, {"id": "f3739ef851e87f09a5b90a88e25b6c39e4f68c2f1ee4a9c558d43bedcdc68b7f", "content": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of \"fact-checking\" are\nbased on verifying each piece of a model generation against potential evidence\nusing an LLM. However, this process can be very computationally expensive,\nrequiring many calls to LLMs to check a single response. In this work, we show\nhow to build small models that have GPT-4-level performance but for 400x lower\ncost. We do this by constructing synthetic training data with GPT-4, which\ninvolves creating realistic yet challenging instances of factual errors via a\nstructured generation procedure. Training on this data teaches models to check\neach fact in the claim and recognize synthesis of information across sentences.\nFor evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact,\ncollected from recent work on fact-checking and grounding LLM generations. Our\nbest system MiniCheck-FT5 (770M parameters) outperforms all systems of\ncomparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for\ndata synthesis, and models.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2404.10774v1", "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "content": "http://arxiv.org/pdf/2404.10774v1", "datetime": "2024-04-16 17:59:10", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting breakthrough in NLP! Researchers have developed a cost-effective method to enhance fact-checking capabilities of language models like GPT-4. By training small models with synthetic data generated from GPT-4, they achieved GPT-4-level performance at 400x lower cost. The new benchmark LLM-AggreFact outperforms other systems in fact-checking and information synthesis. Learn more about this innovative approach at: http://arxiv.org/abs/2404.10774v1 #NLP #AI #LLM #FactChecking #Innovation \ud83d\udd0d\ud83d\udcca", "x": "\ud83d\ude80 Exciting breakthrough in NLP research! Learn how MiniCheck-FT5, a small model with GPT-4-level performance at 400x lower cost, is revolutionizing fact-checking for LLMs. Check out the paper here: http://arxiv.org/abs/2404.10774v1 #AI #NLP #LLMs #TechInnovation", "source_id": "008b82ebe6ca815c4fda03862c38c60a3321e6d30e3f9ab5a3b5bdf5634dfb80", "page_number": 1}, "score": null, "embedding": [-0.24489767849445343, 0.024771815165877342, 0.11332601308822632, -0.08777137845754623, 0.299771785736084, 0.2517421841621399, -0.24166202545166016, -0.07232672721147537, 0.15615323185920715, -0.3335495591163635, 0.18184220790863037, -0.1765809804201126, 0.15131573379039764, 0.3290174603462219, -0.05932466685771942, 0.08044549822807312, -0.15943312644958496, 0.2986326813697815, -0.007857013493776321, 0.04201221466064453, 0.3500896394252777, -0.2946479320526123, -0.04459037631750107, -0.08301432430744171, 0.10804054141044617, 0.17545993626117706, -0.04238557815551758, -0.07000923901796341, -0.24876338243484497, -1.4215278625488281, -0.05915524438023567, -0.0507311150431633, 0.3764212727546692, 0.021718956530094147, 0.05341435223817825, 0.04948800429701805, -0.2780010998249054, 0.1587284803390503, -0.1735297292470932, 0.14896520972251892, 0.04588188976049423, 0.09605715423822403, 0.012238327413797379, -0.18247655034065247, -0.11035076528787613, -0.30142098665237427, -0.16304460167884827, 0.10826417803764343, -0.4953683614730835, -0.3309241533279419, -0.06409389525651932, -0.06090308353304863, -0.024076033383607864, 0.043340351432561874, -0.0239630825817585, 0.0004878451582044363, 0.27596110105514526, 0.12598669528961182, 0.2876015305519104, 0.017919354140758514, 0.27214646339416504, 0.5538138151168823, -1.1221377849578857, 0.08997482061386108, 0.031624890863895416, 0.1730901300907135, -0.04877348244190216, 0.02171856164932251, 0.16131755709648132, 0.15810653567314148, -0.17462432384490967, -0.06692488491535187, 0.12279929220676422, 0.2993437945842743, 0.1364125907421112, 0.24711859226226807, 0.12297190725803375, -0.18626195192337036, 0.023338397964835167, -0.06619830429553986, -0.10881505906581879, 0.08515167236328125, -0.13044750690460205, -0.009846288710832596, -0.2330990433692932, -0.1648934781551361, 0.017297882586717606, 0.020565830171108246, 0.24672353267669678, -0.04051751643419266, 0.017295056954026222, -0.03560780733823776, -0.14379297196865082, 0.26959484815597534, -0.11023427546024323, 0.16836407780647278, 0.18752743303775787, 0.06595935672521591, -0.043660953640937805, 0.5830998420715332, -0.0893031433224678, 0.12416823208332062, -0.22029384970664978, 0.12973982095718384, 0.08140739798545837, -0.032685261219739914, -0.20986363291740417, 0.06143441051244736, -0.14391300082206726, -0.1757129579782486, -0.0680936649441719, 0.2546316385269165, 0.10128585249185562, -0.1706414669752121, 0.04050520434975624, -0.12063415348529816, 0.3678762912750244, -0.08710464090108871, -0.20603802800178528, 0.025794576853513718, -0.24110108613967896, 0.12865565717220306, -0.2546003460884094, -0.12756046652793884, 0.2173849642276764, -0.22223961353302002, 0.13691702485084534, 0.39779287576675415, 0.12627151608467102, 0.17904803156852722, 0.12289884686470032, -0.11679986119270325, -0.4250882863998413, -0.2534197270870209, 0.23370927572250366, -0.16259920597076416, 0.39305999875068665, 0.05236067995429039, -0.003010575659573078, 0.4142020344734192, -0.08390432596206665, 0.08945050835609436, 0.04418230056762695, 0.03836431726813316, -0.5370074510574341, 0.7218722105026245, -0.2342934012413025, 0.03777607902884483, -0.16034385561943054, -0.4251599907875061, 0.1379646360874176, 0.33037203550338745, -0.21479882299900055, -0.23326388001441956, 0.3431151509284973, 0.014459947124123573, 0.13701659440994263, 0.016161467880010605, -0.47479069232940674, 0.04240143299102783, -0.07071799784898758, 0.014700457453727722, -0.2020588517189026, 0.6643903255462646, -0.08950944244861603, -0.3327251672744751, -0.3037571907043457, 0.11591409146785736, 0.3602806329727173, -0.40405961871147156, 0.2079903781414032, 0.20333224534988403, 0.015363877639174461, -0.32832589745521545, -0.22167229652404785, -0.28133270144462585, -0.5339145660400391, 0.09048773348331451, -0.11612208187580109, 0.27428746223449707, 0.15429341793060303, -0.29755669832229614, -0.17244884371757507, 0.2448420226573944, 0.014714025892317295, -0.08835753798484802, 0.02226327359676361, -0.32014715671539307, 0.2009958177804947, -0.03185264766216278, -0.3070489764213562, 0.0899343267083168, -0.03731667995452881, 0.17445632815361023, -0.17367838323116302, -0.2165086269378662, -0.12126510590314865, 0.12933677434921265, 0.18161866068840027, -0.16873714327812195, -0.012683156877756119, -0.15029706060886383, -0.03378137946128845, 0.026060916483402252, 0.0255748238414526, 0.01946772076189518, -0.1319957673549652, 0.0012222614604979753, 0.3552854061126709, 0.33242297172546387, -0.18218031525611877, -0.10219789296388626, 0.10524836927652359, -0.227272629737854, -0.19501036405563354, 0.03777482360601425, 0.21338653564453125, 0.38220471143722534, -0.37903764843940735, 0.1987294852733612, -0.013637064956128597, 0.06798097491264343, -0.11121775209903717, -1.0931999683380127, -0.29621756076812744, 0.2416556030511856, 0.31571054458618164, 0.4429347515106201, -0.3547252416610718, 0.1457219123840332, 0.1572628915309906, 0.009648572653532028, 0.2871696352958679, 0.06472210586071014, 0.20443099737167358, -0.16301411390304565, 0.16948699951171875, -0.13777372241020203, -0.042458295822143555, -0.15421634912490845, 0.03637031465768814, -0.19491547346115112, 0.17703351378440857, -0.26977595686912537, 0.04713036119937897, -0.07160937041044235, -0.6790460348129272, 0.05070190131664276, -0.228683203458786, 0.7717533111572266, -0.14339196681976318, 0.13298581540584564, -0.00216200714930892, 0.028044937178492546, 0.1332312673330307, -0.02834535948932171, -0.3456297218799591, 0.46392494440078735, 0.12698590755462646, -0.031646158546209335, 0.06882455199956894, 0.12078545987606049, -0.1517791450023651, -0.26919466257095337, -0.002273108810186386, 0.12513086199760437, -0.6115936040878296, -0.34110167622566223, 0.011349090375006199, -0.13350418210029602, 0.054359689354896545, -0.20811180770397186, 0.22758954763412476, 0.19610600173473358, -0.07233989238739014, 0.43616706132888794, -0.033773377537727356, -0.2001662254333496, -0.15650933980941772, -0.5496731996536255, 0.0358540304005146, -0.2616675794124603, -0.05091353505849838, 0.21378283202648163, -0.07442694902420044, -0.06679287552833557, -0.13865940272808075, 0.4080488979816437, -0.14169704914093018, -0.1427837461233139, -0.055166929960250854, 0.3003440499305725, -0.20694716274738312, 0.2816014289855957, 0.5293822288513184, 0.07673567533493042, 0.2671310007572174, 0.13177059590816498, 0.15076719224452972, -0.13289514183998108, -0.36360904574394226, -0.32486164569854736, -0.21365748345851898, 0.38791465759277344, 0.21772171556949615, 0.1332206279039383, 0.07762634754180908, -0.05076819285750389, 0.2716550827026367, 0.4362708330154419, 0.14381133019924164, 0.2032375931739807, -0.05095309764146805, 0.048725225031375885, 0.16479922831058502, -0.19194296002388, -0.08129335939884186, 0.04370344802737236, 0.08618157356977463, -1.101419448852539, -0.04725700616836548, -0.16149529814720154, 0.4304172396659851, -0.30680814385414124, -0.11177344620227814, 0.11419408023357391, -0.051363732665777206, 0.03146618604660034, 0.178850919008255, -0.05726664513349533, 0.06810370832681656, 0.30377626419067383, -0.2853488326072693, 0.1249655932188034, 0.18471422791481018, 0.24745529890060425, -0.3797200322151184, -0.15747228264808655, -0.3124513626098633, 0.16753464937210083, 0.3032509386539459, 0.900011420249939, 0.0019178339280188084, 0.057243216782808304, 0.2980411648750305, -0.06485901772975922, 0.12073211371898651, 0.16721299290657043, -0.06735079735517502, -0.15898939967155457, -0.06173669546842575, 0.8125963807106018, 0.08518106490373611, 0.13361400365829468, 0.5482099056243896, -0.21665409207344055, 0.06824369728565216, 0.1817592978477478, -0.018296638503670692, 0.3229610025882721, -0.15540948510169983, 0.09110085666179657, -0.02094460465013981, 0.4381689727306366, -0.1592206358909607, 0.26027947664260864, -0.30951517820358276, -0.16486817598342896, 0.0057158088311553, -0.04218393564224243, 0.035610973834991455, 0.06748049706220627, -0.16500402987003326, 0.3984247148036957, 0.3229164779186249, 0.039306432008743286, -0.05396384745836258, 0.10627150535583496, -0.27749499678611755, -0.05454543977975845, -0.041989073157310486, 0.23855853080749512, 0.11828915774822235, -0.2584190368652344], "sparse_embedding": null}, {"id": "76fdc885df31c6caa21f82525c3b3d2210531d7bd77feef288288e7295d234ee", "content": "Large language models (LLMs) are remarkable data annotators. They can be used\nto generate high-fidelity supervised training data, as well as survey and\nexperimental data. With the widespread adoption of LLMs, human gold--standard\nannotations are key to understanding the capabilities of LLMs and the validity\nof their results. However, crowdsourcing, an important, inexpensive way to\nobtain human annotations, may itself be impacted by LLMs, as crowd workers have\nfinancial incentives to use LLMs to increase their productivity and income. To\ninvestigate this concern, we conducted a case study on the prevalence of LLM\nusage by crowd workers. We reran an abstract summarization task from the\nliterature on Amazon Mechanical Turk and, through a combination of keystroke\ndetection and synthetic text classification, estimate that 33-46% of crowd\nworkers used LLMs when completing the task. Although generalization to other,\nless LLM-friendly tasks is unclear, our results call for platforms,\nresearchers, and crowd workers to find new ways to ensure that human data\nremain human, perhaps using the methodology proposed here as a stepping stone.\nCode/data: https://github.com/epfl-dlab/GPTurk", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2306.07899v1", "title": "Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks", "content": "http://arxiv.org/pdf/2306.07899v1", "datetime": "2023-06-13 16:46:24", "query": "synthetic data generation llms", "linkedin": "\ud83d\ude80 Exciting findings in the world of AI and crowdsourcing! A recent study delves into the impact of Large Language Models (LLMs) on crowd workers. Research suggests that 33-46% of crowd workers used LLMs to complete tasks, raising important questions about the integrity of human-generated data. Check out the full study and methodology here: http://arxiv.org/abs/2306.07899v1\n\n#AI #NLP #LLMs #Crowdsourcing #Research #Tech #Innovation", "x": "\ud83d\ude80 Investigating the impact of Large Language Models (LLMs) on crowd workers! A recent study found that 33-46% of crowd workers used LLMs for an abstract summarization task. How can we ensure human data remain human in the age of AI? Find out more: http://arxiv.org/abs/2306.07899v1 #AI #LLMs #Research #TechEthics", "source_id": "9f87efd423a6effd721e76110c43e78b5bd5bd13ca3a4f7d6ee53572feb34773", "page_number": 1}, "score": null, "embedding": [-0.3446113169193268, -0.23002922534942627, 0.012291175313293934, -0.01784185878932476, 0.061887066811323166, -0.14213670790195465, -0.03900010883808136, -0.09666896611452103, 0.204078808426857, -0.35064199566841125, 0.11318489909172058, -0.02541397325694561, 0.0545683316886425, 0.31988197565078735, 0.16668209433555603, 0.15436159074306488, -0.11548511683940887, -0.027915509417653084, -0.37181356549263, -0.146237313747406, 0.27379998564720154, -0.1946173757314682, 0.0374549962580204, -0.04342114180326462, -0.16745761036872864, -0.06730791181325912, -0.3916100561618805, -0.15132220089435577, -0.3853778839111328, -1.3043313026428223, 0.27065369486808777, -0.12418132275342941, 0.5888561010360718, 0.11538673937320709, -0.13940002024173737, -0.013710364699363708, 0.045651379972696304, 0.2279273420572281, -0.018917083740234375, 0.3402582108974457, -0.1864681839942932, -0.017929010093212128, 0.11295180767774582, -0.0599459633231163, 0.00028282098355703056, -0.4557562470436096, -0.22024847567081451, -0.13784955441951752, -0.6061627864837646, 0.20582087337970734, -0.05660214275121689, -0.34433886408805847, -0.01999448984861374, 0.3556288182735443, 0.21798790991306305, -0.13541825115680695, 0.20605537295341492, 0.12039349228143692, 0.123565174639225, 0.07941567152738571, 0.315268874168396, 0.2741556763648987, -0.9707534313201904, 0.23759199678897858, -0.2039649337530136, 0.31148678064346313, -0.013288678601384163, 0.05708992853760719, 0.08236779272556305, -0.08451370894908905, 0.003992629237473011, 0.08718644082546234, 0.16205599904060364, 0.21125516295433044, 0.3102879524230957, 0.19895881414413452, 0.08869162201881409, -0.13901132345199585, 0.24206268787384033, -0.12711074948310852, -0.11480497568845749, 0.11646515130996704, 0.05158771947026253, -0.20510786771774292, -0.12209118157625198, 0.044071611016988754, -0.01291557028889656, -0.1612834632396698, 0.22012588381767273, -0.28385576605796814, -0.03398514539003372, -0.0861731767654419, -0.16862259805202484, 0.3380797207355499, -0.037908509373664856, -0.08080446720123291, 0.18850991129875183, 0.1919415444135666, -0.22425305843353271, 0.7163200378417969, -0.1149432510137558, 0.10783172398805618, -0.17472682893276215, -0.1329052597284317, 0.308031290769577, -0.0915079191327095, -0.21907028555870056, -0.17933039367198944, -0.09606579691171646, 0.036141157150268555, 0.03383379429578781, 0.30642542243003845, -0.13253401219844818, -0.06104642525315285, 0.08475696295499802, -0.04546848312020302, 0.4191812574863434, 0.11115454882383347, 0.026502294465899467, 0.2688315808773041, -0.3014450967311859, 0.013860564678907394, 0.020568417385220528, -0.05433191731572151, 0.07844673097133636, 0.02098606340587139, 0.13993684947490692, 0.5672350525856018, 0.4722268879413605, 0.08146361261606216, 0.1919202357530594, 0.03643380478024483, -0.7635665535926819, -0.1835721731185913, 0.0977984219789505, -0.03435052931308746, 0.10357706993818283, -0.15332208573818207, 0.11179787665605545, 0.2688734829425812, 0.05400729924440384, 0.07168533653020859, 0.024660231545567513, -0.0865032896399498, -0.2551039159297943, 0.8387208580970764, 0.04140797257423401, 0.020255066454410553, -0.17023198306560516, -0.18892879784107208, 0.17772434651851654, 0.24832379817962646, -0.08806158602237701, -0.17273738980293274, 0.2926671802997589, 0.1819872111082077, 0.5225852727890015, 0.3579050302505493, -0.44086697697639465, -0.10605181753635406, 0.030162660405039787, -0.053751736879348755, -0.07230647653341293, 0.6261501908302307, -0.13790802657604218, -0.5416116118431091, -0.15161949396133423, -0.024927765130996704, 0.21627604961395264, -0.09526491910219193, 0.1964091807603836, 0.00013336833217181265, -0.2156393975019455, 0.21392865478992462, -0.08830055594444275, 0.15449506044387817, -0.6373661160469055, 0.031450651586055756, -0.09324116259813309, 0.3534698188304901, 0.2086373269557953, -0.20442792773246765, -0.32716450095176697, 0.2307945340871811, -0.18746072053909302, -0.33870649337768555, -0.061574261635541916, -0.18094831705093384, 0.1485944539308548, 0.23110593855381012, 0.11420425772666931, 0.37108272314071655, -0.1445281058549881, -0.15269583463668823, -0.008897148072719574, -0.2471712976694107, -0.2421712726354599, -0.036871060729026794, 0.12302844971418381, -0.2222633957862854, -0.21543444693088531, 0.37343740463256836, 0.09557485580444336, -0.057721517980098724, 0.20669040083885193, 0.05785973370075226, -0.16121800243854523, -0.12434031814336777, 0.40018540620803833, 0.21995654702186584, -0.22510984539985657, -0.04440970718860626, 0.16626648604869843, 0.08152946829795837, -0.22386084496974945, 0.10832910239696503, 0.3946278989315033, 0.29169556498527527, -0.04633978754281998, 0.06783760339021683, 0.1826183795928955, 0.3645966947078705, -0.3138371706008911, -1.2466096878051758, -0.26902568340301514, 0.2769414782524109, 0.06367775797843933, 0.09581676870584488, -0.1892765909433365, 0.03386731445789337, 0.10688621550798416, 0.11048483103513718, 0.2757313549518585, 0.22650489211082458, -0.11429097503423691, -0.27830180525779724, 0.17769014835357666, 0.11043863743543625, 0.18866638839244843, -0.47292187809944153, 0.14301909506320953, -0.13999922573566437, 0.28443530201911926, 0.004606835078448057, -0.08025228977203369, -0.060262635350227356, -0.7217048406600952, 0.17185458540916443, -0.10595270246267319, 0.824509859085083, -0.03957429900765419, -0.26710689067840576, -0.24749481678009033, -0.14620287716388702, 0.38115906715393066, -0.21698446571826935, -0.5825871825218201, 0.4817289710044861, -0.05110285431146622, 0.06775125861167908, -0.20331363379955292, -0.015293385833501816, 0.024064958095550537, -0.32959845662117004, -0.039835549890995026, 0.03348394110798836, -0.5377172231674194, -0.29482850432395935, 0.12174350023269653, -0.06783381849527359, -0.2106010615825653, -0.21927011013031006, 0.11607799679040909, 0.11197817325592041, 0.13411954045295715, 0.4458104968070984, 0.03581525757908821, -0.2481595128774643, -0.1820584088563919, -0.43875783681869507, 0.224492609500885, -0.3073238134384155, 0.11632010340690613, 0.25180986523628235, -0.17363770306110382, -0.08099134266376495, -0.14047937095165253, 0.2826072871685028, -0.337566077709198, -0.26975899934768677, 0.15374855697155, 0.12164565920829773, -0.2099367380142212, -0.20941589772701263, 0.7656469345092773, 0.1086663082242012, 0.17246019840240479, 0.20673975348472595, 0.1393754929304123, -0.037760864943265915, -0.3548777103424072, -0.3242456018924713, 0.005870177410542965, 0.43839791417121887, 0.23729443550109863, -0.06799564510583878, 0.15405678749084473, 0.07291820645332336, 0.2339274287223816, 0.22316308319568634, 0.016170352697372437, 0.31690514087677, -0.06498516350984573, -0.05473890155553818, -0.0306851789355278, -0.4871050715446472, -0.013238859362900257, 0.009911393746733665, 0.04785574600100517, -1.0801700353622437, 0.12449586391448975, -0.10298073291778564, 0.3489472270011902, 0.018372396007180214, 0.04267866536974907, 0.11181516945362091, -0.09761021286249161, 0.1458185613155365, 0.22724418342113495, 0.015566214919090271, 0.1942850798368454, 0.17394177615642548, -0.2557942867279053, 0.1849406510591507, -0.0171201154589653, 0.4604552686214447, -0.04159868136048317, 0.040563032031059265, -0.32808029651641846, -0.07523130625486374, 0.14361122250556946, 1.07389497756958, -0.055578358471393585, -0.093827024102211, 0.26424551010131836, -0.1438254863023758, -0.03456764668226242, 0.33080288767814636, -0.041027627885341644, -0.15815074741840363, -0.09926497936248779, 0.7188711166381836, -0.08635369688272476, -0.09303339570760727, 0.20675408840179443, -0.26541489362716675, -0.19106633961200714, 0.1422611027956009, 0.15751662850379944, 0.24388337135314941, 0.05367119610309601, -0.06600884348154068, -0.30467137694358826, 0.5640103220939636, 0.11038137227296829, 0.12550662457942963, -0.6761223673820496, -0.008328828029334545, 0.06931068748235703, -0.19806279242038727, -0.32931771874427795, -0.23597756028175354, 0.18238139152526855, 0.1516280323266983, 0.18672633171081543, 0.018266892060637474, 0.06081622466444969, -0.09945856779813766, -0.1482730656862259, -0.039544858038425446, -0.1800912767648697, -0.09300395101308823, 0.07241924852132797, -0.13295519351959229], "sparse_embedding": null}, {"id": "70765de51dfb956ec7b0ff4a774142731ca4d9c4136387a3071b15c894d0b777", "content": "Adopting human and large language models (LLM) as judges (\\textit{a.k.a}\nhuman- and LLM-as-a-judge) for evaluating the performance of LLMs has recently\ngained attention. Nonetheless, this approach concurrently introduces potential\nbiases from human and LLM judges, questioning the reliability of the evaluation\nresults. In this paper, we propose a novel framework that is free from\nreferencing groundtruth annotations for investigating Fallacy Oversight Bias,\nAuthority Bias and Beauty Bias on LLM and human judges. We curate a dataset\nreferring to the revised Bloom's Taxonomy and conduct thousands of human and\nLLM evaluations. Results show that human and LLM judges are vulnerable to\nperturbations to various degrees, and that even the cutting-edge judges possess\nconsiderable biases. We further exploit their weakness and conduct attacks on\nLLM judges. We hope that our work can notify the community of the vulnerability\nof human- and LLM-as-a-judge against perturbations, as well as the urgency of\ndeveloping robust evaluation systems.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.10669v3", "title": "Humans or LLMs as the Judge? A Study on Judgement Biases", "content": "http://arxiv.org/pdf/2402.10669v3", "datetime": "2024-04-17 09:56:26", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Exciting new research alert! \ud83d\ude80\n\nAdopting human and large language models (LLM) as judges for evaluating LLM performance has been a hot topic lately. But, what if I told you that this approach might introduce biases that could impact the reliability of evaluation results? \ud83d\ude31\n\nIn a recent paper, a novel framework was proposed to investigate Fallacy Oversight Bias, Authority Bias, and Beauty Bias on LLM and human judges without relying on groundtruth annotations. Thousands of human and LLM evaluations were conducted using a curated dataset based on the revised Bloom's Taxonomy.\n\nThe results were eye-opening! Both human and LLM judges showed vulnerabilities to perturbations, with even cutting-edge judges displaying significant biases. The study went a step further by conducting attacks on LLM judges to exploit these weaknesses.\n\nCheck out the full paper to learn more about the vulnerability of human- and LLM-as-a-judge against perturbations and the importance of developing robust evaluation systems. Knowledge is power! \ud83d\udcaa\n\nRead the full paper here: [Link to the research paper](http://arxiv.org/abs/2402.10669v3)\n\n#AI #NLP #LLM #Research #Tech #EvaluationBias #Innovation #TechNews #ArtificialIntelligence", "x": "\ud83e\udd16\ud83e\udde0 New research alert! Investigating biases in human and large language models (LLM) as judges for evaluating LLM performance. Results show vulnerability to perturbations and considerable biases even in cutting-edge judges. Learn more at: http://arxiv.org/abs/2402.10669v3 #AI #NLP #LLM #Research #TechBias", "source_id": "1a48ed1fa75fea779c4410a2272c1cf9aaac4ef9dea37b80896b2d082ae8472b", "page_number": 1}, "score": null, "embedding": [-0.35722899436950684, -0.04715906083583832, -0.0015070358058437705, -0.02302667312324047, 0.0973634272813797, -0.016528336331248283, 0.07362625747919083, -0.02337620034813881, 0.2546743154525757, 0.009883408434689045, 0.16612401604652405, 0.01543447282165289, 0.21630993485450745, 0.2826668322086334, 0.29237645864486694, 0.3043152093887329, -0.21443095803260803, 0.32313305139541626, -0.019659847021102905, 0.023995865136384964, 0.19164207577705383, -0.08353160321712494, 0.06375265121459961, 0.023457897827029228, -0.1923375129699707, -0.02596828155219555, -0.3307874798774719, -0.32350894808769226, -0.3366301655769348, -1.5331186056137085, 0.21079212427139282, -0.05684288963675499, 0.2569406032562256, -0.08032967150211334, -0.1525835245847702, -0.1194748505949974, -0.2317947894334793, 0.05053060129284859, -0.16518297791481018, 0.25813916325569153, 0.07678841799497604, 0.014050445519387722, 3.550833207555115e-05, -0.18850186467170715, -0.1175556406378746, -0.1855558604001999, -0.1471254974603653, 0.0077674854546785355, -1.0380038022994995, 0.136357843875885, -0.24493949115276337, -0.36951377987861633, 0.07991116493940353, 0.3011728525161743, 0.02911529876291752, 0.003141162684187293, 0.4562253952026367, 0.23248885571956635, 0.08847302198410034, 0.14546586573123932, 0.09237443655729294, 0.29422175884246826, -1.0005722045898438, 0.29342788457870483, 0.044929102063179016, 0.2644735872745514, -0.11614367365837097, -0.1111101433634758, 0.029818924143910408, -0.0559266097843647, 0.09789453446865082, 0.15513600409030914, 0.3840198516845703, 0.3146241307258606, 0.1538894772529602, 0.17628687620162964, 0.17804919183254242, -0.15240521728992462, 0.4090971052646637, -0.0033775039482861757, 0.05235946550965309, 0.12955540418624878, -0.12393629550933838, -0.36128610372543335, 0.033410653471946716, -0.08882438391447067, -0.15969178080558777, -0.07957448065280914, 0.058180369436740875, 0.07552925497293472, 0.19090920686721802, -0.09356053173542023, -0.07311923056840897, 0.2988615930080414, 0.08488709479570389, -0.03665512800216675, 0.16088320314884186, -0.19581183791160583, -0.3244243562221527, 0.5934720635414124, -0.10301414132118225, 0.2124554067850113, -0.1252904087305069, 0.0976756364107132, 0.3841998875141144, -0.053936902433633804, -0.0878041461110115, -0.4990813732147217, -0.011058734729886055, -0.17495465278625488, 0.1680373102426529, 0.0816512480378151, 0.09561865776777267, -0.011448894627392292, 0.19783669710159302, 0.020221883431077003, 0.31108248233795166, 0.0049966610968112946, -0.13025444746017456, -0.2766209840774536, -0.4205220639705658, 0.24857264757156372, 0.0003982718335464597, 0.05681183561682701, 0.161235511302948, -0.09830780327320099, 0.1319456845521927, 0.5532264113426208, 0.26950573921203613, -0.1523887664079666, 0.15309344232082367, -0.23271799087524414, -0.30972549319267273, -0.19339147210121155, 0.15954935550689697, -0.08375181257724762, -0.06944452971220016, 0.025878744199872017, 0.1801930069923401, 0.23048923909664154, 0.03761344775557518, -0.3622205853462219, 0.045495085418224335, -0.04785836488008499, -0.4934995770454407, 0.5894958972930908, -0.0596684031188488, -0.006679181009531021, -0.2756290137767792, 0.007296200376003981, -0.023809734731912613, 0.20849718153476715, -0.16453486680984497, -0.011141420342028141, 0.06876720488071442, 0.22473593056201935, -0.06304840743541718, 0.3947695195674896, -0.3165334463119507, -0.029682762920856476, 0.20001497864723206, 0.13260626792907715, -0.25432348251342773, 0.5011566281318665, -0.2339867502450943, -0.28218916058540344, -0.17221565544605255, -0.08127635717391968, 0.1196366474032402, 0.02962440624833107, 0.3568553924560547, -0.20872235298156738, -0.3171575367450714, 0.19647906720638275, -0.18704387545585632, 0.01604158617556095, -0.37361055612564087, 0.008995975367724895, -0.07921745628118515, 0.08450812846422195, 0.3976195752620697, -0.2505232095718384, -0.21272721886634827, 0.3257838785648346, -0.12654301524162292, -0.23090454936027527, -0.23558653891086578, -0.3650328516960144, 0.12286539375782013, 0.2845958173274994, -0.49218353629112244, 0.156546950340271, -0.15587985515594482, 0.0050031463615596294, -0.1644381880760193, -0.46102750301361084, -0.010144834406673908, -0.18287904560565948, 0.013333713635802269, -0.13246648013591766, -0.05331992357969284, 0.2778083086013794, 0.03465268388390541, -0.15939830243587494, 0.3117881119251251, 0.11994276940822601, -0.040516164153814316, -0.11071984469890594, 0.3685189187526703, 0.24449951946735382, -0.18572445213794708, 0.07660552114248276, -0.04214243218302727, 0.14516681432724, -0.037790365517139435, 0.04523022472858429, 0.49271100759506226, 0.19695469737052917, 0.07726573199033737, 0.13909320533275604, 0.06239065155386925, 0.17694632709026337, -0.14321404695510864, -1.3490593433380127, -0.16125553846359253, -0.017647134140133858, 0.10387703776359558, 0.08388814330101013, -0.25799760222435, 0.040144890546798706, 0.00355443824082613, 0.2284289002418518, 0.46865761280059814, 0.05344298481941223, 0.08083835989236832, -0.16685540974140167, 0.18780475854873657, 0.034260328859090805, 0.048899583518505096, -0.3879269063472748, 0.17052318155765533, -0.3341173231601715, 0.1669982671737671, -0.29756322503089905, 0.09263437986373901, 0.1395827680826187, -0.7589467763900757, 0.20994290709495544, 0.011972547508776188, 0.7424306869506836, -0.06985807418823242, -0.11155104637145996, 0.017232391983270645, 0.11285834014415741, 0.2278907746076584, 0.0390651598572731, -0.522382378578186, 0.5205286741256714, 0.1773221492767334, -0.05616642162203789, -0.08168458938598633, -0.18469108641147614, -0.09383995831012726, -0.2011725902557373, -0.1108982115983963, 0.2331959456205368, -0.6529443860054016, -0.3627781867980957, 0.10068199038505554, 0.034400824457407, 0.017368212342262268, -0.3441942036151886, 0.18827174603939056, 0.04202994704246521, -0.14729733765125275, 0.32685235142707825, 0.041047461330890656, -0.23991934955120087, -0.027028482407331467, -0.5986960530281067, 0.19517189264297485, -0.09116015583276749, 0.1569754034280777, 0.3553675413131714, -0.17073212563991547, -0.28004270792007446, -0.2783791720867157, 0.26625052094459534, -0.19997915625572205, 0.009497695602476597, 0.004181442782282829, 0.24089515209197998, 0.06263820827007294, -0.2677735686302185, 0.6086475253105164, 0.008935971185564995, -0.06339015811681747, 0.12326999753713608, 0.22968925535678864, -0.026989245787262917, -0.3154739439487457, -0.45325714349746704, -0.040174055844545364, 0.5470061898231506, 0.2591656744480133, -0.016290463507175446, 0.15851354598999023, 0.10967659950256348, 0.05547226220369339, 0.1305711567401886, -0.020449770614504814, 0.22785882651805878, 0.1438724547624588, -0.0644632875919342, 0.06033831089735031, -0.4393748939037323, -0.031225409358739853, -0.25229689478874207, 0.22471581399440765, -1.030313491821289, 0.025971099734306335, -0.1789044439792633, 0.3394308090209961, 0.08568913489580154, 0.06088393181562424, 0.3175053596496582, -0.26102501153945923, -0.002562210662290454, -0.11301230639219284, -0.10099723190069199, 0.3059787452220917, 0.10872261226177216, -0.15146808326244354, 0.2783939838409424, -0.2788436710834503, 0.4051767885684967, -0.12983468174934387, 0.2270996868610382, -0.3477984368801117, 0.0963377133011818, 0.34273210167884827, 1.0209665298461914, -0.16802147030830383, -0.113426074385643, 0.267236590385437, 0.1186678409576416, -0.09817884862422943, -0.167613223195076, 0.09022457897663116, -0.15624549984931946, 0.05641533061861992, 0.6909139752388, -0.04453594237565994, 0.005084710195660591, 0.4955230951309204, -0.3144577443599701, -0.12203451246023178, 0.47088077664375305, -0.04945807158946991, 0.33398929238319397, 0.07761598378419876, 0.052413590252399445, -0.06287220120429993, 0.8362300992012024, 0.018921446055173874, 0.15871195495128632, -0.20100809633731842, 0.2688519060611725, 0.09065398573875427, -0.19516941905021667, -0.05906379967927933, -0.1169675886631012, -0.050680264830589294, 0.12673437595367432, 0.08410758525133133, -0.06714396923780441, 0.12659284472465515, -0.008290731348097324, -0.23185056447982788, -0.046531278640031815, 0.015470622107386589, -0.1619962900876999, 0.13613954186439514, -0.13358674943447113], "sparse_embedding": null}, {"id": "9cddd29dd173d2bc65c5919560c8b6250120323af4e1529c23a5c890389fec89", "content": "Ensuring that large language models (LLMs) reflect diverse user values and\npreferences is crucial as their user bases expand globally. It is therefore\nencouraging to see the growing interest in LLM personalization within the\nresearch community. However, current works often rely on the LLM-as-a-Judge\napproach for evaluation without thoroughly examining its validity. In this\npaper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking\nLLMs to judge user preferences based on personas. Our findings suggest that\ndirectly applying LLM-as-a-Personalized-Judge is less reliable than previously\nassumed, showing low and inconsistent agreement with human ground truth. The\npersonas typically used are often overly simplistic, resulting in low\npredictive power. To address these issues, we introduce verbal uncertainty\nestimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to\nexpress low confidence on uncertain judgments. This adjustment leads to much\nhigher agreement (above 80%) on high-certainty samples for binary tasks.\nThrough human evaluation, we find that the LLM-as-a-Personalized-Judge achieves\ncomparable performance to third-party humans evaluation and even surpasses\nhuman performance on high-certainty samples. Our work indicates that\ncertainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for\ndeveloping more reliable and scalable methods for evaluating LLM\npersonalization.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2406.11657v1", "title": "Can LLM be a Personalized Judge?", "content": "http://arxiv.org/pdf/2406.11657v1", "datetime": "2024-06-17 15:41:30", "query": "LLMs as judges", "linkedin": "\ud83c\udf1f Exciting Research Alert! \ud83c\udf1f\n\nEnsuring that large language models truly reflect diverse user values and preferences is crucial as their global user bases expand. The latest research delves into the realm of LLM personalization, aiming to enhance user experience. Check out the intriguing findings on the reliability of LLM-as-a-Personalized-Judge approach in evaluating user preferences based on personas. \n\nDiscover more about this insightful study and its implications for the future of LLM personalization here: http://arxiv.org/abs/2406.11657v1\n\n#AI #LLM #NLP #Personalization #Research #Tech #Innovation #UserExperience", "x": "\"Just in: Research investigates the reliability of LLM-as-a-Personalized-Judge for evaluating user preferences based on personas. Findings show low reliability without verbal uncertainty estimation. Learn more at: http://arxiv.org/abs/2406.11657v1 #LLM #personalization #AI\"", "source_id": "6ddcd6573e73f9ba02f3b63fbff244531fe43524afb3480d050a33d24ca5bbdb", "page_number": 1}, "score": null, "embedding": [-0.4771283268928528, 0.015173480845987797, -0.004386215005069971, -0.20850633084774017, -0.18152137100696564, -0.004693531896919012, 0.2453608512878418, 0.030227236449718475, 0.3573363423347473, -0.11857662349939346, 0.10506735742092133, -0.05253875255584717, 0.2354157269001007, 0.26748013496398926, 0.330457478761673, 0.2984898090362549, -0.18415485322475433, 0.07012930512428284, -0.05855478718876839, 0.0780784860253334, 0.19059520959854126, -0.20726697146892548, 0.11451887339353561, -0.06637217849493027, -0.06885787099599838, -0.18819348514080048, -0.2014513462781906, -0.19993171095848083, -0.23151230812072754, -1.3388080596923828, 0.10099723190069199, 0.05868147313594818, 0.4981575608253479, -0.1484513282775879, -0.25182127952575684, -0.05914366617798805, -0.1826033592224121, 0.013668138533830643, -0.05705367773771286, 0.07896849513053894, 0.03865816816687584, 0.08522941172122955, 0.09023217856884003, -0.2209307998418808, 0.030735306441783905, -0.25457438826560974, -0.008221340365707874, -0.07290201634168625, -0.7757787704467773, 0.07769148796796799, 0.005909196101129055, -0.3541406989097595, 0.0754387304186821, 0.411799818277359, -0.009665969759225845, 0.022586073726415634, 0.3722342550754547, 0.15121065080165863, -0.006591612473130226, 0.13325762748718262, 0.0762350931763649, 0.2585219442844391, -0.9713201522827148, 0.3287394940853119, -0.22940418124198914, 0.08269315212965012, -0.055774036794900894, -0.2122296690940857, -0.31209930777549744, -0.012211170978844166, 0.14411592483520508, 0.2753341794013977, 0.3997291028499603, 0.556306004524231, 0.3403854966163635, 0.2009720653295517, 0.22420528531074524, -0.11993087083101273, 0.19903860986232758, 0.1576625108718872, -0.045014284551143646, 0.20255404710769653, -0.13248509168624878, -0.4719676077365875, -0.15083430707454681, -0.08487895876169205, -0.017441535368561745, -0.2494370937347412, -0.16862407326698303, -0.13924245536327362, -0.013977414928376675, -0.11599660664796829, -0.28273582458496094, 0.26556140184402466, -0.012377689592540264, -0.06291624158620834, 0.09113838523626328, -0.09460265189409256, -0.4317893981933594, 0.6617931127548218, -0.017121322453022003, 0.3015159070491791, -0.04994962736964226, 0.013357758522033691, 0.2621499001979828, 0.0683603435754776, -0.2156866043806076, -0.26847729086875916, -0.17393583059310913, -0.2698436975479126, -0.06732293963432312, 0.14968986809253693, 0.04270334914326668, -0.07382135838270187, 0.07742799818515778, 0.35054653882980347, 0.45541679859161377, 0.03845309466123581, 0.1708887368440628, -0.30485934019088745, -0.27035266160964966, 0.21668609976768494, -0.15342988073825836, -0.05234890058636665, 0.02193148247897625, -0.06008638069033623, 0.2302229106426239, 0.43429821729660034, -0.07612156867980957, -0.029366319999098778, 0.17246076464653015, -0.2093418538570404, -0.3818499445915222, -0.13048730790615082, 0.1327364146709442, -0.045711636543273926, -0.11269918084144592, 0.0935172438621521, 0.24218562245368958, 0.2241935133934021, 0.05659991875290871, -0.1567009538412094, 0.15881089866161346, 0.006260842550545931, -0.5479077696800232, 0.6585726737976074, -0.19193346798419952, 0.006908229552209377, -0.1854998767375946, 0.02546105720102787, 0.15788835287094116, 0.24723418056964874, -0.17009110748767853, -0.01813340187072754, 0.2915540635585785, 0.298924058675766, 0.27021586894989014, 0.004410076420754194, -0.5041830539703369, 0.03752022609114647, 0.2755114734172821, 0.04630165547132492, -0.09328573942184448, 0.5505943894386292, -0.23170359432697296, -0.3349475562572479, -0.2891122102737427, -0.07821456342935562, 0.2895428240299225, -0.032438136637210846, 0.28702619671821594, -0.011827893555164337, -0.3080584406852722, 0.25609713792800903, -0.07974808663129807, 0.19927939772605896, -0.328034371137619, 0.0573589913547039, -0.10405899584293365, 0.16376270353794098, 0.2308959662914276, -0.20374652743339539, -0.07700259238481522, 0.021049784496426582, -0.22837094962596893, -0.1408156305551529, -0.1397259533405304, -0.4171721637248993, 0.2577323317527771, 0.009482135996222496, -0.2456168532371521, 0.13415373861789703, -0.10460130125284195, -0.03711441159248352, -0.20414352416992188, -0.3281567692756653, -0.1286466419696808, -0.23314324021339417, 0.03617844358086586, -0.12881720066070557, 0.026146577671170235, 0.404676616191864, 0.1119263768196106, -0.12865594029426575, 0.31402388215065, 0.26962077617645264, 0.09167627990245819, -0.09071287512779236, 0.45820069313049316, 0.3335919976234436, -0.3128978908061981, 0.15796899795532227, 0.11660513281822205, 0.08483559638261795, -0.10003955662250519, -0.031175222247838974, 0.4311855137348175, 0.27946293354034424, 0.07199481129646301, -0.017504015937447548, 0.03599366918206215, 0.33022016286849976, -0.1701669842004776, -1.2371631860733032, -0.06671039760112762, 0.04240721836686134, 0.1822396069765091, 0.13544723391532898, -0.36431795358657837, 0.06420589983463287, 0.12893465161323547, 0.19583405554294586, 0.32467788457870483, -0.05204665660858154, -0.17734673619270325, -0.1923241913318634, 0.5377672910690308, 0.07424285262823105, 0.09368938952684402, -0.20686371624469757, 0.193711057305336, -0.27792391180992126, 0.09078355878591537, -0.4204302728176117, 0.1447589099407196, 0.02377142943441868, -0.8325920701026917, 0.11709660291671753, -6.786396988900378e-05, 0.7226023077964783, -0.2138495296239853, -0.31065505743026733, 0.0051444354467093945, 0.07638391852378845, 0.18914808332920074, -0.0488397479057312, -0.6701429486274719, 0.440797358751297, 0.20490729808807373, -0.18273819983005524, -0.13077551126480103, -0.08330914378166199, -0.18071791529655457, -0.1451832801103592, -0.05866799131035805, 0.2152327597141266, -0.68604576587677, -0.44581806659698486, -0.15254439413547516, 0.16341258585453033, 0.20891521871089935, -0.39917874336242676, 0.14373770356178284, 0.20907624065876007, -0.021914593875408173, 0.1741267889738083, -0.17392182350158691, -0.08213919401168823, -0.25416848063468933, -0.6122554540634155, 0.11312758922576904, -0.016129830852150917, 0.2761518955230713, 0.1849089413881302, -0.25831952691078186, -0.15693941712379456, -0.25532692670822144, 0.337249755859375, -0.5351234078407288, 0.007858709432184696, -0.04751281812787056, 0.34808263182640076, -0.016717754304409027, -0.19996590912342072, 0.4151773452758789, -0.022694416344165802, 0.039201702922582626, 0.20892155170440674, 0.2987005412578583, -0.14710886776447296, -0.11544641852378845, -0.2712172865867615, -0.1298646330833435, 0.4901279807090759, 0.20752966403961182, 0.08527036011219025, 0.11405223608016968, 0.05338499695062637, 0.0174040999263525, 0.1265113204717636, 0.2412993311882019, 0.011252352967858315, 0.1326589584350586, 0.012817849405109882, -0.05803762003779411, -0.19819456338882446, -0.21241188049316406, -0.014758060686290264, -0.00878799706697464, -1.160658359527588, -0.0018310253508388996, -0.40094298124313354, 0.39596518874168396, -0.12077641487121582, 0.10880932956933975, 0.23009997606277466, -0.029448848217725754, -0.0945887491106987, 0.099296435713768, -0.019754571840167046, 0.2614741325378418, 0.17055203020572662, -0.15247273445129395, 0.16876383125782013, -0.20392249524593353, 0.4593621790409088, -0.016728950664401054, 0.11076148599386215, -0.44268956780433655, 0.09391681104898453, 0.14517371356487274, 1.0220484733581543, 0.007374611683189869, 0.02126525528728962, 0.39018023014068604, 0.03865300118923187, -0.13361161947250366, -0.1669965237379074, -0.06461870670318604, -0.25156083703041077, 0.06831397861242294, 0.8062664866447449, -0.0781545341014862, 0.09991266578435898, 0.4043094217777252, -0.3635925054550171, -0.03327525034546852, 0.23934444785118103, -0.08155826479196548, 0.31750136613845825, -0.09928902983665466, 0.03905898705124855, -0.18883539736270905, 0.9107034802436829, 0.07976926863193512, 0.12848292291164398, 0.04840841516852379, 0.04031096398830414, -0.10324115306138992, -0.1872367113828659, -0.09063760191202164, -0.15114209055900574, -0.04724659025669098, 0.3633178770542145, 0.08199606835842133, 0.1597384214401245, 0.11471427232027054, -0.04990031570196152, -0.2438206672668457, -0.007536006160080433, 0.3481373190879822, 0.058063309639692307, -0.13829489052295685, 0.03307952731847763], "sparse_embedding": null}, {"id": "ef6d813e2c26d16a046f50650c4514ae0924c549b49b4a1677503d0ff844be92", "content": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-source models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this study, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\npropose an effective indicator to measure the reliability of fine-tuned judges,\nwith the aim of maximizing their utility in LLM evaluation.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.02839v2", "title": "On the Limitations of Fine-tuned Judge Models for LLM Evaluation", "content": "http://arxiv.org/pdf/2403.02839v2", "datetime": "2024-06-17 12:10:34", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Just in: A new study delves into the evaluation of Large Language Models (LLMs) using fine-tuned judge models. Findings reveal key insights on performance, generalizability, fairness, and scalability in comparison to using GPT-4 as the evaluator. Dive deeper into the research here: [Read more](http://arxiv.org/abs/2403.02839v2) #LLM #AI #NLP #Research #GPT4 #TechInnovation", "x": "\ud83d\ude80 New research alert! Discover the latest insights on utilizing Large Language Models for evaluation and fine-tuning judge models in AI. Find out more about the study's findings and proposed indicators for maximizing utility in LLM evaluation: \nhttp://arxiv.org/abs/2403.02839v2 #AI #LLM #NLP #Research #TechInnovation", "source_id": "4f4519c2d1bc4a243ff2e4d7f39729391bfe40df9fbaeb9a62d371876204222c", "page_number": 1}, "score": null, "embedding": [-0.3251035511493683, 0.023360593244433403, 0.09744531661272049, -0.12226183712482452, 0.026433028280735016, -0.04830925539135933, -0.2444395124912262, 0.30486205220222473, 0.2831912636756897, -0.2531719505786896, -0.11050106585025787, -0.0491807796061039, 0.13196010887622833, 0.17381025850772858, 0.14192484319210052, 0.19376395642757416, -0.16278107464313507, 0.22149285674095154, -0.042815811932086945, 0.007498209364712238, 0.35217446088790894, -0.05867389217019081, 0.06748823076486588, -0.13645124435424805, 0.029414519667625427, 0.06711667031049728, -0.06501349061727524, -0.2335873246192932, -0.22522954642772675, -1.6648340225219727, -0.046526145190000534, -0.05373238027095795, 0.05925339460372925, -0.029792722314596176, -0.3583736717700958, -0.03325953707098961, -0.37727296352386475, -0.21826983988285065, -0.26862800121307373, -0.020788878202438354, 0.10112939774990082, 0.38309600949287415, -0.13053691387176514, -0.19693587720394135, 0.009283303283154964, -0.3914462924003601, -0.2725512981414795, 0.08188164234161377, -0.7155698537826538, -0.09550579637289047, 0.08182115107774734, -0.3641000986099243, -0.03119926154613495, 0.19507735967636108, -0.14258603751659393, -0.019877275452017784, 0.14724454283714294, 0.09796919673681259, 0.12158644199371338, -0.045875102281570435, 0.2248469442129135, 0.5354433059692383, -0.9743764996528625, 0.10868186503648758, -0.055718664079904556, 0.2549254894256592, -0.10064505785703659, -0.009988714009523392, -0.011108587495982647, 0.1705673187971115, 0.07306614518165588, 0.07621262967586517, 0.37692511081695557, 0.1881234496831894, 0.14804895222187042, 0.3046209216117859, 0.17971301078796387, -0.1381571739912033, 0.060600828379392624, 0.1871204674243927, 0.29164794087409973, -0.24789337813854218, 0.056771036237478256, -0.45392319560050964, 0.052997834980487823, -0.06687123328447342, -0.15980271995067596, -0.0501592643558979, 0.17269647121429443, -0.0416521392762661, 0.02089737541973591, -0.040289849042892456, -0.15537984669208527, 0.25083109736442566, -0.09990072250366211, 0.13446485996246338, 0.38319769501686096, -0.06894047558307648, -0.3533267676830292, 0.6771354675292969, -0.15803618729114532, 0.18428345024585724, -0.03683832660317421, 0.057372577488422394, 0.21660983562469482, 0.04870546609163284, 0.033723216503858566, -0.1352532058954239, 0.000804979121312499, -0.12350152432918549, 0.01582159288227558, 0.07819655537605286, 0.20163270831108093, -0.15824419260025024, -0.11883706599473953, 0.1375051587820053, 0.26160475611686707, -0.14035916328430176, 0.08238514512777328, -0.22075259685516357, -0.33068379759788513, 0.24085649847984314, -0.18574410676956177, -0.15472018718719482, 0.28519895672798157, -0.0311985332518816, 0.08100124448537827, 0.36094287037849426, 0.058408427983522415, 0.14405934512615204, -0.033730506896972656, -0.37845557928085327, -0.1982978731393814, -0.1287880837917328, 0.1383042186498642, 0.010505303740501404, 0.11623892933130264, 0.1370171457529068, 0.2239057421684265, 0.29682353138923645, -0.12724138796329498, -0.2188359647989273, 0.07856830209493637, -0.1761517971754074, -0.4315263032913208, 0.5530719757080078, -0.1813417375087738, -0.1572527140378952, -0.24642036855220795, -0.05193345621228218, 0.24304845929145813, 0.26490581035614014, 0.054408278316259384, -0.12393554300069809, 0.38835805654525757, 0.18360574543476105, 0.15414628386497498, 0.13044321537017822, -0.45313596725463867, 0.051469869911670685, -0.0012518464354798198, 0.186393603682518, -0.2119925320148468, 0.6894049644470215, -0.3215290904045105, -0.2442379891872406, -0.40859416127204895, -0.02897954359650612, 0.13937555253505707, -0.16727064549922943, 0.311201810836792, 0.051352839916944504, -0.14337530732154846, -0.08198538422584534, -0.19408702850341797, -0.1800266057252884, -0.426488995552063, 0.07969340682029724, -0.2709971070289612, 0.02635507844388485, 0.29756200313568115, -0.04581642895936966, -0.32203784584999084, 0.16797029972076416, -0.10511951893568039, -0.11074382811784744, 0.04558934271335602, -0.3865469992160797, 0.1814066469669342, -0.025196941569447517, -0.50513756275177, 0.07391833513975143, -0.07062521576881409, 0.24306441843509674, -0.10192651301622391, -0.23518912494182587, -0.23982667922973633, -0.27247241139411926, 0.24051235616207123, -0.15244828164577484, 0.045866530388593674, 0.12777119874954224, 0.12410411983728409, -0.23968105018138885, -0.04749823734164238, 0.09381966292858124, -0.05119294673204422, -0.1371949315071106, 0.17518354952335358, 0.46117734909057617, -0.4005707800388336, 0.26397407054901123, 0.12743262946605682, 0.13663236796855927, -0.06514831632375717, 0.0016502946382388473, 0.2523181140422821, 0.34386783838272095, -0.20824815332889557, 0.21539776027202606, 0.04598892852663994, 0.28449925780296326, -0.35217565298080444, -1.207904577255249, -0.16537214815616608, -0.11542374640703201, 0.08000585436820984, 0.48803088068962097, -0.39261457324028015, 0.09046516567468643, 0.09230668842792511, 0.35343173146247864, 0.2737025320529938, 0.0025465027429163456, 0.11698586493730545, -0.20803263783454895, 0.159144788980484, 0.004453467670828104, -0.02978566102683544, -0.20900723338127136, 0.2802867889404297, -0.28382381796836853, 0.23611629009246826, -0.11795265972614288, 0.26787900924682617, 0.1679736077785492, -0.7408063411712646, 0.20316307246685028, -0.11983095854520798, 0.7319365739822388, -0.3323521912097931, -0.06533104926347733, -0.19139577448368073, 0.25301411747932434, 0.09739145636558533, 0.08894049376249313, -0.2866409420967102, 0.6121429800987244, 0.0017659015720710158, 0.038055669516325, 0.1360992193222046, 0.037426579743623734, 0.021817246451973915, -0.029057404026389122, -0.10108871012926102, 0.17305155098438263, -0.7441793084144592, -0.38069990277290344, 0.1572859138250351, 0.083831787109375, 0.04838065430521965, -0.4562429189682007, 0.36260437965393066, 0.01606266014277935, -0.17392238974571228, 0.24656234681606293, -0.006993803661316633, -0.16429810225963593, -0.32982325553894043, -0.7160959839820862, 0.239010751247406, -0.06770823150873184, 0.0944695919752121, 0.19374190270900726, -0.07091130316257477, -0.19903047382831573, -0.015639297664165497, 0.28160372376441956, -0.2592442035675049, -0.0740109458565712, -0.09263075888156891, 0.19842489063739777, -0.21057778596878052, 0.0759798213839531, 0.3941015303134918, 0.04567690193653107, 0.15337935090065002, 0.002280217595398426, 0.11907397955656052, -0.13770867884159088, -0.016750311478972435, -0.3097243905067444, -0.17317640781402588, 0.49235472083091736, 0.5938024520874023, 0.10117839276790619, -0.06357567757368088, -0.1535964161157608, 0.16386465728282928, 0.2810588777065277, 0.05737510696053505, 0.008009391836822033, 0.141139954328537, -0.06286872178316116, 0.2544671893119812, -0.235057532787323, -0.14031974971294403, 0.040955204516649246, 0.1550760418176651, -1.0056474208831787, 0.05457038804888725, -0.06190064921975136, 0.17632301151752472, 0.004574141930788755, -0.044668275862932205, 0.12024890631437302, -0.289396196603775, -0.09267635643482208, -0.07657767087221146, 0.10685193538665771, 0.28274181485176086, 0.010125428438186646, 0.0031168516725301743, 0.20977628231048584, -0.029624609276652336, 0.4025425314903259, -0.3604106307029724, 0.2567640542984009, -0.3434065580368042, -0.0522766187787056, 0.42832323908805847, 0.9622629284858704, -0.061357561498880386, 0.10832937806844711, 0.25150686502456665, 0.0871429517865181, 0.01726306602358818, 0.1432865709066391, 0.09604669362306595, -0.01916119083762169, 0.01112444419413805, 1.011250615119934, -0.07967379689216614, 0.1196998730301857, 0.7047435641288757, -0.14251790940761566, -0.009020394645631313, 0.19796687364578247, -0.16270330548286438, 0.6192656755447388, -0.12703628838062286, 0.043386898934841156, -0.03286962956190109, 0.5927512645721436, 0.17118299007415771, -0.045499950647354126, -0.0918145403265953, -0.03404538333415985, -0.022229742258787155, -0.15950025618076324, -0.06221656873822212, -0.174052432179451, -0.2404390126466751, 0.25579845905303955, 0.2662951350212097, 0.033803362399339676, 0.2076813131570816, -0.0324631929397583, -0.1940542459487915, 0.09817515313625336, -0.17268294095993042, 0.022971298545598984, 0.39668938517570496, -0.2311757653951645], "sparse_embedding": null}, {"id": "cc7b4cb45b132cc3d23ba68a3834cd8451e8162897864fc7aba7ad6a63370881", "content": "Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, and multi-turn chat.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.17631v1", "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges", "content": "http://arxiv.org/pdf/2310.17631v1", "datetime": "2023-10-26 17:48:58", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Exciting advancements in the field of Large Language Models (LLMs)! Researchers have introduced JudgeLM, fine-tuned LLMs acting as scalable judges to efficiently evaluate LLMs in open-ended scenarios. By leveraging a high-quality dataset and novel benchmark, JudgeLM has achieved state-of-the-art performance, outperforming human-level agreement. \n\nCurious to learn more about this groundbreaking approach and its implications? Check out the full paper at: http://arxiv.org/abs/2310.17631v1\n\n#AI #NLP #LLMs #JudgeLM #TechInnovation #Research #ArtificialIntelligence", "x": "\ud83d\ude80 Exciting new research on evaluating Large Language Models (LLMs) efficiently and effectively in open-ended scenarios! Learn how JudgeLM fine-tunes LLMs as scalable judges, achieving state-of-the-art performance and surpassing human agreement. Check out the paper here: http://arxiv.org/abs/2310.17631v1 #AI #NLP #LLMs #JudgeLM", "source_id": "de7badc76e9f57b140fe8e646c8a8cc157583cab826d4b0d7c807ccc94c91f29", "page_number": 1}, "score": null, "embedding": [-0.40727168321609497, -0.19536390900611877, 0.05350472405552864, -0.1523999571800232, 0.06818462163209915, 0.08997891843318939, -0.32292789220809937, 0.06884883344173431, 0.4194769561290741, -0.019729552790522575, 0.10747530311346054, -0.1365586221218109, 0.1273912489414215, 0.2685379683971405, 0.0893033891916275, 0.17661860585212708, -0.036135848611593246, 0.14752334356307983, -0.19731417298316956, 0.0031720835249871016, 0.13499462604522705, -0.06850045174360275, -0.10397078096866608, -0.041105493903160095, 0.022087812423706055, 0.07492991536855698, -0.30788561701774597, -0.48528918623924255, -0.2606027126312256, -1.5013729333877563, 0.20856373012065887, -0.03848148137331009, 0.16125702857971191, -0.015903351828455925, -0.2923797070980072, -0.017001817002892494, -0.12541517615318298, 0.06643403321504593, -0.06501149386167526, 0.04632960632443428, 0.2384559065103531, 0.3196389377117157, 0.014369803480803967, -0.13736820220947266, -0.06138597056269646, -0.39056825637817383, -0.1312185376882553, -0.02571910247206688, -0.7194406390190125, 0.13034948706626892, -0.2451821118593216, -0.49583303928375244, -0.014852811582386494, 0.22008691728115082, -0.11241461336612701, 0.13745087385177612, 0.14642715454101562, 0.18830472230911255, 0.17003333568572998, 0.0043487767688930035, 0.3022787868976593, 0.11305522918701172, -0.9073123335838318, 0.19013898074626923, -0.05730695277452469, 0.11863338947296143, -0.04416465386748314, -0.06168996915221214, 0.05804118886590004, 0.32157814502716064, 0.029167532920837402, 0.034293267875909805, 0.5749931335449219, 0.264965295791626, 0.2164149433374405, 0.34131407737731934, -0.013569154776632786, -0.24332760274410248, 0.1845329850912094, 0.21143387258052826, -0.01468242984265089, -0.32470327615737915, -0.1276961714029312, -0.45752450823783875, -0.0911816954612732, 0.014415629208087921, -0.18702705204486847, -0.0667126327753067, 0.06298664957284927, -0.07658355683088303, 0.20019105076789856, -0.10638398677110672, -0.3065168857574463, 0.37600117921829224, -0.12446229159832001, -0.04992017522454262, 0.336974173784256, -0.06980796158313751, -0.24649927020072937, 0.5320882201194763, -0.1841495782136917, 0.23654423654079437, -0.12638480961322784, -0.10747423768043518, 0.3308354318141937, -0.023630429059267044, -0.05940316990017891, -0.30737218260765076, -0.02048596180975437, -0.23729878664016724, -0.03341463580727577, -0.08223410695791245, 0.1216018944978714, -0.08301711082458496, 0.053111013025045395, 0.24673201143741608, 0.35306233167648315, -0.16622059047222137, -0.01021614857017994, -0.20870010554790497, -0.3880661427974701, 0.17167772352695465, -0.10020121186971664, -0.1956157684326172, 0.17694838345050812, -0.020863112062215805, 0.13615800440311432, 0.4254196882247925, 0.11441807448863983, 0.04172695055603981, 0.11804454028606415, -0.19079287350177765, -0.5152077078819275, -0.09737333655357361, 0.15530802309513092, 0.007730205077677965, 0.1615341305732727, 0.10787630081176758, 0.20927836000919342, 0.16705773770809174, 0.0620872899889946, -0.2871660888195038, 0.08149729669094086, -0.19888919591903687, -0.41453853249549866, 0.7260327339172363, -0.11889290064573288, -0.027505237609148026, -0.3722643554210663, -0.23987025022506714, 0.10614470392465591, 0.30406343936920166, 0.03291520103812218, -0.2826386094093323, 0.11476535350084305, 0.20253655314445496, 0.17610689997673035, 0.1927788257598877, -0.2718668580055237, -0.005147374235093594, -0.0878048986196518, 0.07037565112113953, -0.1300046294927597, 0.6741523146629333, -0.19685856997966766, -0.2423410713672638, -0.3729993402957916, -0.028232941403985023, 0.003453813726082444, -0.014404532499611378, 0.3082292675971985, -0.0884615033864975, -0.26474815607070923, 0.058044642210006714, -0.0685841292142868, -0.14778606593608856, -0.5142064690589905, 0.005768163595348597, -0.14254677295684814, -0.0863301157951355, 0.338703989982605, -0.08631826937198639, -0.22149881720542908, 0.08843351900577545, -0.14382454752922058, -0.2692953646183014, 0.028852878138422966, -0.34838515520095825, 0.1482459008693695, 0.07267652451992035, -0.3577762842178345, 0.17237453162670135, -0.07628333568572998, 0.1888580620288849, -0.1289616972208023, -0.3968948721885681, -0.06791732460260391, -0.10796768963336945, 0.3020770847797394, -0.049893058836460114, 0.06839267164468765, 0.1848396509885788, -0.06026623770594597, -0.08748172968626022, 0.2190054953098297, 0.013692679814994335, -0.049419570714235306, -0.12099801003932953, 0.35432982444763184, 0.4341203570365906, -0.24268661439418793, 0.09114489704370499, 0.05118167772889137, 0.12943121790885925, -0.1643536537885666, -0.17354099452495575, 0.4030326306819916, 0.25985491275787354, -0.3585151433944702, 0.2302120327949524, 0.08390281349420547, 0.1322537660598755, -0.19678795337677002, -1.2508662939071655, -0.0877620056271553, 0.10147527605295181, 0.04141319543123245, 0.41777002811431885, -0.242755725979805, 0.2223140150308609, -0.03673125430941582, 0.24781522154808044, 0.3598170280456543, -0.028155673295259476, 0.12118276953697205, -0.16706368327140808, 0.15388420224189758, 0.04037816822528839, -0.050210773944854736, -0.411220908164978, 0.25327375531196594, -0.3193773925304413, 0.21900004148483276, -0.005161735229194164, 0.2399120032787323, -0.09801176935434341, -0.8124167919158936, 0.3427736759185791, -0.16060812771320343, 0.7888489961624146, -0.33783167600631714, -0.021211931481957436, -0.20599965751171112, 0.17592692375183105, 0.06258414685726166, 0.06119461730122566, -0.36338141560554504, 0.49976226687431335, 0.20161142945289612, 0.13192182779312134, 0.16649341583251953, 0.1303580403327942, 0.08593999594449997, -0.08363094925880432, -0.08779226988554001, 0.053221702575683594, -0.6527836322784424, -0.3411092460155487, 0.23357674479484558, 0.20952452719211578, -0.05486970394849777, -0.5001561045646667, 0.2905641496181488, 0.013822680339217186, -0.1523704081773758, 0.3030807077884674, 0.03142230585217476, -0.13231757283210754, -0.2918354570865631, -0.6409048438072205, 0.07816088199615479, -0.1219915971159935, 0.03353935107588768, 0.10340750217437744, -0.12768356502056122, -0.31630849838256836, -0.09818746894598007, 0.3437459468841553, -0.22269318997859955, 0.0747745931148529, 0.04876719042658806, 0.007584339007735252, -0.2687622308731079, -0.10192305594682693, 0.26934924721717834, -0.0976480022072792, 0.19653964042663574, 0.37869080901145935, 0.15184983611106873, -0.0725419744849205, 0.04793349653482437, -0.22432997822761536, -0.010209505446255207, 0.5215165615081787, 0.29206714034080505, -0.07389326393604279, 0.1352895200252533, -0.023160578683018684, 0.11275814473628998, 0.3869616687297821, 0.05694857984781265, -0.023433668538928032, 0.24737296998500824, 0.06317232549190521, 0.1535526067018509, -0.28911423683166504, -0.03380407765507698, -0.21033506095409393, 0.25980231165885925, -0.9382908344268799, 0.13791139423847198, -0.20653748512268066, 0.13655740022659302, 0.13966801762580872, 0.05594104155898094, 0.053167279809713364, -0.12727025151252747, 0.19494245946407318, 0.09123462438583374, -0.0742029920220375, 0.27849093079566956, -0.07751142978668213, -0.033585257828235626, 0.20151515305042267, -0.1634635180234909, 0.44594335556030273, -0.032035622745752335, 0.3498920202255249, -0.2151385098695755, -0.05247778445482254, 0.29105931520462036, 0.9573483467102051, -0.13182765245437622, -0.012777110561728477, 0.2427607774734497, 0.023279665037989616, -0.19176824390888214, 0.24011574685573578, 0.04156416282057762, 0.06241140887141228, -0.0662686675786972, 0.9341022968292236, -0.10146380960941315, -0.1311539113521576, 0.7446792125701904, -0.179967999458313, -0.040984466671943665, 0.2547714412212372, 0.010469729080796242, 0.7251814603805542, -0.18121933937072754, 0.13978250324726105, -0.13078944385051727, 0.6209569573402405, 0.13402043282985687, -0.008161955513060093, -0.30427077412605286, 0.23370008170604706, -0.1142905056476593, -0.14119331538677216, 0.08747798949480057, -0.3167281746864319, -0.018917741253972054, 0.12233666330575943, 0.12453234940767288, 0.2410135269165039, 0.1891583502292633, -0.10103489458560944, -0.2024305909872055, 0.11283737421035767, -0.1135726124048233, -0.12753817439079285, 0.06824953109025955, -0.146882563829422], "sparse_embedding": null}, {"id": "ad26039fc4e1875e4fe645df28ec3e8ea82fd42edf299860ff2ab0939bf8118d", "content": "Large Language Models (LLMs) are powerful zero-shot assessors and are\nincreasingly used in real-world situations such as for written exams or\nbenchmarking systems. Despite this, no existing work has analyzed the\nvulnerability of judge-LLMs against adversaries attempting to manipulate\noutputs. This work presents the first study on the adversarial robustness of\nassessment LLMs, where we search for short universal phrases that when appended\nto texts can deceive LLMs to provide high assessment scores. Experiments on\nSummEval and TopicalChat demonstrate that both LLM-scoring and pairwise\nLLM-comparative assessment are vulnerable to simple concatenation attacks,\nwhere in particular LLM-scoring is very susceptible and can yield maximum\nassessment scores irrespective of the input text quality. Interestingly, such\nattacks are transferable and phrases learned on smaller open-source LLMs can be\napplied to larger closed-source models, such as GPT3.5. This highlights the\npervasive nature of the adversarial vulnerabilities across different judge-LLM\nsizes, families and methods. Our findings raise significant concerns on the\nreliability of LLMs-as-a-judge methods, and underscore the importance of\naddressing vulnerabilities in LLM assessment methods before deployment in\nhigh-stakes real-world scenarios.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.14016v1", "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "content": "http://arxiv.org/pdf/2402.14016v1", "datetime": "2024-02-21 18:55:20", "query": "LLMs as judges", "linkedin": "\ud83d\udea8 New Research Alert! \ud83d\udea8\n\nExciting findings on the vulnerability of assessment Large Language Models (LLMs) have been published. This groundbreaking study sheds light on the susceptibility of judge-LLMs to manipulation by adversaries aiming to deceive the system and obtain high assessment scores.\n\nDiscover more about this innovative research and its implications for the reliability of LLMs in real-world scenarios by reading the full paper here: [http://arxiv.org/abs/2402.14016v1](http://arxiv.org/abs/2402.14016v1)\n\n#AI #NLP #LLMs #Research #TechInnovation #ArtificialIntelligence #MachineLearning #TechNews", "x": "\ud83d\udea8 New research alert! A study on the vulnerability of assessment Large Language Models (LLMs) reveals susceptibility to simple attacks, raising concerns about their reliability in real-world scenarios. Learn more at: http://arxiv.org/abs/2402.14016v1 #AI #NLP #LLMs #Research #TechEthics \ud83e\udd16\ud83d\udd0d\ud83d\udcda", "source_id": "573f9009d291a6f3dec58d2b0ea760e62b15dac3d71ab0c43830c424e9c384ae", "page_number": 1}, "score": null, "embedding": [-0.2748948335647583, -0.0022477342281490564, -0.16719026863574982, -0.18397141993045807, 0.05540198087692261, -0.005605350248515606, -0.08541622012853622, 0.22806981205940247, 0.192071333527565, -0.21739083528518677, -0.026835525408387184, -0.08689446747303009, 0.19434121251106262, 0.2557167112827301, -0.06570009887218475, 0.14670231938362122, -0.05748383328318596, 0.05502169579267502, -0.0078109740279614925, 0.21571961045265198, 0.44340378046035767, -0.060573842376470566, 0.2294774055480957, 0.10208531469106674, 0.10982783883810043, 0.04192926734685898, -0.11782574653625488, -0.24045009911060333, -0.36589211225509644, -1.5517057180404663, -0.08792577683925629, -0.04243791848421097, 0.20721863210201263, -0.0693914070725441, -0.28631120920181274, 0.06469761580228806, -0.22932927310466766, 0.117747001349926, -0.06071961671113968, 0.03323942422866821, 0.1932402104139328, 0.16946201026439667, -0.024349527433514595, -0.342668354511261, -0.08817930519580841, -0.5597801208496094, -0.2197370082139969, -0.13394924998283386, -0.4208371639251709, -0.055818237364292145, 0.073651023209095, -0.42077958583831787, 0.12881994247436523, 0.15525183081626892, 0.10291766375303268, -0.22132991254329681, 0.2964688241481781, 0.18003110587596893, -0.000782507355324924, 0.2347494214773178, 0.09458597004413605, 0.325673371553421, -1.0022960901260376, 0.2179373949766159, -0.012008363381028175, 0.1653946489095688, -0.06040046736598015, 0.18460099399089813, 0.043454356491565704, 0.2209397256374359, 0.12642495334148407, 0.18517087399959564, 0.20573389530181885, 0.3712881803512573, 0.27430570125579834, 0.2913559377193451, -0.012108078226447105, -0.18696674704551697, 0.2554892897605896, 0.13874046504497528, 0.29648298025131226, -0.09252926707267761, -0.11158601194620132, -0.31995126605033875, -0.06249938905239105, -0.253151535987854, -0.12552028894424438, -0.18400906026363373, 0.2789200246334076, -0.22876732051372528, 0.045736461877822876, -0.19831432402133942, 0.02964821644127369, 0.4068850874900818, -0.08501212298870087, -0.11505192518234253, 0.30564501881599426, -0.20644700527191162, -0.37279626727104187, 0.6251452565193176, -0.1740454137325287, 0.013335960917174816, -0.15782181918621063, -0.03627362474799156, 0.2971906363964081, 0.1262495070695877, -0.1450486034154892, -0.5388216972351074, -0.12473216652870178, -0.06972194463014603, -0.019681360572576523, 0.10906978696584702, 0.027359025552868843, -0.03868262469768524, 0.09056845307350159, 0.16443054378032684, 0.48767149448394775, -0.16479286551475525, -0.020281409844756126, -0.09906015545129776, -0.3915823698043823, 0.23190845549106598, 0.0013422671472653747, -0.21198487281799316, 0.2868545651435852, -0.046250469982624054, 0.1888597458600998, 0.45797428488731384, 0.2353219836950302, -0.004860179033130407, -0.07796522974967957, -0.24620693922042847, -0.21028701961040497, -0.009763821959495544, -0.07596208155155182, -0.06551490724086761, 0.09761283546686172, 0.09352915734052658, 0.2511047422885895, 0.2118469774723053, -0.17026139795780182, -0.05927770584821701, 0.29671892523765564, -0.006912773475050926, -0.2612488865852356, 0.5780943632125854, -0.09797507524490356, -0.004528946243226528, -0.3353394567966461, -0.27135658264160156, 0.24764032661914825, 0.2518017292022705, -0.07936134934425354, -0.1612003743648529, 0.26469507813453674, 0.1839723289012909, 0.08377978205680847, -0.018952855840325356, -0.13652057945728302, -0.18813562393188477, 0.043614860624074936, 0.08970289677381516, -0.13844457268714905, 0.7413995862007141, -0.20717480778694153, -0.2665884792804718, -0.06983812153339386, 0.0287430752068758, 0.20992214977741241, -0.16658508777618408, 0.2833139896392822, -0.12714263796806335, -0.24126572906970978, -0.06231885030865669, -0.3477737605571747, -0.045066408812999725, -0.5074182152748108, -0.17570194602012634, -0.20585842430591583, 0.17847083508968353, -0.02215939573943615, -0.13820773363113403, -0.022487102076411247, 0.28016185760498047, -0.0333755686879158, -0.17485976219177246, -0.011522735469043255, -0.43473339080810547, 0.3202655613422394, -0.028381653130054474, -0.4218600392341614, 0.06602945178747177, -0.21674679219722748, -0.0019485660595819354, -0.36307433247566223, -0.1554175466299057, -0.028621535748243332, -0.02448791079223156, 0.04440027102828026, -0.20771841704845428, 0.025758380070328712, 0.1755266636610031, -0.13831378519535065, -0.07058311253786087, 0.3520454466342926, -0.21800492703914642, 0.07525166869163513, -0.1892789751291275, 0.19008955359458923, 0.29971152544021606, -0.18331530690193176, -0.027218177914619446, 0.16663800179958344, 0.021077193319797516, -0.14092209935188293, 0.008564678952097893, 0.2552279531955719, 0.22821080684661865, -0.16908392310142517, 0.2612491250038147, 0.1719008833169937, 0.12309613078832626, -0.1485048085451126, -1.2107577323913574, -0.3277449607849121, -0.032394930720329285, 0.054246686398983, 0.16261683404445648, -0.3924504816532135, 0.05094026401638985, -0.14828407764434814, 0.30733320116996765, 0.3223763108253479, 0.10723329335451126, 0.04231865331530571, -0.2671133279800415, 0.2011748105287552, 0.10797344893217087, 0.15262740850448608, -0.5371409058570862, 0.09473811835050583, -0.1805741935968399, 0.3504299521446228, -0.35197192430496216, 0.1732887476682663, 0.03491492569446564, -0.5277422070503235, 0.02066042833030224, 0.025327544659376144, 0.7993298172950745, -0.0889255702495575, -0.07217390090227127, 0.023075059056282043, 0.16419167816638947, 0.2807125747203827, 0.08622131496667862, -0.2010742574930191, 0.44344547390937805, -0.05648931860923767, 0.045026861131191254, 0.13104528188705444, 0.03609431907534599, -0.02081920951604843, -0.15644855797290802, -0.12294001132249832, 0.17407426238059998, -0.7082095742225647, -0.4203140437602997, 0.12344326823949814, -0.00734924478456378, -0.0202732402831316, -0.3179384469985962, 0.28238198161125183, 0.017923811450600624, 0.12505967915058136, 0.42351141571998596, -0.052966780960559845, 0.0010766583727672696, -0.08268528431653976, -0.7134584784507751, 0.2219773530960083, -0.003441553097218275, 0.013251597993075848, 0.16181209683418274, -0.04869377240538597, -0.19236250221729279, -0.3048953711986542, 0.3383793532848358, -0.13562726974487305, 0.12407196313142776, -0.12690909206867218, 0.22256936132907867, 0.0021883687004446983, -0.09905560314655304, 0.619978666305542, 0.04740341007709503, 0.08046326041221619, 0.12820357084274292, 0.132542222738266, 0.11877544224262238, -0.2817402780056, -0.40216928720474243, -0.08243808150291443, 0.4730883836746216, 0.3064858913421631, -0.00023985342704690993, 0.11368081718683243, 0.1706172376871109, 0.10406749695539474, 0.16735254228115082, 0.20387712121009827, 0.13323186337947845, 0.15956221520900726, -0.07422155141830444, 0.04036710038781166, -0.2724662721157074, -0.08674411475658417, -0.0009869805071502924, 0.04814772680401802, -0.9835671186447144, -0.098302461206913, -0.07886416465044022, 0.1820240020751953, -0.19947409629821777, -0.03542996570467949, 0.07522127032279968, -0.1348259449005127, 0.004287162330001593, 0.3111415207386017, -0.11749020963907242, 0.23048147559165955, 0.05123734846711159, -0.1854587346315384, 0.031036876142024994, -0.2706964313983917, 0.4221087098121643, -0.30056998133659363, 0.267898291349411, -0.2974066734313965, 0.06378483027219772, 0.20898263156414032, 1.0108896493911743, -0.021940093487501144, -0.0142158642411232, 0.2865108549594879, 0.07722410559654236, 0.2950425446033478, -0.03287440165877342, -0.0994717925786972, 0.02126387506723404, -0.04156040400266647, 0.7477925419807434, 0.03379393741488457, 0.0187542587518692, 0.49020230770111084, -0.06315828859806061, -0.011152787134051323, 0.28764858841896057, -0.054654769599437714, 0.34372463822364807, 0.04257240891456604, -0.00665651960298419, -0.1154504343867302, 0.5589362978935242, 0.039453424513339996, 0.06037360429763794, -0.2154300957918167, 0.16201595962047577, -0.03947427123785019, -0.22193075716495514, -0.10605090856552124, 0.1054530218243599, -0.03526297211647034, 0.2877267301082611, 0.10059970617294312, 0.09807231277227402, -0.04141806811094284, -0.14153708517551422, -0.06953742355108261, 0.0283950362354517, 0.13365218043327332, 0.09362555295228958, 0.11641799658536911, -0.26369211077690125], "sparse_embedding": null}, {"id": "ecd1090feb2cefa189f30901fc838f310b18873555d3760d9aadfd495d0aa83d", "content": "As Large Language Models (LLMs) have become more advanced, they have outpaced\nour abilities to accurately evaluate their quality. Not only is finding data to\nadequately probe particular model properties difficult, but evaluating the\ncorrectness of a model's freeform generation alone is a challenge. To address\nthis, many evaluations now rely on using LLMs themselves as judges to score the\nquality of outputs from other LLMs. Evaluations most commonly use a single\nlarge model like GPT4. While this method has grown in popularity, it is costly,\nhas been shown to introduce intramodel bias, and in this work, we find that\nvery large models are often unnecessary. We propose instead to evaluate models\nusing a Panel of LLm evaluators (PoLL). Across three distinct judge settings\nand spanning six different datasets, we find that using a PoLL composed of a\nlarger number of smaller models outperforms a single large judge, exhibits less\nintra-model bias due to its composition of disjoint model families, and does so\nwhile being over seven times less expensive.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2404.18796v2", "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models", "content": "http://arxiv.org/pdf/2404.18796v2", "datetime": "2024-05-01 15:37:11", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Exciting advancements in Large Language Models (LLMs)! A recent study delves into evaluating LLM quality with a fresh approach - using a Panel of LLM evaluators (PoLL). The research suggests that employing multiple smaller models in the evaluation process yields superior results compared to a single large model like GPT4, while also reducing costs significantly. Learn more about this innovative methodology here: http://arxiv.org/abs/2404.18796v2 #AI #NLP #LLMs #TechInnovation \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udcc8", "x": "\ud83d\ude80 New study on evaluating the quality of Large Language Models (LLMs) reveals a more efficient method - Panel of LLM evaluators (PoLL). Using multiple smaller models outperforms a single large judge, reduces bias, and is over seven times less expensive. Check out the research at: http://arxiv.org/abs/2404.18796v2 #AI #NLP #LLM #Research #TechInnovation", "source_id": "5f9b54597b8a3c6eb514939884971d131ebc2d2160059403d7a7edbdcd182812", "page_number": 1}, "score": null, "embedding": [-0.15818332135677338, -0.048581600189208984, 0.18096806108951569, -0.1938246786594391, 0.08338365703821182, 0.09416014701128006, -0.35982874035835266, 0.05662345513701439, 0.33962318301200867, -0.07637479901313782, 0.03976272791624069, -0.0325951911509037, 0.042990654706954956, 0.21280178427696228, 0.13817906379699707, 0.2440638393163681, -0.15068237483501434, 0.0016324763419106603, 0.03261905536055565, 0.11327632516622543, 0.2830760180950165, -0.1995781660079956, 0.06583645194768906, -0.1283399611711502, 0.03332638368010521, 0.13703837990760803, -0.08609426766633987, -0.3057113587856293, -0.29237115383148193, -1.6198527812957764, 0.017902035266160965, 0.08078394830226898, 0.3204360902309418, 0.09913990646600723, -0.17249912023544312, -0.07528214156627655, -0.2579233646392822, -0.0936911329627037, -0.23905523121356964, 0.18947111070156097, 0.19935975968837738, 0.3160935044288635, -0.018422812223434448, -0.08025740087032318, -0.08285041898488998, -0.34657737612724304, -0.2494724690914154, 0.028882049024105072, -0.5578737854957581, -0.007136736065149307, 0.047606632113456726, -0.3779057264328003, 0.02741197496652603, 0.13616923987865448, -0.025368398055434227, 0.03047461062669754, 0.22631101310253143, 0.03455689549446106, -0.049569021910429, -0.06050201505422592, 0.27152079343795776, 0.4312992990016937, -0.9613226056098938, 0.23871688544750214, 0.11371456831693649, 0.3077283799648285, -0.03649613633751869, -0.026796704158186913, -0.13192254304885864, 0.1299087554216385, -0.14354953169822693, -0.08204715698957443, 0.3713757395744324, 0.29491522908210754, 0.18169009685516357, 0.131914421916008, 0.15650343894958496, 0.08478394150733948, 0.13011585175991058, 0.02549763210117817, -0.026274917647242546, -0.2390914261341095, 0.02809024788439274, -0.285343199968338, -0.15674062073230743, -0.142461359500885, -0.14541040360927582, -0.07874900102615356, 0.12494459003210068, -0.14927278459072113, 0.11857583373785019, -0.06579098850488663, -0.18776075541973114, 0.3510991930961609, -0.07885664701461792, 0.011425037868320942, 0.3034595549106598, -0.020887134596705437, -0.26513075828552246, 0.6785150766372681, -0.22003106772899628, 0.14623868465423584, -0.08799628168344498, 0.06560467183589935, 0.007739669177681208, -0.03420182690024376, -0.11847097426652908, -0.1026347279548645, -0.010248753242194653, -0.10640934854745865, 0.039574407041072845, 0.09913016855716705, 0.0124924685806036, -0.16606689989566803, -0.20510229468345642, -0.12801413238048553, 0.2608608901500702, -0.0777631402015686, 0.08327358961105347, -0.07365328073501587, -0.5271235108375549, 0.22636233270168304, -0.06641417741775513, -0.10408841818571091, 0.25342628359794617, -0.13386337459087372, 0.08223246037960052, 0.4756770730018616, 0.15956182777881622, 0.030802251771092415, -0.07119768112897873, -0.22945639491081238, -0.4222036898136139, -0.12306509912014008, 0.04350227862596512, -0.25802385807037354, 0.32301872968673706, 0.1453617960214615, 0.15346188843250275, 0.32087090611457825, -0.20479834079742432, -0.218847393989563, -0.021236123517155647, -0.16401150822639465, -0.3844151198863983, 0.6300001740455627, -0.1674860566854477, 0.09595756232738495, -0.2673148214817047, -0.09244426339864731, 0.04710913449525833, 0.4369660019874573, -0.20303258299827576, -0.24707812070846558, 0.41579803824424744, 0.20145367085933685, -0.13466410338878632, 0.152383491396904, -0.42026835680007935, 0.03495419770479202, 0.003170653013512492, 0.1848582923412323, -0.23675072193145752, 0.48229172825813293, -0.23368655145168304, -0.1307058483362198, -0.3368670344352722, 0.01353329699486494, 0.18443550169467926, -0.019884100183844566, 0.2012813538312912, -0.035983458161354065, -0.29903778433799744, 0.08757743239402771, -0.05260169133543968, -0.09822304546833038, -0.5034782290458679, 0.034441929310560226, -0.22846992313861847, 0.050582919269800186, 0.16739970445632935, -0.07148588448762894, -0.20961473882198334, 0.32838714122772217, -0.015081821009516716, -0.2133602499961853, -0.039055727422237396, -0.2419365644454956, 0.026709116995334625, 0.16396339237689972, -0.46054327487945557, 0.18438409268856049, -0.002774856286123395, 0.2248421013355255, -0.1886701136827469, -0.23737235367298126, -0.24233509600162506, -0.023867743089795113, 0.30413103103637695, -0.15206101536750793, -0.08390440791845322, -0.03251849487423897, 0.00847402773797512, -0.18465033173561096, 0.06285833567380905, 0.19766773283481598, 0.05630456656217575, -0.1197594404220581, 0.25710511207580566, 0.2665559649467468, -0.3039192855358124, 0.12003825604915619, 0.1895238310098648, -0.060047272592782974, -0.1621059626340866, 0.11639974266290665, 0.4932217597961426, 0.21566708385944366, -0.08537673950195312, 0.21368232369422913, 0.007924305275082588, 0.0027008794713765383, -0.18431660532951355, -1.1655395030975342, -0.2243274748325348, 0.14931775629520416, 0.26752448081970215, 0.3564784526824951, -0.5399073958396912, 0.16776670515537262, 0.03894132375717163, 0.22833676636219025, 0.610608696937561, -0.05747741460800171, 0.18193967640399933, -0.2594950795173645, 0.21895936131477356, -0.0013308849884197116, -0.00025095505407080054, -0.4214402735233307, 0.30196377635002136, -0.5035741925239563, 0.19049881398677826, -0.09138426184654236, 0.03127575293183327, 0.1395616978406906, -0.5160894393920898, 0.23527272045612335, -0.17999115586280823, 0.7344574928283691, -0.41539618372917175, 0.006799942348152399, 0.12699417769908905, 0.1503928154706955, 0.17249180376529694, 0.11691395938396454, -0.07547125965356827, 0.4294215142726898, 0.05112682282924652, 0.021231288090348244, 0.15979833900928497, 0.14112409949302673, 0.01889641024172306, -0.10071101784706116, -0.24850831925868988, 0.017810242250561714, -0.5549702048301697, -0.24541988968849182, 0.23751477897167206, 0.10755102336406708, -0.0023398692719638348, -0.5795411467552185, 0.40225154161453247, -0.05371077358722687, -0.12462220340967178, 0.43679338693618774, 0.012557493522763252, -0.01682077720761299, -0.14769130945205688, -0.7720913290977478, 0.1456136852502823, -0.098093181848526, 0.09755223244428635, 0.18543215095996857, -0.029912510886788368, -0.24916991591453552, -0.24934948980808258, 0.2225872427225113, -0.18786463141441345, -0.031727246940135956, -0.030530933290719986, 0.18879276514053345, -0.1454724222421646, 0.0061478582210838795, 0.4555460810661316, -0.2301691174507141, -0.07868791371583939, 0.1455642729997635, 0.11881030350923538, 0.07794539630413055, -0.24111118912696838, -0.30465415120124817, -0.08611755073070526, 0.464677631855011, 0.24545808136463165, -0.025462044402956963, 0.05042435973882675, -0.019582418724894524, 0.1047074943780899, 0.18094854056835175, 0.16776826977729797, -0.053927820175886154, 0.007121099624782801, -0.07226183265447617, 0.05331544950604439, -0.34164491295814514, 0.1395876705646515, -0.06435739248991013, -0.04420977458357811, -0.9259735345840454, 0.10384826362133026, -0.1289304494857788, 0.3449607789516449, 0.03375444933772087, -0.20713892579078674, -0.09278509765863419, -0.1493384689092636, 0.13570266962051392, -0.04273692145943642, 0.12237176299095154, 0.40008336305618286, 0.17710314691066742, -0.24517807364463806, 0.1602960079908371, -0.10232145339250565, 0.14881744980812073, -0.25705552101135254, 0.46465301513671875, -0.260624498128891, 0.17298392951488495, 0.15992136299610138, 0.9353548884391785, -0.014715099707245827, -0.004285495262593031, 0.2916151285171509, 0.01049319189041853, 0.04622962698340416, -0.10216565430164337, 0.044890910387039185, 0.01360079925507307, -0.01025103684514761, 0.7578657865524292, -0.019141709432005882, 0.1181395947933197, 0.4921872019767761, -0.17305482923984528, -0.07256264239549637, 0.29589396715164185, 0.14383529126644135, 0.4232136905193329, -0.08357616513967514, -0.08894852548837662, -0.009711340069770813, 0.6896188259124756, 0.19332794845104218, 0.22851690649986267, -0.12747842073440552, -0.06967920809984207, 0.014870606362819672, -0.18637748062610626, 0.16666239500045776, -0.16425788402557373, 0.018238002434372902, 0.2424573451280594, 0.2298920750617981, -0.08146613836288452, 0.16338840126991272, 0.13181065022945404, -0.03972949460148811, -0.0716896504163742, -0.0815817192196846, 0.032894033938646317, 0.12736481428146362, -0.15912757813930511], "sparse_embedding": null}, {"id": "bcbec230eddceaf3f7eef0799d1cf561ff6fa83a8ce09ed3233fe97112d5dac4", "content": "Recent researches indicate that Pre-trained Large Language Models (LLMs)\npossess cognitive constructs similar to those observed in humans, prompting\nresearchers to investigate the cognitive aspects of LLMs. This paper focuses on\nexplicit and implicit social bias, a distinctive two-level cognitive construct\nin psychology. It posits that individuals' explicit social bias, which is their\nconscious expression of bias in the statements, may differ from their implicit\nsocial bias, which represents their unconscious bias. We propose a two-stage\napproach and discover a parallel phenomenon in LLMs known as \"re-judge\ninconsistency\" in social bias. In the initial stage, the LLM is tasked with\nautomatically completing statements, potentially incorporating implicit social\nbias. However, in the subsequent stage, the same LLM re-judges the biased\nstatement generated by itself but contradicts it. We propose that this re-judge\ninconsistency can be similar to the inconsistency between human's unaware\nimplicit social bias and their aware explicit social bias. Experimental\ninvestigations on ChatGPT and GPT-4 concerning common gender biases examined in\npsychology corroborate the highly stable nature of the re-judge inconsistency.\nThis finding may suggest that diverse cognitive constructs emerge as LLMs'\ncapabilities strengthen. Consequently, leveraging psychological theories can\nprovide enhanced insights into the underlying mechanisms governing the\nexpressions of explicit and implicit constructs in LLMs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2308.12578v1", "title": "Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models", "content": "http://arxiv.org/pdf/2308.12578v1", "datetime": "2023-08-24 05:35:58", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Exciting insights into the cognitive constructs of Large Language Models (LLMs) have been uncovered! Research suggests that LLMs exhibit cognitive phenomena akin to human behavior, such as explicit and implicit social bias. Learn more about a new study delving into this intriguing parallel between human cognition and LLM behavior. Check out the research paper here: http://arxiv.org/abs/2308.12578v1 #AI #NLP #LLMs #Research #CognitiveScience \ud83e\udde0\ud83d\udd0d", "x": "\ud83e\udde0 New research suggests that Large Language Models exhibit cognitive constructs resembling humans, sparking interest in investigating their cognitive aspects. Learn about the parallel phenomenon of \"re-judge inconsistency\" in social bias discovered in LLMs. Check out the study here: http://arxiv.org/abs/2308.12578v1 #AI #NLP #LLM #Research #CognitiveBias", "source_id": "5d09f6c5493b8472478e6d9a3800f4d48d4f377782bae98b81c74a6a21036f6b", "page_number": 1}, "score": null, "embedding": [-0.15575706958770752, -0.17942138016223907, 0.07036533206701279, -0.0562882237136364, -0.07461787015199661, 0.14354971051216125, 0.11151731759309769, -0.039893101900815964, 0.4352213144302368, -0.3811330795288086, 0.10327562689781189, -0.1642521619796753, 0.16672664880752563, 0.35005658864974976, 0.32224616408348083, 0.37608644366264343, -0.3556382954120636, 0.06311918050050735, -0.1546836942434311, 0.17501387000083923, 0.2528790831565857, -0.5789933204650879, 0.11356036365032196, 0.22684787213802338, -0.0380929671227932, -0.11529450863599777, -0.03404558077454567, -0.14790226519107819, -0.12569992244243622, -1.4172685146331787, 0.24396023154258728, -0.0471603125333786, 0.10890770703554153, -0.09894245117902756, -0.1637846976518631, -0.2607521414756775, -0.25128743052482605, 0.24130472540855408, -0.07381076365709305, 0.365863561630249, -0.2086494266986847, 0.2513767182826996, 0.15938998758792877, -0.17874282598495483, -0.3897286653518677, -0.0524488128721714, -0.11428225785493851, 0.05126709118485451, -0.7798020243644714, -0.3716721832752228, -0.18633148074150085, -0.34887146949768066, 0.05427832528948784, 0.2914181053638458, -0.006619742605835199, 0.5327439904212952, 0.31720098853111267, 0.2952975630760193, 0.07158537209033966, 0.07532494515180588, 0.2643783986568451, 0.18890446424484253, -0.9634439945220947, 0.2838883697986603, 0.15944373607635498, 0.33435139060020447, -0.22447970509529114, -0.14547282457351685, 0.15401022136211395, -0.005546554923057556, -0.1385798156261444, 0.22436058521270752, 0.33906230330467224, -0.02293168380856514, 0.39401429891586304, 0.22316589951515198, 0.2414451390504837, -0.040532633662223816, 0.4192686080932617, 0.07924678921699524, -0.013221526518464088, 0.32701316475868225, -0.08853203803300858, -0.16287055611610413, -0.13946354389190674, -0.24017807841300964, -0.12706243991851807, -0.2169315665960312, -0.03935034200549126, 0.08708088845014572, -0.0925121009349823, -0.17340610921382904, -0.3694979250431061, 0.126257061958313, 0.13074249029159546, 0.13227763772010803, 0.19609598815441132, -0.051116231828927994, -0.16748695075511932, 0.5075595378875732, -0.11309632658958435, 0.06818947941064835, -0.03542417287826538, 0.1515488475561142, 0.2239721417427063, -0.06970001012086868, -0.10234557092189789, -0.4727022349834442, -0.20072931051254272, -0.08390184491872787, -0.1571643352508545, -0.12936051189899445, 0.13910986483097076, -0.1922423392534256, 0.1777014136314392, 0.19616274535655975, 0.44886136054992676, -0.0012352922931313515, 0.17985430359840393, 0.05496184155344963, -0.38389918208122253, 0.37929511070251465, -0.13485834002494812, -0.2929760217666626, -0.09401515126228333, -0.2983577251434326, -0.07006411999464035, 0.39400389790534973, 0.2043597251176834, -0.4459165632724762, 0.19480903446674347, -0.1221204623579979, -0.0959409773349762, 0.05369098111987114, 0.07216425985097885, 0.08144490420818329, -0.014914062805473804, -0.11869920045137405, 0.10690833628177643, 0.2480066567659378, 0.11738751083612442, 0.04683995991945267, 0.04285924881696701, -0.15485668182373047, -0.49809882044792175, 0.7247395515441895, -0.038102664053440094, -0.03634558990597725, -0.08572730422019958, 0.37567201256752014, -0.1454443633556366, 0.09161984920501709, 0.029826799407601357, -0.2641710638999939, 0.11834236979484558, 0.08371124416589737, -0.23077450692653656, 0.39890217781066895, -0.409859836101532, 0.08176194876432419, 0.14949028193950653, 0.01766633428633213, -0.1756676435470581, 0.4313138425350189, 0.015129881910979748, -0.07583972066640854, -0.06183263286948204, 0.02911045402288437, 0.40111082792282104, -0.21839798986911774, 0.3355601727962494, -0.1661793738603592, -0.16778232157230377, -0.01926981471478939, -0.17394691705703735, 0.09056293219327927, -0.5553890466690063, 0.18762868642807007, -0.3807595670223236, 0.21927839517593384, 0.36286839842796326, -0.1238381490111351, -0.05841255933046341, 0.2973192036151886, -0.153312548995018, -0.14813487231731415, -0.0040893531404435635, -0.39514100551605225, 0.25931769609451294, 0.2711004614830017, -0.11591538786888123, 0.37461671233177185, -0.22172050178050995, -0.03392357751727104, -0.18613681197166443, -0.39894363284111023, 0.03175404295325279, -0.1285390555858612, -0.10061097145080566, 0.09747897833585739, -0.19054889678955078, 0.04155745729804039, -0.08485285192728043, -0.13572145998477936, 0.159305140376091, 0.14232061803340912, 0.11687251180410385, -0.16795776784420013, 0.44921785593032837, 0.07430555671453476, -0.09936791658401489, 0.016997449100017548, 0.007428244221955538, -0.062833271920681, -0.007238470483571291, -0.005973245017230511, 0.21939751505851746, 0.012103407643735409, 0.10495023429393768, 0.05775654315948486, -0.16026939451694489, 0.010310961864888668, -0.2537006437778473, -1.1790850162506104, -0.42524367570877075, 0.25744444131851196, -0.021079640835523605, -0.13977764546871185, -0.2517283856868744, 0.23967145383358002, 0.1726188212633133, 0.44400838017463684, 0.5876356959342957, 0.046537432819604874, 0.13829055428504944, -0.23307166993618011, 0.0836649239063263, 0.10592825710773468, 0.03213629499077797, -0.24909420311450958, 0.0380568690598011, -0.22634060680866241, 0.23440657556056976, -0.28005343675613403, 0.20349113643169403, 0.2121543288230896, -0.6123970746994019, 0.3662642240524292, 0.05948474630713463, 0.7148155570030212, -0.11654376238584518, -0.02078821137547493, -0.0799407958984375, -0.12245550751686096, 0.3325875401496887, 0.1778201460838318, -0.6069460511207581, 0.479464054107666, 0.0356844998896122, -0.16028662025928497, -0.27549371123313904, 0.10274908691644669, -0.19748707115650177, -0.17134429514408112, -0.08953854441642761, -0.07695790380239487, -0.4714737832546234, -0.5354965329170227, 0.13206686079502106, 0.02485903538763523, -0.061864759773015976, -0.17217271029949188, 0.1488155871629715, 0.1594521701335907, -0.382936030626297, 0.35843029618263245, 0.024611372500658035, -0.27638381719589233, -0.07004501670598984, -0.7009021043777466, 0.08342961966991425, -0.27020499110221863, 0.14751288294792175, 0.17296963930130005, -0.09188710898160934, -0.3375842869281769, -0.13016805052757263, 0.3991073668003082, 0.18804799020290375, 0.10863000154495239, -0.004436233080923557, 0.028939589858055115, 0.04833243787288666, -0.03209134191274643, 0.8426681756973267, -0.02984999679028988, -0.06278999149799347, 0.07217307388782501, -0.133514404296875, -0.1253644824028015, -0.23249317705631256, -0.38969939947128296, 0.0705890953540802, 0.5403587222099304, -0.04744412750005722, 0.15675216913223267, 0.14932048320770264, 0.04996450990438461, -0.34452369809150696, 0.2273821085691452, -0.06119143217802048, 0.44886913895606995, -0.04236340522766113, -0.24132615327835083, 0.12758256494998932, -0.22463026642799377, -0.21768908202648163, 0.018117081373929977, -0.09214737266302109, -1.1167352199554443, -0.15194092690944672, -0.30132076144218445, 0.11475050449371338, 0.16022805869579315, 0.11882466822862625, 0.14605990052223206, -0.21267372369766235, -0.051860131323337555, 0.007192120887339115, -0.21826106309890747, 0.18125154078006744, 0.42847323417663574, 0.14740513265132904, -0.05218406766653061, -0.15896905958652496, 0.47430405020713806, -0.05566663295030594, 0.3179270327091217, -0.19054311513900757, -0.13179297745227814, 0.2567548155784607, 1.0202069282531738, -0.10142512619495392, 0.17496135830879211, 0.0629258081316948, 0.1497015506029129, -0.0453447550535202, 0.0042037866078317165, 0.2861701250076294, -0.17270979285240173, 0.30792951583862305, 0.67890864610672, -0.12999899685382843, -0.12038616836071014, 0.4478939473628998, -0.506730854511261, -0.05361507087945938, 0.38265928626060486, -0.062093354761600494, 0.16764546930789948, 0.16102010011672974, -0.01029830053448677, -0.017154211178421974, 0.7858757972717285, 0.14897139370441437, 0.0023732248228043318, -0.331947922706604, 0.2165278196334839, -0.08037286251783371, -0.2467271238565445, -0.015032188035547733, -0.09311690181493759, 0.02010653167963028, 0.33742791414260864, -0.05553080886602402, -0.15964505076408386, 0.015588750131428242, 0.0423235148191452, -0.224952831864357, 0.011939529329538345, 0.08745725452899933, 0.06148542836308479, 0.33171096444129944, -0.06437047570943832], "sparse_embedding": null}, {"id": "4a8e937da1200627830870c07497131fe7c84309187edbcf18961a0425853aa2", "content": "Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2306.05685v4", "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "content": "http://arxiv.org/pdf/2306.05685v4", "datetime": "2023-12-24 02:01:34", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Exciting news in the world of AI and large language models (LLMs)! Researchers have delved into using strong LLMs as judges to evaluate chat assistants on open-ended questions, addressing the challenges of measuring human preferences. The study explores the limitations and biases of LLM-as-a-judge and proposes solutions to enhance evaluation accuracy. \n\nResults show that powerful LLM judges like GPT-4 can closely match human preferences, achieving over 80% agreement. This approach offers a scalable and explainable method to approximate human preferences efficiently. The study introduces two benchmarks, MT-bench, and Chatbot Arena, to verify agreement between LLM judges and human preferences.\n\nAccess the MT-bench questions, expert votes, and conversations at: [GitHub - FastChat](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge). \n\nCurious to learn more? Dive into the research here: [Research Paper](http://arxiv.org/abs/2306.05685v4)\n\n#AI #LLM #Research #NLP #ChatAssistants #ArtificialIntelligence", "x": "Exciting new research on using large language models as judges to evaluate chat assistants reveals promising results! Strong LLM judges like GPT-4 can match human preferences with over 80% agreement, making them a scalable and cost-effective alternative. Learn more about this innovative approach and access the benchmarks at: http://arxiv.org/abs/2306.05685v4 #AI #NLP #LLMs #TechResearch", "source_id": "4ad471bb30620aa4404d0d47716a8427e90dba00fbf9bdaf3c402ffde15b32b5", "page_number": 1}, "score": null, "embedding": [-0.5396351218223572, -0.0025531179271638393, -0.14261594414710999, 0.047339942306280136, -0.18320362269878387, 0.02089434489607811, 0.07410305738449097, -0.02108154632151127, 0.23568125069141388, -0.07739266008138657, 0.020966261625289917, -0.006832391023635864, 0.2730940580368042, 0.42779433727264404, 0.4339506924152374, 0.3298308253288269, -0.07945801317691803, -0.12488284707069397, -0.06396008282899857, 0.18778125941753387, 0.15806080400943756, -0.30934789776802063, -0.026232510805130005, -0.12192051857709885, 0.02633843757212162, -0.09568172693252563, -0.3121613562107086, -0.22091741859912872, -0.1831444948911667, -1.3661925792694092, 0.41016456484794617, -0.02512640878558159, 0.3983452320098877, -0.08279824256896973, -0.4341476559638977, 0.017772192135453224, -0.07089763134717941, -0.15830640494823456, -0.2208278328180313, 0.16964954137802124, 0.09006399661302567, 0.0642235055565834, 0.0016384932678192854, -0.18401561677455902, -0.004099344369024038, -0.4475952088832855, -0.021333126351237297, 0.021829670295119286, -0.7864834070205688, 0.0915675088763237, -0.008619324304163456, -0.47649669647216797, 0.17712277173995972, 0.14043352007865906, 0.07970345765352249, 0.08428582549095154, 0.06798219680786133, 0.2357092946767807, -0.15754687786102295, 0.022739307954907417, 0.1911499798297882, 0.16252551972866058, -0.904437780380249, 0.4397619664669037, -0.16873657703399658, 0.2813097834587097, -0.2700289487838745, 0.1725052446126938, -0.1338122934103012, 0.2140974998474121, 0.14601972699165344, 0.19990113377571106, 0.385978639125824, 0.1886509507894516, 0.23499569296836853, 0.3040497601032257, -0.02430863492190838, -0.12891677021980286, 0.2687360942363739, 0.13335080444812775, -0.1903945952653885, -0.04864608123898506, -0.23879404366016388, -0.44986075162887573, -0.13136032223701477, -0.11551667749881744, -0.04553258791565895, 0.06501144170761108, -0.11906174570322037, -0.047353848814964294, -0.014591863378882408, 0.01898038201034069, -0.08849404752254486, 0.3127151131629944, -0.05655296519398689, -0.028624271973967552, 0.15036238729953766, 0.026300881057977676, -0.37425124645233154, 0.7816358804702759, -0.10267859697341919, 0.30563002824783325, -0.3156471848487854, -0.1078701913356781, 0.316828191280365, -0.02969609946012497, -0.1377311646938324, -0.3580259084701538, -0.012561440467834473, -0.06775067746639252, 0.033131666481494904, 0.21465963125228882, 0.1068909540772438, 0.015382091514766216, -0.07148400694131851, 0.2569217383861542, 0.5142605304718018, 0.006832819432020187, 0.02418072335422039, -0.1251283437013626, -0.34626662731170654, 0.21163222193717957, -0.0965125784277916, -0.17427003383636475, 0.12294752150774002, -0.05633186548948288, 0.3949272036552429, 0.30021950602531433, 0.19077645242214203, -0.005396123509854078, 0.331635445356369, -0.03188156336545944, -0.3880551755428314, -0.02537098340690136, 0.15926243364810944, -0.08640769124031067, -0.053710658103227615, -0.039019253104925156, 0.13636218011379242, 0.10641159862279892, -0.07974382489919662, -0.32538679242134094, 0.1701115518808365, -0.1260376125574112, -0.44170114398002625, 0.5802148580551147, -0.10482063889503479, -0.018874015659093857, -0.3863258361816406, -0.054418161511421204, 0.2902334928512573, 0.22177496552467346, 0.0713912844657898, -0.18127429485321045, 0.2250903844833374, 0.1679833084344864, 0.14754973351955414, 0.195296049118042, -0.28494319319725037, -0.013825597241520882, 0.25197649002075195, 0.07126083225011826, -0.05433937907218933, 0.48830488324165344, -0.10367192327976227, -0.5493035316467285, -0.33294445276260376, -0.18587656319141388, 0.1317824423313141, -0.043182067573070526, 0.2642911672592163, -0.01228850893676281, -0.27299490571022034, 0.04926031082868576, 0.009341663680970669, -0.016729658469557762, -0.6934711933135986, -0.03640357032418251, -0.1269753873348236, 0.25268077850341797, 0.25357288122177124, -0.09402970224618912, -0.10867708921432495, 0.11879675835371017, -0.34088802337646484, -0.2963959872722626, 0.15660959482192993, -0.43256479501724243, 0.13351093232631683, -0.07636109739542007, -0.1295764148235321, 0.06461020559072495, -0.06583532691001892, 0.16116346418857574, -0.10856470465660095, -0.3237098157405853, -0.045237284153699875, -0.14629407227039337, 0.04241572692990303, 0.03412095457315445, -0.023900998756289482, 0.3995521664619446, -0.20904387533664703, -0.031988725066185, 0.2090705931186676, -0.08941373974084854, -0.0018376221414655447, 0.002327520167455077, 0.6411479115486145, 0.34678173065185547, -0.3401980400085449, 0.22285957634449005, 0.004909342620521784, 0.03434697911143303, -0.09761439263820648, -0.22760625183582306, 0.384561687707901, 0.22832995653152466, -0.002425064565613866, 0.1131177470088005, 0.1272592842578888, 0.3092883825302124, -0.08401035517454147, -1.1947197914123535, -0.04216521605849266, -0.011913538910448551, 0.017360791563987732, 0.03695589303970337, -0.46146148443222046, 0.10189713537693024, -0.054733533412218094, 0.37733444571495056, 0.4880606532096863, 0.2597227990627289, -0.08650388568639755, -0.26126888394355774, 0.36048609018325806, -0.04499860107898712, 0.0817132219672203, -0.4157645106315613, 0.41350239515304565, -0.2790759801864624, 0.11179506778717041, -0.11622076481580734, 0.224903404712677, 0.0289472509175539, -0.6971770524978638, 0.1283155232667923, -0.12252318859100342, 0.8241205215454102, -0.10931162536144257, -0.06759895384311676, 0.05882863327860832, 0.1524379998445511, 0.26907333731651306, -0.019881879910826683, -0.5300318598747253, 0.3359348177909851, 0.31869590282440186, -0.012439310550689697, 0.020058656111359596, 0.10394672304391861, -0.03811481222510338, -0.3137873113155365, -0.017414802685379982, 0.04530847817659378, -0.7781225442886353, -0.36987486481666565, 0.11184485256671906, 0.1781572699546814, -0.20465904474258423, -0.428758442401886, 0.13513323664665222, -0.019041094928979874, -0.12291979044675827, 0.2306651771068573, 0.09301105886697769, -0.2345372587442398, -0.17519159615039825, -0.5888349413871765, -0.07305652648210526, -0.09178589284420013, 0.2028333693742752, 0.007124839350581169, 0.012930777855217457, -0.2731195390224457, -0.08137068897485733, 0.2891727685928345, -0.13684211671352386, 0.015461642295122147, -0.08701221644878387, 0.04378632456064224, -0.13070048391819, -0.11229877918958664, 0.42563292384147644, -0.08283504843711853, 0.21538494527339935, 0.18130096793174744, 0.2589224576950073, 0.03987608850002289, -0.09102402627468109, -0.21030545234680176, -0.07697408646345139, 0.5009990334510803, 0.22125717997550964, 0.08423937857151031, 0.03558329865336418, 0.1481507420539856, -0.07600772380828857, 0.22698956727981567, 0.058382317423820496, 0.13691124320030212, 0.25156015157699585, -0.004021313041448593, 0.21337901055812836, -0.28325343132019043, -0.24046540260314941, -0.17292897403240204, 0.042881373316049576, -1.0066134929656982, 0.12988844513893127, -0.2891775965690613, 0.2974316477775574, -0.10740463435649872, 0.07314673066139221, 0.037008728832006454, -0.1550813764333725, -0.008635916747152805, 0.022807005792856216, 0.13776235282421112, 0.49699801206588745, 0.14288951456546783, -0.13607075810432434, 0.15938206017017365, -0.20963571965694427, 0.35653018951416016, -0.10900983959436417, 0.3093988597393036, -0.3816744387149811, 0.06588367372751236, 0.043558429926633835, 0.9237898588180542, -0.030063483864068985, 0.12961620092391968, 0.3873509168624878, -0.15638920664787292, -0.2112959772348404, 0.11106164008378983, -0.07317304611206055, -0.30650144815444946, -0.04590218886733055, 0.7943397164344788, -0.23493239283561707, -0.054088447242975235, 0.44290968775749207, -0.19551968574523926, -0.09768165647983551, 0.15267659723758698, 0.049087464809417725, 0.4436163306236267, 0.02697361446917057, 0.016595419496297836, -0.11400335282087326, 0.6093105673789978, 0.41509491205215454, -0.0580458827316761, -0.19971050322055817, 0.07402731478214264, -0.08795373886823654, -0.17144501209259033, -0.12931585311889648, -0.2664506435394287, -0.026090750470757484, 0.1519625186920166, 0.008837930858135223, 0.2373204082250595, 0.10319854319095612, -0.1692730188369751, -0.21144138276576996, 0.11512144654989243, 0.15688163042068481, 0.10432440787553787, 0.025209976360201836, -0.008030477911233902], "sparse_embedding": null}, {"id": "237ff27acb132247fa6ce0e92ff5489c49b62ec7834673f8ba035850a131b670", "content": "The advent of Large Language Models (LLMs) has significantly transformed the\nAI landscape, enhancing machine learning and AI capabilities. Factuality issue\nis a critical concern for LLMs, as they may generate factually incorrect\nresponses. In this paper, we propose GraphEval to evaluate an LLM's performance\nusing a substantially large test dataset. Specifically, the test dataset is\nretrieved from a large knowledge graph with more than 10 million facts without\nexpensive human efforts. Unlike conventional methods that evaluate LLMs based\non generated responses, GraphEval streamlines the evaluation process by\ncreating a judge model to estimate the correctness of the answers given by the\nLLM. Our experiments demonstrate that the judge model's factuality assessment\naligns closely with the correctness of the LLM's generated outputs, while also\nsubstantially reducing evaluation costs. Besides, our findings offer valuable\ninsights into LLM performance across different metrics and highlight the\npotential for future improvements in ensuring the factual integrity of LLM\noutputs. The code is publicly available at https://github.com/xz-liu/GraphEval.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2404.00942v1", "title": "Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs", "content": "http://arxiv.org/pdf/2404.00942v1", "datetime": "2024-04-01 06:01:17", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Exciting advancements in the world of AI and Large Language Models (LLMs)! \ud83c\udf10\n\nThe rise of LLMs has revolutionized AI capabilities, but the issue of factuality remains a crucial concern. How can we ensure that LLMs provide accurate responses? \ud83e\udd16\n\nIntroducing GraphEval - a novel approach to evaluating LLM performance using a vast test dataset sourced from a knowledge graph with over 10 million facts. \ud83d\udcca By leveraging a judge model, GraphEval estimates the correctness of LLM outputs, streamlining evaluation processes and reducing costs significantly.\n\nOur experiments have shown that the judge model's factuality assessment closely aligns with the accuracy of LLM-generated responses, offering valuable insights into performance metrics and paving the way for future enhancements in ensuring factual integrity. \ud83d\udcc8\n\nCurious to learn more? Dive into the details and explore the code at: https://github.com/xz-liu/GraphEval \ud83d\udcdd\n\nRead the full paper here: http://arxiv.org/abs/2404.00942v1 \ud83d\udcd1\n\n#AI #LLMs #GraphEval #MachineLearning #ArtificialIntelligence #TechInnovation #ResearchPaper #GitHub\n\nLet's continue pushing the boundaries of AI together! \ud83d\udca1\ud83d\udd0d", "x": "\ud83c\udf1f Exciting research on evaluating Large Language Models (LLMs) using GraphEval for factuality assessment without costly human efforts! Find out how this approach enhances LLM performance and reduces evaluation costs. Read the paper at: http://arxiv.org/abs/2404.00942v1 #AI #NLP #LLM #GraphEval \ud83e\udd16\ud83d\udcca", "source_id": "fc0a43682fc1433b5e5d7603362f59c5a7b43757caea8b06fb338a3e841e746f", "page_number": 1}, "score": null, "embedding": [-0.3477201759815216, -0.08002245426177979, 0.19361156225204468, 0.029071109369397163, 0.2957710325717926, 0.17501331865787506, -0.3082415759563446, 0.05605291202664375, 0.2101515680551529, -0.09608670324087143, 0.08576538413763046, -0.11872385442256927, 0.1653461754322052, 0.34783509373664856, 0.25896865129470825, 0.26215940713882446, -0.10623787343502045, 0.10090075433254242, -0.22720980644226074, -0.03165185824036598, 0.3910520374774933, -0.18655648827552795, 0.11621727049350739, -0.0632132887840271, -0.04599536582827568, 0.03534811735153198, -0.170395627617836, -0.13461518287658691, -0.22961416840553284, -1.5672112703323364, 0.1348535418510437, -0.039958931505680084, 0.4626927673816681, -0.09280479699373245, -0.017247680574655533, 0.07720589637756348, -0.0923105999827385, -0.014064163900911808, -0.10749676078557968, 0.09850668907165527, 0.1384209841489792, 0.009719660505652428, 0.06776503473520279, -0.12284216284751892, 0.12282568961381912, -0.15511395037174225, 0.01925373263657093, 0.10271298885345459, -0.6946460008621216, -0.06682231277227402, -0.006297206040471792, -0.40334266424179077, 0.05146895721554756, 0.19446809589862823, 0.1480250060558319, 0.06722287088632584, 0.2430538535118103, 0.25906845927238464, 0.28799891471862793, 0.016041817143559456, 0.20832648873329163, 0.3066207766532898, -1.0029171705245972, 0.19472429156303406, -0.0266653411090374, 0.12129853665828705, -0.07401126623153687, -0.04864181578159332, 0.2214476615190506, 0.19754678010940552, -0.09557104855775833, -0.022977415472269058, 0.3495759665966034, 0.25123828649520874, 0.33529871702194214, 0.20669662952423096, 0.10257239639759064, 0.006806806195527315, 0.27749431133270264, -0.15892352163791656, -0.055553946644067764, -0.11478105187416077, -0.22666284441947937, -0.27310267090797424, -0.12590335309505463, -0.09856045246124268, -0.2509516179561615, -0.0183450598269701, 0.1499665230512619, -0.07345674186944962, 0.22067037224769592, -0.11110028624534607, -0.3611534535884857, 0.3521663248538971, 0.00634576054289937, 0.033841121941804886, 0.17369864881038666, -0.020115122199058533, -0.19886331260204315, 0.5955632328987122, -0.08125528693199158, 0.029305048286914825, -0.132957324385643, -0.05800529196858406, 0.17706298828125, -0.19650167226791382, -0.06357395648956299, -0.19213049113750458, -0.1215134933590889, -0.1746738702058792, 0.038603514432907104, -0.05922544375061989, -0.04687686637043953, 0.0872294083237648, 0.03953062742948532, -0.05303921177983284, 0.6181227564811707, 0.09235858917236328, -0.26724594831466675, -0.08551795035600662, -0.3817705512046814, 0.28231489658355713, -0.10408229380846024, -0.20244723558425903, 0.0976797491312027, -0.20547932386398315, 0.17156030237674713, 0.4083568751811981, 0.041942156851291656, -0.11997918039560318, 0.11339545994997025, -0.16306769847869873, -0.4355565309524536, -0.10937188565731049, 0.03889883682131767, -0.20783647894859314, 0.16616152226924896, -0.07973619550466537, 0.022298496216535568, 0.4339405298233032, -0.04228706657886505, 0.12797369062900543, 0.0461641401052475, -0.002065952168777585, -0.5665072798728943, 0.6203111410140991, -0.08696947991847992, 0.0928918719291687, -0.3202059268951416, -0.21143434941768646, 0.1582764834165573, 0.30817127227783203, -0.11597595363855362, -0.2697368860244751, 0.28983908891677856, 0.26173093914985657, 0.012073109857738018, 0.1719398945569992, -0.4346873164176941, -0.06962644308805466, 0.06841906905174255, 0.02356235682964325, -0.2515457272529602, 0.7654232382774353, -0.2686113119125366, -0.1627451777458191, -0.2980770766735077, 0.012793605215847492, 0.26908767223358154, -0.021452246233820915, 0.20858849585056305, 0.09840040653944016, -0.25166770815849304, 0.008058304898440838, -0.23751585185527802, -0.00010590004967525601, -0.6422778367996216, -0.08350487053394318, -0.0950690433382988, 0.0315096452832222, 0.14738893508911133, -0.14950712025165558, -0.2102523297071457, 0.2643028795719147, -0.004618063569068909, -0.04285771772265434, -0.15465939044952393, -0.1799519807100296, 0.10488550364971161, -0.019541267305612564, -0.09447988122701645, 0.3391149640083313, -0.13345938920974731, 0.08755052834749222, -0.20423267781734467, -0.30981215834617615, 0.05762156844139099, 0.026689883321523666, 0.07350590825080872, -0.1496419459581375, -0.0664927288889885, -0.07397681474685669, -0.06707767397165298, -0.14758604764938354, -0.0641498938202858, 0.0038209024351090193, -0.1457439512014389, -0.033505283296108246, 0.22056375443935394, 0.343414306640625, -0.3115737736225128, -0.18876533210277557, 0.16181327402591705, 0.0932854413986206, -0.22416804730892181, 0.0006023125606589019, 0.3464664816856384, 0.13036586344242096, -0.3238341808319092, 0.3695313334465027, 0.1729854792356491, 0.14179271459579468, -0.29776352643966675, -1.2692676782608032, -0.2722828984260559, 0.22925877571105957, 0.18669553101062775, 0.37272244691848755, -0.33847102522850037, -0.11533615738153458, 0.021859265863895416, 0.35955101251602173, 0.46281397342681885, 0.08838869631290436, 0.11340860277414322, -0.28113946318626404, -0.0017154295928776264, 0.01764729432761669, -0.07850749790668488, -0.18632082641124725, 0.24004685878753662, -0.2799246907234192, 0.1952601969242096, -0.3346313536167145, 0.024004559963941574, 0.11965342611074448, -0.7228827476501465, 0.06172354146838188, -0.13838927447795868, 0.7951151728630066, -0.30857232213020325, 0.025477658957242966, -0.05003490671515465, -0.138965904712677, 0.26464611291885376, -0.10758361220359802, -0.4257214665412903, 0.5905975103378296, 0.141824871301651, -0.026609323918819427, 0.2874692678451538, 0.006680043414235115, 0.003932683728635311, -0.22096332907676697, -0.14318110048770905, -0.005991817452013493, -0.6379144191741943, -0.31091025471687317, 0.340696781873703, 0.23764416575431824, 0.06724317371845245, -0.48602670431137085, 0.4096249043941498, 0.0947679653763771, 0.06790687143802643, 0.3236231803894043, 0.062135715037584305, 0.09954622387886047, -0.18695126473903656, -0.7039698958396912, 0.16425779461860657, -0.02930585853755474, -0.12030276656150818, 0.3724440038204193, -0.15117202699184418, -0.22830107808113098, -0.18942326307296753, 0.39068520069122314, -0.24710412323474884, -0.14173299074172974, -0.10836179554462433, 0.13580960035324097, -0.026815034449100494, -0.13054315745830536, 0.5738265514373779, 0.17342831194400787, 0.13353145122528076, 0.26867008209228516, 0.17522689700126648, 0.1203179657459259, -0.35688456892967224, -0.07544425874948502, -0.10130101442337036, 0.4653002619743347, 0.1668849140405655, 0.023083049803972244, 0.1456950157880783, 0.2480161339044571, 0.08199067413806915, 0.14575614035129547, 0.2101345956325531, 0.22061879932880402, 0.2088640034198761, -0.013371477834880352, -0.006959917489439249, -0.3255157470703125, -0.1467438042163849, 0.014460094273090363, 0.1004665195941925, -1.090407371520996, -0.015421323478221893, -0.12253203988075256, 0.33278390765190125, -0.19187039136886597, -0.19903963804244995, 0.13659557700157166, -0.033050086349248886, 0.09849028289318085, -0.0030362235847860575, -0.13317951560020447, 0.39325687289237976, 0.17722859978675842, -0.19098591804504395, -0.053605467081069946, -0.08700273185968399, 0.2120596021413803, -0.15577559173107147, 0.2644028663635254, -0.2206215262413025, 0.10424204915761948, 0.24421772360801697, 0.9814416766166687, 0.057817865163087845, -0.03595545142889023, 0.38989996910095215, -0.06620471924543381, -0.08609841018915176, 0.04256974160671234, -0.05802353471517563, -0.034989360719919205, 0.03740788251161575, 0.5361189842224121, 0.1613890677690506, 0.11560515314340591, 0.5685359835624695, -0.4069679081439972, -0.17128030955791473, 0.18343745172023773, 0.009907467290759087, 0.4283923804759979, -0.013784334063529968, 0.07695643603801727, -0.027426326647400856, 0.6919971704483032, 0.2565353810787201, 0.320281982421875, -0.3576819896697998, -0.02543647773563862, -0.03652932867407799, -0.15655741095542908, -0.1617555171251297, -0.07166030257940292, 0.17943069338798523, 0.16709090769290924, 0.09947508573532104, 0.13144069910049438, 0.013518760912120342, 0.0016778565477579832, -0.3563539683818817, -0.10946418344974518, -0.32351624965667725, 0.057003382593393326, -0.10091519355773926, -0.33230793476104736], "sparse_embedding": null}, {"id": "cfbe349f56e950ad4fde2cc529897893012f1d0466fcc1645f8530572efc04fd", "content": "Leveraging Large Language Models (LLMs) as judges for evaluating the\nperformance of LLMs has recently garnered attention. Nonetheless, this type of\napproach concurrently introduces potential biases from LLMs, raising concerns\nabout the reliability of the evaluation results. To mitigate this issue, we\npropose and study two versions of many-shot in-context prompts, Reinforced and\nUnsupervised ICL, for helping GPT-4o-as-a-Judge in single answer grading. Based\non the designed prompts, we investigate the impact of scaling the number of\nin-context examples on the agreement and quality of the evaluation.\nFurthermore, we first reveal the symbol bias in GPT-4o-as-a-Judge for pairwise\ncomparison and then propose a simple yet effective approach to mitigate it.\nExperimental results show that advanced long-context LLMs, such as GPT-4o,\nperform better in the many-shot regime than in the zero-shot regime. Meanwhile,\nthe experimental results further verify the effectiveness of the symbol bias\nmitigation approach.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2406.11629v1", "title": "Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!", "content": "http://arxiv.org/pdf/2406.11629v1", "datetime": "2024-06-17 15:11:58", "query": "LLMs as judges", "linkedin": "\ud83d\ude80 Exciting advancements in the world of Large Language Models (LLMs)! \ud83e\udd16 Our latest study explores the use of many-shot in-context prompts, Reinforced and Unsupervised ICL, to enhance the evaluation process of LLMs like GPT-4o-as-a-Judge in single answer grading. Find out how scaling the number of in-context examples impacts evaluation quality and agreement. Discover the revealed symbol bias in GPT-4o-as-a-Judge and a novel approach to address it effectively. \ud83d\udcca\ud83d\udd0d\n\nRead more about our research and experimental results here: [Link to the study](http://arxiv.org/abs/2406.11629v1) \n\n#AI #NLP #LLMs #Research #GPT4o #TechInnovation #ArtificialIntelligence #LanguageModels", "x": "\ud83d\ude80 Exciting new research on leveraging Large Language Models (LLMs) as judges for evaluating LLM performance! Learn about the proposed many-shot in-context prompts for GPT-4o-as-a-Judge in single-answer grading and how to mitigate potential biases. Check out the study at: http://arxiv.org/abs/2406.11629v1 #AI #NLP #LLMs #Research #GPT4o", "source_id": "f3f10dd532472230e3b14cee5e607defba9591babce325105c7850a820eabd86", "page_number": 1}, "score": null, "embedding": [-0.41506558656692505, 0.0734608843922615, 0.07602249085903168, -0.20167027413845062, -0.16598591208457947, 0.19533765316009521, -0.15673856437206268, 0.13574931025505066, 0.28225040435791016, -0.4404977858066559, 0.004489163868129253, -0.10054841637611389, 0.11961646378040314, 0.20275476574897766, 0.15844926238059998, 0.15957915782928467, -0.11668744683265686, -0.00779544934630394, -0.3428853750228882, 0.12299800664186478, 0.29309356212615967, -0.3668577969074249, 0.2816239595413208, -0.17531244456768036, 0.2757852375507355, -0.04233643412590027, -0.0032115967478603125, -0.1778097301721573, -0.14175714552402496, -1.7442809343338013, 0.038230784237384796, 0.07085396349430084, 0.16967295110225677, -0.10516934841871262, -0.37821778655052185, 0.07471784949302673, -0.12904663383960724, 0.21289092302322388, -0.14770475029945374, 0.13289012014865875, 0.20707689225673676, 0.32584914565086365, 0.04777370020747185, -0.12162632495164871, -0.22338905930519104, -0.4635271728038788, -0.2534343898296356, -0.11687920987606049, -0.7713304162025452, 0.013205425813794136, -0.11956576257944107, -0.328759104013443, 0.18875710666179657, 0.22970838844776154, -0.09366206079721451, 0.09540403634309769, 0.16463226079940796, 0.23472197353839874, 0.16020676493644714, 0.022742990404367447, 0.2741031050682068, 0.2807765007019043, -1.009401798248291, 0.394271582365036, 0.2086673229932785, 0.20169270038604736, -0.031962353736162186, 0.11232449114322662, -0.055924057960510254, 0.3334254324436188, -0.04121408611536026, 0.11572086066007614, 0.19174225628376007, 0.24449020624160767, 0.10131271928548813, 0.2915440499782562, 0.10164187103509903, -0.13791786134243011, 0.14848800003528595, -0.01132518332451582, 0.07463065534830093, -0.17751093208789825, -0.1565747857093811, -0.4161562919616699, -0.06799177825450897, 0.043837256729602814, -0.25709500908851624, -0.03513270244002342, 0.16866113245487213, -0.09714652597904205, 0.06069616228342056, -0.06750927120447159, -0.07073793560266495, 0.12171847373247147, -0.033162251114845276, 0.004699427168816328, 0.015584331005811691, -0.08911007642745972, -0.31104224920272827, 0.5890218615531921, -0.1720370352268219, 0.1565520167350769, -0.10688582062721252, 0.04384160414338112, 0.36001119017601013, 0.04593590646982193, 0.21537066996097565, -0.20995086431503296, -0.20390614867210388, -0.060733404010534286, -0.027508556842803955, 0.003557506250217557, 0.027851713821291924, -0.10483163595199585, -0.11467909812927246, 0.1667778491973877, 0.4378006160259247, -0.12282713502645493, 0.21288439631462097, -0.07105947285890579, -0.4739755094051361, 0.2491665780544281, -0.20409022271633148, -0.15085677802562714, -0.09137734770774841, -0.060572925955057144, 0.08539777994155884, 0.4876424968242645, 0.09736716002225876, 0.0006210575229488313, -0.10977035015821457, -0.16042640805244446, -0.22236664593219757, -0.16551856696605682, 0.08054889738559723, 0.1006818413734436, 0.05721018463373184, -0.00805360171943903, 0.20045387744903564, 0.2832176685333252, -0.0009431490325368941, -0.07605645060539246, 0.29530829191207886, -0.11733946204185486, -0.23329000174999237, 0.6827785968780518, -0.222499817609787, -0.024622414261102676, -0.20584292709827423, -0.18458260595798492, 0.1568913608789444, 0.2270144820213318, -0.2761000096797943, -0.2044934779405594, 0.10666599869728088, 0.24129080772399902, -0.07713472098112106, 0.33690792322158813, -0.24694843590259552, -0.011325426399707794, -0.2686462104320526, -0.028342461213469505, -0.16567911207675934, 0.632685124874115, -0.11797571182250977, -0.30140891671180725, -0.19407474994659424, -0.09715647250413895, 0.17454731464385986, -0.2321741282939911, 0.206815704703331, -0.07399452477693558, -0.07816271483898163, -0.09244336932897568, 0.022208308801054955, 0.10271549969911575, -0.5570889711380005, -0.2163023203611374, -0.12287014722824097, 0.18469101190567017, 0.1958221048116684, -0.23249328136444092, -0.08033932000398636, 0.2140597701072693, -0.020668895915150642, -0.1557614505290985, -0.005943914409726858, -0.1572834551334381, 0.26000604033470154, -0.032351184636354446, -0.3330686390399933, 0.1632755994796753, -0.03259532153606415, 0.18008440732955933, -0.04353642091155052, -0.31632277369499207, -0.09016449749469757, -0.20086248219013214, -0.02262764610350132, -0.08060584962368011, 0.11442896723747253, 0.06351547688245773, 0.06816504150629044, -0.23020808398723602, 0.2914726436138153, -0.037580981850624084, 0.06403124332427979, -0.12183848768472672, 0.20537765324115753, 0.21189945936203003, -0.307197242975235, 0.08513713628053665, 0.36283525824546814, 0.11985629796981812, -0.04754076525568962, -0.19344183802604675, 0.2660231292247772, 0.2448902577161789, -0.1428256630897522, 0.19615784287452698, 0.019588442519307137, 0.13824599981307983, -0.3301478624343872, -1.3237552642822266, -0.16277769207954407, 0.013190982863307, -0.06522048264741898, 0.39036211371421814, -0.38934826850891113, 0.06547453999519348, 0.008877819404006004, 0.4922974407672882, 0.28582486510276794, 0.15200741589069366, 0.046419963240623474, -0.30832356214523315, 0.1270424723625183, -0.015638424083590508, 0.08752193301916122, -0.3102758526802063, 0.22869904339313507, -0.2454124242067337, 0.2894032597541809, -0.12928257882595062, 0.27940335869789124, 0.1695682406425476, -0.5011882781982422, 0.059592269361019135, -0.22714969515800476, 0.7868274450302124, -0.17464697360992432, -0.1414857655763626, -0.03648754954338074, 0.23507487773895264, 0.21898436546325684, 0.10082978755235672, -0.2103872150182724, 0.4903186857700348, 0.10261784493923187, 0.12319836765527725, 0.24424032866954803, 0.07107704877853394, 0.02342035435140133, -0.06774108856916428, -0.05873936042189598, 0.12808331847190857, -0.4991407096385956, -0.17156344652175903, 0.1746872365474701, 0.1280192881822586, 0.11920209228992462, -0.26879334449768066, 0.2689492702484131, 0.022521499544382095, -0.20507380366325378, 0.1607012301683426, -0.06699392199516296, -0.029606608673930168, 0.001537530915811658, -0.5072162747383118, 0.12167230993509293, 0.07752986252307892, 0.017648596316576004, 0.19870305061340332, -0.236281618475914, -0.12966413795948029, -0.0844731405377388, 0.2536431849002838, -0.08007600158452988, -0.11533716320991516, -0.17685340344905853, 0.03807999938726425, -0.01059155073016882, 0.029652366414666176, 0.5355439186096191, -0.08427529036998749, -0.019657909870147705, 0.16616596281528473, 0.14404690265655518, 0.046500567346811295, -0.18863599002361298, -0.3469609022140503, -0.14014630019664764, 0.5269770622253418, 0.2614275813102722, 0.04247792437672615, -0.036533333361148834, -0.028851483017206192, 0.20129501819610596, 0.17231597006320953, 0.15266603231430054, 0.06105935946106911, 0.04689198359847069, -0.02833366207778454, 0.15313772857189178, -0.21109019219875336, 0.04487192630767822, -0.033097513020038605, -0.10073777288198471, -1.2184594869613647, 0.08801642060279846, -0.010378936305642128, 0.19757205247879028, -0.032335471361875534, 0.12791737914085388, 0.16454486548900604, -0.3364039361476898, -0.1396954208612442, 0.07994741201400757, -0.21109512448310852, 0.41603150963783264, 0.24613353610038757, -0.0957079827785492, -0.01628006063401699, -0.16510009765625, 0.22706839442253113, -0.3585340976715088, 0.35723626613616943, -0.2693098485469818, 0.1061137244105339, 0.17976486682891846, 1.0694818496704102, -0.11938954144716263, -0.005663024727255106, 0.14673833549022675, 0.14508885145187378, 0.012064516544342041, 0.09326352924108505, 0.17680124938488007, -0.043379779905080795, 0.10875514149665833, 0.756769061088562, -0.2577752470970154, -0.04599285498261452, 0.5241352319717407, -0.2279704511165619, -0.031860508024692535, 0.29718929529190063, 0.1361224353313446, 0.4419286847114563, 0.04553255811333656, 0.0749196857213974, -0.24809564650058746, 0.6128638982772827, -0.02780928649008274, 0.15860584378242493, -0.23926404118537903, -0.01638283208012581, -0.06214631348848343, -0.19552943110466003, 0.128158837556839, -0.1832299381494522, -0.21844205260276794, 0.13542219996452332, 0.19369137287139893, -0.01349356397986412, 0.08219308406114578, -0.05519501864910126, -0.07054176181554794, 0.02954920195043087, -0.10328271985054016, 0.01304924301803112, 0.21535362303256989, -0.09707385301589966], "sparse_embedding": null}, {"id": "f7f262cce54cf8b35012c5328fd7f800707f80925f207c2ea4fbf59f76120000", "content": "Large language models (LLMs) can label data faster and cheaper than humans\nfor various NLP tasks. Despite their prowess, LLMs may fall short in\nunderstanding of complex, sociocultural, or domain-specific context,\npotentially leading to incorrect annotations. Therefore, we advocate a\ncollaborative approach where humans and LLMs work together to produce reliable\nand high-quality labels. We present MEGAnno+, a human-LLM collaborative\nannotation system that offers effective LLM agent and annotation management,\nconvenient and robust LLM annotation, and exploratory verification of LLM\nlabels by humans.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.18050v1", "title": "MEGAnno+: A Human-LLM Collaborative Annotation System", "content": "http://arxiv.org/pdf/2402.18050v1", "datetime": "2024-02-28 04:58:07", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting News in AI and NLP! \ud83d\ude80\n\nAs large language models (LLMs) continue to revolutionize the field of Natural Language Processing (NLP), a critical discussion emerges on the importance of combining human expertise with machine efficiency. While LLMs excel in rapid and cost-effective data labeling, their limitations in grasping intricate social, cultural, or domain-specific nuances may result in inaccuracies.\n\nIntroducing MEGAnno+ - a cutting-edge human-LLM collaborative annotation system designed to enhance the accuracy and quality of labeled data. By leveraging the strengths of both humans and LLMs, MEGAnno+ offers efficient LLM agent and annotation management, robust annotation processes, and human verification to ensure precise labeling.\n\nDiscover how collaborative efforts between humans and LLMs can elevate the standard of data annotation and drive impactful advancements in AI and NLP. Dive into the details of MEGAnno+ here: [Read more](http://arxiv.org/abs/2402.18050v1)\n\n#AI #NLP #Collaboration #DataAnnotation #MEGAnno+ #TechInnovation \ud83c\udf1f", "x": "\ud83d\ude80 Exciting research alert! Large language models (#LLMs) are speeding up data labeling, but may lack nuanced understanding. Check out MEGAnno+, a collaborative human-LLM annotation system for reliable labels. Dive into the details here: http://arxiv.org/abs/2402.18050v1 #NLP #AI", "source_id": "cd304708627c9dc9f709a2627a3aad84bb01c26774e0a6e4b6020abc814ccafe", "page_number": 1}, "score": null, "embedding": [-0.25679105520248413, -0.17732124030590057, -0.2987532615661621, -0.101365827023983, 0.013261719606816769, 0.029028723016381264, 0.1001134142279625, 0.13136401772499084, 0.22253519296646118, -0.5388450026512146, -0.04862194508314133, 0.021147634834051132, 0.41145992279052734, 0.37492936849594116, 0.21578490734100342, 0.11587224155664444, -0.2049376219511032, 0.3649131655693054, 0.010164585895836353, -0.07823103666305542, 0.3543168604373932, 0.00610333913937211, -0.14087653160095215, 0.08301793783903122, -0.036870747804641724, 0.06906035542488098, -0.2978105843067169, -0.21075473725795746, -0.21688275039196014, -1.3358253240585327, 0.3192721903324127, -0.13315103948116302, 0.40723341703414917, 0.11400565505027771, -0.12456165254116058, -0.005770998541265726, -0.08903536945581436, -0.07266238331794739, 0.08670976758003235, 0.05286474898457527, -0.20196178555488586, 0.08837366849184036, 0.011241285130381584, -0.1365506947040558, -0.06442002207040787, -0.3557389974594116, -0.05094851180911064, -0.23590056598186493, -0.8196366429328918, -0.1583656221628189, 0.019639188423752785, -0.2019738256931305, -0.023353679105639458, 0.50421142578125, 0.017795691266655922, 0.3701797127723694, 0.35870489478111267, 0.07372298836708069, 0.11190927773714066, 0.05620166286826134, 0.14343126118183136, 0.3505447804927826, -1.0207493305206299, 0.5051278471946716, -0.08780486136674881, 0.16731832921504974, -0.11214959621429443, 0.2053040713071823, -0.10333804786205292, 0.1349133402109146, -0.060458481311798096, 0.13969460129737854, 0.22916646301746368, 0.20555691421031952, 0.2408653050661087, 0.24040274322032928, 0.12330606579780579, -0.23829896748065948, 0.21341943740844727, -0.2338365614414215, -0.07142072916030884, 0.05409723520278931, -0.10739848762750626, -0.42120301723480225, -0.10285834223031998, -0.052848268300294876, -0.17257152497768402, -0.1716800332069397, 0.15402092039585114, -0.10282617807388306, -0.0023788916878402233, 0.03128752484917641, -0.16656193137168884, 0.3455847501754761, -0.04591221362352371, -0.03870917111635208, -0.03297644481062889, 0.1010405644774437, -0.3843744397163391, 0.7617672681808472, -0.17095947265625, 0.23177719116210938, 0.00446005305275321, -0.2870456278324127, 0.2539472281932831, -0.1487051546573639, -0.27683553099632263, -0.4046051800251007, -0.11279895901679993, 0.06335749477148056, -0.09751950949430466, 0.19692756235599518, -0.06748814880847931, -0.17331849038600922, 0.15011543035507202, -0.15356285870075226, 0.5705808997154236, -0.08866209536790848, -0.054914601147174835, 0.10326188802719116, -0.06255985051393509, 0.08573208749294281, 0.040506862103939056, -0.023904375731945038, 0.12379837781190872, -0.008729495108127594, 0.14316107332706451, 0.4955374598503113, 0.19518709182739258, 0.16335460543632507, 0.21592599153518677, 0.03480328992009163, -0.25825390219688416, -0.012546143494546413, 0.3432970941066742, -0.16870670020580292, 0.008672423660755157, -0.17245125770568848, 0.13920342922210693, 0.22997967898845673, -0.10527582466602325, 0.14895270764827728, 0.06079369783401489, -0.23866479098796844, -0.31631967425346375, 0.6462021470069885, -0.038681644946336746, -0.08448540419340134, -0.5142508745193481, -0.04408988729119301, 0.037300486117601395, 0.1675456464290619, -0.2833908498287201, -0.1760246455669403, -0.027378488332033157, 0.04593636840581894, 0.4539988934993744, -0.08825630694627762, -0.4352978765964508, 0.07718763500452042, 0.16697950661182404, -0.1711946576833725, -0.05269506573677063, 0.7159136533737183, -0.211091548204422, -0.661359965801239, -0.060144465416669846, -0.1527574211359024, 0.3146471083164215, -0.0804734081029892, 0.2495747059583664, -0.004723720718175173, -0.15749426186084747, 0.038144301623106, -0.13423621654510498, 0.22814276814460754, -0.5364405512809753, -0.11287905275821686, -0.08748657256364822, 0.11157703399658203, -0.014421217143535614, -0.002983677200973034, -0.2855570316314697, 0.14795026183128357, 0.030939700081944466, -0.15852105617523193, -0.04392768815159798, -0.2893742024898529, 0.04428407549858093, 0.11060623824596405, 0.05784763768315315, 0.4228630065917969, 0.22333233058452606, -0.03156360238790512, -0.21335797011852264, -0.31056731939315796, -0.11052644997835159, 0.07705272734165192, 0.1537552922964096, -0.28429001569747925, -0.01895063929259777, 0.23236727714538574, 0.2665935158729553, -0.10609704256057739, 0.4005222022533417, -0.22009578347206116, -0.12591750919818878, -0.009403497911989689, 0.22693847119808197, 0.16938331723213196, -0.25778520107269287, 0.10257495939731598, 0.18808768689632416, 0.06217318773269653, -0.22142823040485382, 0.06560851633548737, 0.4454297721385956, 0.434997022151947, 0.03403252363204956, 0.2058221697807312, 0.003910131752490997, 0.3814057409763336, -0.16873006522655487, -1.3338243961334229, -0.202005073428154, 0.22228069603443146, 0.043655876070261, 0.17032945156097412, -0.19701619446277618, 0.1677456498146057, 0.13785332441329956, 0.1093110665678978, 0.4334914982318878, 0.27712833881378174, 0.11971034109592438, -0.17671185731887817, 0.09428029507398605, -0.0685160756111145, 0.10601398348808289, 0.005157129373401403, 0.16682837903499603, 0.038073331117630005, 0.371002733707428, -0.19468344748020172, -0.10501352697610855, 0.011094911955296993, -0.7101193070411682, 0.17320211231708527, -0.22923362255096436, 1.065675973892212, -0.174296036362648, 0.012643368914723396, 0.03705804422497749, -0.11802539229393005, 0.2344282865524292, -0.3108116090297699, -0.5126668214797974, 0.4497166574001312, 0.02702641300857067, -0.057611264288425446, -0.06665899604558945, 0.08462204784154892, -0.2740977704524994, -0.3064518868923187, -0.023001719266176224, 0.3196786344051361, -0.9080406427383423, -0.3507104218006134, 0.049405358731746674, 0.0327034667134285, -0.011717419140040874, -0.3014509677886963, 0.1926199048757553, 0.25856634974479675, 0.2493150234222412, 0.3483125567436218, -0.04289872571825981, -0.25304853916168213, 0.1983194500207901, -0.5696750283241272, 0.049748387187719345, -0.34981319308280945, 0.06332675367593765, 0.132285013794899, -0.2602725625038147, -0.2652812600135803, -0.06895361095666885, 0.21891218423843384, -0.18935243785381317, -0.009701663628220558, -0.060492079704999924, 0.24040593206882477, -0.20492751896381378, -0.31566348671913147, 0.6346036195755005, -0.25584957003593445, -0.03560977056622505, 0.17833760380744934, 0.24591349065303802, -0.20241762697696686, -0.40345990657806396, -0.45070570707321167, 0.2533045709133148, 0.4953400492668152, 0.2499600499868393, 0.2693803310394287, -0.08542942255735397, 0.30057135224342346, 0.12861402332782745, 0.23513297736644745, 0.014708956703543663, 0.1701464205980301, -0.007282670121639967, 0.041331157088279724, 0.027105316519737244, -0.17221681773662567, -0.09307487308979034, -0.17228685319423676, -0.11826624721288681, -1.1378631591796875, 0.14446862041950226, -0.16354645788669586, 0.23542971909046173, -0.3094954490661621, 0.11931762099266052, 0.09708710759878159, -0.010623730719089508, 0.13608190417289734, 0.027092937380075455, -0.09171777218580246, 0.00180706731043756, 0.10740631818771362, -0.1590951383113861, 0.062319397926330566, 0.23869553208351135, 0.42368412017822266, 0.01411138754338026, 0.2616402804851532, -0.30386632680892944, 0.008249053731560707, 0.025636419653892517, 1.1964768171310425, 0.017010167241096497, -0.08386687934398651, 0.3433445394039154, 0.10213501751422882, 0.016844552010297775, 0.13781732320785522, 0.041209716349840164, -0.22041729092597961, 0.27576783299446106, 0.7671112418174744, -0.24424172937870026, -0.09638393670320511, 0.37263715267181396, -0.3078776001930237, -0.23433154821395874, 0.24507761001586914, 0.28831520676612854, 0.14567166566848755, -0.12418774515390396, -0.0364052839577198, -0.21949917078018188, 0.5597219467163086, 0.1564345508813858, 0.07386806607246399, -0.3277197480201721, -0.10875509679317474, 0.03464949131011963, -0.2443052977323532, -0.21158801019191742, -0.20256417989730835, 0.07699074596166611, 0.22065220773220062, 0.1984512209892273, 0.16249829530715942, -0.21233238279819489, -0.2561112642288208, -0.14141374826431274, -0.153717041015625, -0.1642749160528183, 0.029632963240146637, 0.1691676825284958, -0.21228967607021332], "sparse_embedding": null}, {"id": "3dd33a07282cc7d5994269d2e955a480df34a788ab2de927a326f72d36b24534", "content": "Whether Large Language Models (LLMs) can outperform crowdsourcing on the data\nannotation task is attracting interest recently. Some works verified this issue\nwith the average performance of individual crowd workers and LLM workers on\nsome specific NLP tasks by collecting new datasets. However, on the one hand,\nexisting datasets for the studies of annotation quality in crowdsourcing are\nnot yet utilized in such evaluations, which potentially provide reliable\nevaluations from a different viewpoint. On the other hand, the quality of these\naggregated labels is crucial because, when utilizing crowdsourcing, the\nestimated labels aggregated from multiple crowd labels to the same instances\nare the eventually collected labels. Therefore, in this paper, we first\ninvestigate which existing crowdsourcing datasets can be used for a comparative\nstudy and create a benchmark. We then compare the quality between individual\ncrowd labels and LLM labels and make the evaluations on the aggregated labels.\nIn addition, we propose a Crowd-LLM hybrid label aggregation method and verify\nthe performance. We find that adding LLM labels from good LLMs to existing\ncrowdsourcing datasets can enhance the quality of the aggregated labels of the\ndatasets, which is also higher than the quality of LLM labels themselves.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2401.09760v1", "title": "A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation", "content": "http://arxiv.org/pdf/2401.09760v1", "datetime": "2024-01-18 07:23:51", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting developments in the world of AI and NLP! Can Large Language Models (LLMs) outperform crowdsourcing on data annotation tasks? \ud83e\udd14\ud83d\udcca\n\nRecent studies have delved into this question by comparing the performance of individual crowd workers and LLM workers on specific NLP tasks. However, there's a new perspective to explore! \ud83c\udf1f\n\nCheck out this research paper that investigates utilizing existing crowdsourcing datasets for evaluating annotation quality from a different angle. \ud83d\udcdd The findings reveal that incorporating LLM labels can enhance the quality of aggregated labels, surpassing the quality of LLM labels alone. \ud83d\udcc8\n\nDive deeper into the study and explore the proposed Crowd-LLM hybrid label aggregation method for optimizing data annotation tasks. \ud83e\udde0\n\nRead more about this intriguing research here: http://arxiv.org/abs/2401.09760v1\n\n#AI #NLP #LLMs #DataAnnotation #Crowdsourcing #Research #TechInnovation", "x": "\ud83d\ude80 Exciting findings in the world of AI and NLP! Can Large Language Models outperform crowdsourcing for data annotation tasks? This study delves into the comparison between individual crowd workers and LLM workers, along with proposing a Crowd-LLM hybrid label aggregation method. Discover the results here: http://arxiv.org/abs/2401.09760v1 #AI #NLP #LLMs #Crowdsourcing #TechResearch \ud83e\udd16\ud83d\udcca", "source_id": "19cfd790ce006f70d36acdd9957245032367a9c713690da993ab54bbb456e95b", "page_number": 1}, "score": null, "embedding": [-0.181641086935997, -0.3666155934333801, -0.07095056027173996, -0.15243162214756012, 0.23473891615867615, 0.23850491642951965, -0.207556813955307, 0.02569299004971981, -0.02366017922759056, -0.28924649953842163, 0.17701400816440582, -0.07654142379760742, 0.27013441920280457, 0.23921175301074982, 0.08865150809288025, 0.19507279992103577, 0.05126899480819702, 0.0885963886976242, -0.11004018783569336, -0.2512015402317047, 0.06808963418006897, -0.03781444951891899, -0.005296523217111826, 0.00024154431594070047, 0.08904580026865005, 0.11783844232559204, -0.2744603157043457, -0.39188694953918457, -0.4148466885089874, -1.4711421728134155, 0.34232819080352783, -0.17442382872104645, 0.39037734270095825, 0.1787230521440506, 0.14498920738697052, 0.06954876333475113, -0.007767864037305117, 0.34490618109703064, 0.034727148711681366, 0.06765632331371307, -0.030789099633693695, 0.04990960285067558, -0.07357438653707504, -0.3319259583950043, -0.024845600128173828, -0.4243961274623871, -0.10541119426488876, -0.11704204231500626, -0.6625069379806519, 0.23811651766300201, -0.026478568091988564, -0.3419845998287201, -0.10330662876367569, 0.16393907368183136, 0.14182747900485992, 0.3741869330406189, 0.006329479161649942, 0.1213255226612091, 0.16513264179229736, 0.0975661650300026, 0.2507255971431732, 0.1729540228843689, -0.911539614200592, 0.1820414513349533, 0.0809055045247078, 0.17170652747154236, -0.13227835297584534, 0.1586437225341797, -0.20899927616119385, 0.1311873495578766, 0.14051410555839539, 0.04101090133190155, 0.24396204948425293, 0.22820740938186646, 0.18493704497814178, 0.4109773337841034, 0.04078047350049019, -0.0508585125207901, 0.03429706394672394, -0.054968301206827164, -0.3613877296447754, 0.2478095442056656, 0.0903785452246666, -0.2260749340057373, -0.08846400678157806, -0.04493875801563263, 0.08508121222257614, -0.21461042761802673, 0.23701737821102142, -0.19962963461875916, 0.28488972783088684, 0.20029203593730927, -0.2116331309080124, 0.5052372813224792, -0.008195356465876102, -0.02578936517238617, 0.045487869530916214, 0.15716604888439178, -0.005588994827121496, 0.6386499404907227, -0.3964642882347107, -0.04202618449926376, -0.28856346011161804, -0.4051765501499176, 0.2918182909488678, -0.4100407063961029, -0.12921828031539917, -0.3635990023612976, -0.1393541693687439, 0.08680921047925949, -0.17232121527194977, 0.015417286194860935, -0.11593469977378845, -0.2186063677072525, 0.07975753396749496, -0.20377559959888458, 0.40221869945526123, 0.09342935681343079, -0.17148613929748535, 0.031154628843069077, -0.26672425866127014, 0.017523394897580147, 0.23264533281326294, -0.005375293083488941, 0.31495201587677, 0.11791644990444183, 0.187309131026268, 0.44227680563926697, 0.3135865032672882, -0.12415426969528198, 0.1991942673921585, -0.005021072458475828, -0.6759849190711975, -0.016955677419900894, 0.25342243909835815, 0.004957328550517559, 0.11539949476718903, -0.04711179435253143, 0.157388374209404, 0.3798927664756775, 0.1016555055975914, 0.1063682958483696, 0.01299666054546833, -0.10213854908943176, -0.3459872305393219, 1.0094486474990845, -0.10721228271722794, 0.021530255675315857, -0.2585316300392151, -0.3660690188407898, 0.061574820429086685, 0.2751130759716034, -0.2623681128025055, -0.11571542173624039, 0.22304366528987885, 0.2601054608821869, 0.3728446662425995, 0.13815821707248688, -0.36165210604667664, 0.11041723936796188, 0.10308289527893066, -0.21407973766326904, 0.10556816309690475, 0.55995774269104, -0.3454650640487671, -0.5960215330123901, -0.2043098509311676, -0.2159431129693985, 0.058332089334726334, -0.16601617634296417, 0.1946868747472763, -0.0002011826727539301, 0.017899515107274055, 0.3180423676967621, -0.14114481210708618, 0.10260433703660965, -0.5187207460403442, -0.09084978699684143, 0.0042437342926859856, 0.20621652901172638, 0.030752139165997505, 0.04724489524960518, -0.35088229179382324, 0.3748922049999237, -0.036015987396240234, -0.24725396931171417, -0.11225438863039017, -0.2369571328163147, 0.06700319796800613, 0.49785947799682617, -0.1740199327468872, 0.0009016260737553239, 0.019719334319233894, 0.1745126098394394, 0.006728429812937975, -0.4595794677734375, 0.05235571786761284, 0.1538085639476776, 0.36486631631851196, -0.24345192313194275, -0.10907935351133347, 0.2197330892086029, 0.15763601660728455, -0.1329037845134735, 0.18592871725559235, -0.031490735709667206, -0.239002525806427, -0.11501477658748627, 0.28667575120925903, 0.050990454852581024, -0.25141483545303345, -0.15054620802402496, 0.19133514165878296, 0.11278984695672989, -0.10391387343406677, 0.08530202507972717, 0.46425455808639526, 0.25601816177368164, 0.022197389975190163, 0.25077441334724426, -0.16355443000793457, 0.228960782289505, -0.23585614562034607, -1.2125163078308105, -0.25358936190605164, 0.3249778151512146, 0.23924662172794342, 0.2057204246520996, -0.07564564794301987, 0.08587828278541565, 0.38887858390808105, 0.1628858596086502, 0.3635864853858948, 0.26074713468551636, -0.02067558281123638, -0.2969159483909607, 0.1556425839662552, 0.054220251739025116, 0.22693461179733276, -0.13900823891162872, 0.2000129669904709, -0.17054030299186707, 0.07528527081012726, -0.05006920546293259, 0.15306825935840607, 0.1906738430261612, -0.6375799775123596, 0.21582888066768646, -0.14462697505950928, 0.8507192730903625, -0.32239702343940735, -0.2734397053718567, -0.21368610858917236, 0.045682914555072784, 0.2729363739490509, -0.3598697781562805, -0.5950703620910645, 0.46618691086769104, -0.15162703394889832, 0.054845549166202545, 0.012279882095754147, -0.11391319334506989, -0.02152290754020214, -0.1632937639951706, -0.09338429570198059, 0.2142699509859085, -0.9142333269119263, -0.36451199650764465, -0.14624957740306854, 0.013065436854958534, 0.021957440301775932, -0.43316882848739624, 0.1787300556898117, -0.05490988492965698, -0.02510170266032219, 0.5246050357818604, -0.08874790370464325, -0.16069532930850983, 0.1051105335354805, -0.4921870827674866, 0.30327147245407104, -0.23144617676734924, 0.08915269374847412, 0.24312327802181244, -0.11246863752603531, 0.05405910685658455, -0.2878817021846771, 0.42362627387046814, -0.09219899773597717, -0.1363174468278885, -0.07695561647415161, -0.08700105547904968, -0.06306476145982742, -0.29003220796585083, 0.7088525891304016, -0.07981722801923752, 0.08166132867336273, 0.25136086344718933, 0.12160532176494598, -0.29728734493255615, -0.30038949847221375, -0.2160680741071701, -0.025590647011995316, 0.2768848240375519, -0.041214242577552795, 0.1391737014055252, 0.15022340416908264, 0.14950379729270935, 0.24567648768424988, 0.254932701587677, -0.03979657590389252, 0.1244569942355156, -0.018353961408138275, 0.2083268016576767, -0.05203031376004219, -0.46012333035469055, 0.18172766268253326, 0.08457444608211517, 0.23632343113422394, -0.9257053732872009, 0.08940067142248154, -0.22216883301734924, 0.49714532494544983, -0.047453608363866806, -0.18018901348114014, 0.2998276650905609, -0.09947215020656586, 0.25987470149993896, -0.0013519448693841696, -0.11027344316244125, 0.2537504732608795, 0.1118694320321083, -0.4860013723373413, 0.024406617507338524, 0.30787143111228943, 0.32001134753227234, -0.09192550182342529, 0.05498142167925835, -0.5021800994873047, 0.0007456264575012028, 0.12248370796442032, 1.0418668985366821, 0.04392493888735771, -0.3225463032722473, 0.23437239229679108, -0.14112648367881775, 0.021045394241809845, 0.1817074567079544, 0.05148022249341011, -0.14162375032901764, -0.2439556121826172, 0.8831538558006287, -0.01603858545422554, 0.07380158454179764, 0.310416042804718, -0.21629177033901215, -0.3844573497772217, 0.12749019265174866, 0.17674799263477325, 0.410148024559021, -0.1635160744190216, 0.05242673307657242, -0.04889543727040291, 0.3901897370815277, 0.22707970440387726, 0.17014697194099426, -0.5073130130767822, -0.12935400009155273, 0.1143607422709465, -0.3600044846534729, -0.24165363609790802, -0.17348718643188477, -0.07283429801464081, -0.018624473363161087, 0.13439355790615082, 0.1294594407081604, -0.014148677699267864, -0.10247858613729477, -0.048788752406835556, 0.08480481803417206, -0.3510221242904663, 0.03763219341635704, 0.04645274206995964, -0.24468576908111572], "sparse_embedding": null}, {"id": "850614379780f9fbe0e259b9556ab0e774c40107ea95bb2bf1084074fc832c84", "content": "Instruction tuning is critical to large language models (LLMs) for achieving\nbetter instruction following and task adaptation capabilities but its success\nheavily relies on the training data quality. Many recent methods focus on\nimproving the data quality but often overlook the compatibility of the data\nwith the student model being finetuned. This paper introduces Selective\nReflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection\nand introspection for improving existing data quality with the data selection\ncapability of the student LLM, to automatically refine existing\ninstruction-tuning data. This teacher-student collaboration produces\nhigh-quality and student-compatible instruction-response pairs, resulting in\nsample-efficient instruction tuning and LLMs of superior performance. Selective\nReflection-Tuning is a data augmentation and synthesis that generally improves\nLLM finetuning and self-improvement without collecting brand-new data. We apply\nour method to Alpaca and WizardLM data and achieve much stronger and top-tier\n7B and 13B LLMs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.10110v2", "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning", "content": "http://arxiv.org/pdf/2402.10110v2", "datetime": "2024-06-07 20:23:21", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting advancements in large language models! \ud83c\udf1f\n\nImproving instruction tuning for LLMs is crucial for enhancing their performance, but it heavily relies on the quality of training data. Check out this groundbreaking paper introducing Selective Reflection-Tuning, a novel paradigm that leverages teacher-student collaboration to automatically refine instruction-tuning data, resulting in high-quality and student-compatible instruction-response pairs. This innovative approach boosts LLMs' performance and efficiency without the need for additional data collection.\n\nRead more about this cutting-edge research and its impact on Alpaca and WizardLM data at: http://arxiv.org/abs/2402.10110v2\n\n#AI #NLP #LLMs #DataQuality #Research #Innovation", "x": "\ud83d\ude80 Exciting research alert! Learn about Selective Reflection-Tuning, a novel paradigm enhancing instruction tuning for large language models (LLMs). This method improves data quality and student model compatibility, boosting LLM performance. Check out the paper here: http://arxiv.org/abs/2402.10110v2 #AI #NLP #LLMs #Research #TechInnovation", "source_id": "109ae09ef991fd0c2bfc5fa26f1e0ece29d3556ff5b25b87d68e48450085edda", "page_number": 1}, "score": null, "embedding": [-0.5035801529884338, -0.14389050006866455, 0.17840556800365448, -0.1464412659406662, -0.18024443089962006, -0.02193090319633484, -0.34103789925575256, 0.24706986546516418, 0.3874647915363312, -0.1321365088224411, 0.16405700147151947, -0.11545778810977936, 0.1083015725016594, 0.331328809261322, 0.10612411051988602, 0.3023034334182739, -0.012834561057388783, 0.29459598660469055, -0.03505101427435875, -0.04089115187525749, 0.20471154153347015, 0.06532849371433258, -0.05702418088912964, -0.07098092138767242, 0.034806109964847565, 0.004176727496087551, -0.17860287427902222, -0.1564476191997528, -0.3030531704425812, -1.3107969760894775, 0.07069749385118484, -0.20457948744297028, 0.15039585530757904, 0.052363429218530655, -0.2913272976875305, 0.10008849948644638, -0.3177310824394226, 0.08396930992603302, -0.19086597859859467, 0.17145147919654846, -0.1028396263718605, 0.08791519701480865, -0.10129440575838089, -0.1601836234331131, -0.23381049931049347, -0.2936099171638489, -0.12434884905815125, -0.231122225522995, -0.31060147285461426, 0.03596016392111778, -0.33358290791511536, -0.3428545594215393, -0.04297036677598953, -0.008362817578017712, 0.12445424497127533, 0.3679964542388916, 0.1120370626449585, 0.3333461284637451, -0.052007418125867844, 0.2784985303878784, 0.25129154324531555, 0.3514329493045807, -1.1122373342514038, 0.5552638173103333, -0.14848220348358154, 0.08557494729757309, 0.07558592408895493, -0.16346526145935059, 0.3024662733078003, 0.10267749428749084, -0.1879325956106186, -0.002783681731671095, 0.25657927989959717, 0.27333685755729675, 0.40099725127220154, 0.398118793964386, 0.04079480096697807, -0.23635780811309814, 0.3182774484157562, -0.07254783809185028, 0.19998086988925934, -0.29525840282440186, 0.04205452278256416, -0.23511239886283875, 0.12063613533973694, 0.04315491393208504, -0.24597738683223724, -0.2881021797657013, 0.21578171849250793, -0.1227235496044159, 0.2175767570734024, -0.16290421783924103, -0.46890905499458313, 0.222564697265625, -0.10599269717931747, -0.3031653165817261, -0.006123937666416168, 0.06197298318147659, -0.3281458020210266, 0.6390334963798523, 0.01480723824352026, -0.12976409494876862, -0.03075067698955536, 0.1368912011384964, 0.21642228960990906, -0.22835828363895416, 0.029024554416537285, 0.08403955399990082, -0.14611922204494476, -0.24079009890556335, 0.2143760621547699, -0.16602706909179688, -0.11223328113555908, -0.14415118098258972, -0.198614701628685, 0.2908802926540375, 0.27676212787628174, -0.3029242753982544, 0.2701028883457184, 0.051973529160022736, -0.12561990320682526, -0.014386272989213467, 0.20123597979545593, -0.10522367805242538, 0.12176796048879623, -0.15542402863502502, -0.0765993520617485, 0.32645225524902344, 0.11733560264110565, 0.4311322271823883, 0.0865040197968483, -0.34377357363700867, -0.34116318821907043, -0.33796191215515137, 0.100830078125, -0.23150314390659332, 0.08535223454236984, -0.3498505651950836, 0.16896416246891022, 0.14470605552196503, -0.04747238755226135, -0.03769488260149956, -0.08644922822713852, -0.253409743309021, -0.24736778438091278, 0.6681255102157593, -0.09613146632909775, -0.01862931437790394, -0.308794766664505, 0.018852626904845238, -0.1125679612159729, 0.0416705422103405, -0.4419053792953491, 0.03677680343389511, 0.2545374631881714, 0.22981752455234528, 0.21182194352149963, 0.19849608838558197, -0.1318003088235855, -0.05903595685958862, -0.3135055899620056, -0.022962508723139763, -0.17887656390666962, 0.675919771194458, -0.23261207342147827, -0.21948504447937012, -0.5187272429466248, 0.2088077962398529, -0.04106102138757706, -0.0317748598754406, 0.19503988325595856, 0.1712537258863449, -0.09634532779455185, 0.10654184222221375, 0.1482810527086258, 0.1748824417591095, -0.5841864943504333, 0.07236767560243607, -0.07180467247962952, 0.09202392399311066, 0.3949577510356903, -0.23837199807167053, -0.11885656416416168, 0.20991742610931396, 0.2411559671163559, 0.08365590125322342, 0.05848514288663864, -0.23333482444286346, 0.12476079910993576, -0.019355429336428642, -0.464839905500412, 0.10864366590976715, 0.022448673844337463, -0.07900744676589966, -0.25861260294914246, -0.05036037042737007, 0.029054969549179077, -0.06746658682823181, 0.06264612078666687, -0.17666544020175934, -0.07585934549570084, 0.24681100249290466, -0.17988811433315277, 0.026667824015021324, 0.0916745588183403, 0.03947509452700615, 0.01280657947063446, 0.1563243865966797, 0.4824073314666748, 0.41571852564811707, -0.21633249521255493, 0.014374754391610622, 0.24114012718200684, -0.03540259227156639, -0.22139586508274078, -0.03224382549524307, 0.06875786185264587, 0.3454453647136688, -0.02829737961292267, 0.14269769191741943, 0.17516106367111206, -0.1355603039264679, -0.2667436897754669, -1.286442518234253, -0.07544368505477905, 0.1503722220659256, 0.3058147728443146, 0.3819256126880646, -0.40938112139701843, 0.2566297948360443, 0.2898171842098236, 0.12481143325567245, -0.09910168498754501, 0.03835084289312363, 0.30313873291015625, -0.3329377770423889, 0.12792718410491943, -0.04033608362078667, -0.0839431956410408, -0.024622036144137383, 0.08052029460668564, -0.10632918030023575, 0.11781828105449677, -0.18040387332439423, 0.5147721171379089, 0.17017585039138794, -0.3957781195640564, 0.378187358379364, -0.22012916207313538, 0.6420890688896179, -0.11056563258171082, 0.04769327491521835, 0.14217381179332733, 0.2686724066734314, 0.021528048440814018, 0.04144645854830742, -0.17734599113464355, 0.3533113896846771, 0.06047255918383598, 0.19102682173252106, 0.04695175215601921, -0.017675800248980522, -0.18332380056381226, 0.1952645182609558, 0.011879348196089268, -0.058646220713853836, -0.8001723289489746, -0.4480478763580322, 0.09466078132390976, 0.04277942702174187, -0.06389879435300827, -0.49870815873146057, 0.05886760726571083, 0.18682105839252472, -0.06861008703708649, 0.09864217787981033, -0.0748620480298996, -0.328338086605072, -0.10805343836545944, -0.5237051248550415, 0.3435485363006592, -0.2225402444601059, -0.12047767639160156, 0.21229897439479828, -0.029193364083766937, -0.15600672364234924, -0.17386852204799652, -0.04697285592556, 0.0711827203631401, 0.03541348874568939, 0.11891065537929535, 0.04261345788836479, -0.03951772674918175, -0.008561896160244942, 0.3049316704273224, 0.22656452655792236, 0.10868566483259201, -0.09741338342428207, 0.16321444511413574, -0.2269931137561798, -0.11211805790662766, -0.27728933095932007, -0.043540459126234055, 0.3955628275871277, 0.33086511492729187, -0.08807019889354706, 0.28070613741874695, 0.279159277677536, 0.22126176953315735, 0.48773297667503357, 0.13604411482810974, 0.15876224637031555, 0.20204252004623413, -0.026076581329107285, -0.04222183674573898, -0.23998597264289856, -0.1426091492176056, 0.17676670849323273, 0.20534507930278778, -1.286405086517334, -0.1342346966266632, -0.1415199190378189, 0.47028693556785583, -0.07834392040967941, 0.04195914417505264, 0.03356815502047539, -0.015753082931041718, -0.08594831824302673, 0.06556632369756699, 0.0854877233505249, -0.11779341846704483, -0.06965730339288712, 0.19881169497966766, 0.11647077649831772, -0.0303626861423254, 0.6055198311805725, -0.17167970538139343, 0.19445353746414185, -0.45937731862068176, -0.11681311577558517, 0.09760195016860962, 1.0613958835601807, 0.11468444019556046, 0.022625815123319626, -0.009096137247979641, -0.1908172219991684, 0.02412610873579979, 0.2979375720024109, -0.2602314054965973, 0.08683883398771286, 0.05953139066696167, 0.8520869612693787, 0.0011865111300721765, -0.15773507952690125, 0.45591944456100464, -0.10214284062385559, 0.11958771198987961, 0.11443270742893219, 0.03375035524368286, 0.14948832988739014, 0.07575295120477676, 0.03633890300989151, -0.03575531020760536, 0.6700500845909119, 0.04720227047801018, -0.09128694236278534, -0.33470427989959717, -0.18130435049533844, 0.010634256526827812, 0.17407359182834625, 0.04547147825360298, -0.17190031707286835, -0.12581150233745575, 0.2747325599193573, 0.2520619332790375, -0.3447571098804474, 0.07872051000595093, -0.21600495278835297, -0.20567239820957184, 0.2502546012401581, 0.2007874846458435, -0.022541064769029617, 0.0031045768409967422, -0.2435721606016159], "sparse_embedding": null}, {"id": "9259a7c56f4b45036b2ff0b6146afce940635521e648ae2aacb0d2655173e762", "content": "In recent years, there have been remarkable advancements in node\nclassification achieved by Graph Neural Networks (GNNs). However, they\nnecessitate abundant high-quality labels to ensure promising performance. In\ncontrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency\non text-attributed graphs. Yet, they face challenges in efficiently processing\nstructural data and suffer from high inference costs. In light of these\nobservations, this work introduces a label-free node classification on graphs\nwith LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs\nwhile mitigating their limitations. Specifically, LLMs are leveraged to\nannotate a small portion of nodes and then GNNs are trained on LLMs'\nannotations to make predictions for the remaining large portion of nodes. The\nimplementation of LLM-GNN faces a unique challenge: how can we actively select\nnodes for LLMs to annotate and consequently enhance the GNN training? How can\nwe leverage LLMs to obtain annotations of high quality, representativeness, and\ndiversity, thereby enhancing GNN performance with less cost? To tackle this\nchallenge, we develop an annotation quality heuristic and leverage the\nconfidence scores derived from LLMs to advanced node selection. Comprehensive\nexperimental results validate the effectiveness of LLM-GNN. In particular,\nLLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with\na cost less than 1 dollar.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.04668v3", "title": "Label-free Node Classification on Graphs with Large Language Models (LLMS)", "content": "http://arxiv.org/pdf/2310.04668v3", "datetime": "2024-02-24 06:44:45", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting advancements in AI research! A new approach, LLM-GNN, combines Large Language Models and Graph Neural Networks for label-free node classification on graphs. By leveraging LLMs to annotate a small portion of nodes and training GNNs on this data, LLM-GNN achieves impressive accuracy of 74.9% on a large dataset while keeping costs under $1. Learn more about this innovative pipeline here: http://arxiv.org/abs/2310.04668v3 #AI #LLM #GNN #NodeClassification #TechInnovation \ud83c\udf1f", "x": "\ud83d\ude80 Exciting developments in AI research! Introducing LLM-GNN, a novel pipeline for label-free node classification on graphs. By combining Large Language Models and Graph Neural Networks, LLM-GNN achieves 74.9% accuracy on a large dataset with minimal cost. Learn more at: http://arxiv.org/abs/2310.04668v3 #AI #LLM #GNN #NodeClassification", "source_id": "cb3313c6cfa4e51ebc721eebf46b68b7f7f025cf954951dd9f02e70b84a8dd80", "page_number": 1}, "score": null, "embedding": [-0.01853274367749691, -0.2837054431438446, -0.19908097386360168, -0.05293571203947067, 0.15539787709712982, 0.1363011747598648, -0.21735917031764984, 0.013192481361329556, 0.08159398287534714, -0.3023320138454437, -0.07318088412284851, -0.0007768613286316395, 0.27707207202911377, 0.5113014578819275, 0.03621033951640129, 0.18012477457523346, 0.06851693242788315, 0.38548094034194946, -0.09153416752815247, -0.1915205866098404, 0.1488102823495865, -0.05056671425700188, 0.16127172112464905, -0.08638709783554077, 0.34915852546691895, -0.16370108723640442, -0.05089624226093292, -0.11413614451885223, -0.32857275009155273, -1.5804365873336792, 0.20240670442581177, -0.2146429419517517, 0.351284384727478, -0.0982189029455185, -0.08982213586568832, -0.1521824151277542, 0.05172182247042656, -0.0721122995018959, -0.0456693060696125, 0.15838806331157684, -0.022431541234254837, -0.021027160808444023, -0.07694540172815323, -0.2517412006855011, 0.17520751059055328, -0.21432587504386902, -0.012301295064389706, -0.3676532506942749, -0.7231314182281494, -0.11141340434551239, -0.10680252313613892, -0.3716672658920288, -0.028889106586575508, 0.2281835526227951, 0.0845196396112442, 0.3421243131160736, 0.13052135705947876, 0.10364717990159988, 0.2106248140335083, 0.33653756976127625, 0.04220478981733322, 0.44266021251678467, -0.891173243522644, 0.16734062135219574, 0.06457389146089554, 0.2266823947429657, -0.196470707654953, 0.21108677983283997, 0.15065856277942657, 0.3512851893901825, 0.30222082138061523, 0.13710887730121613, 0.32455068826675415, -0.0025201106909662485, 0.06466669589281082, 0.36455559730529785, 0.24305397272109985, 0.04229334741830826, 0.10915462672710419, -0.08101996034383774, 0.0011247607180848718, 0.08826380968093872, -0.01360726822167635, -0.4283805191516876, -0.026298852637410164, 0.025330249220132828, -0.28057268261909485, -0.25013670325279236, 0.09869448095560074, -0.03099372424185276, 0.13584187626838684, 0.045305293053388596, -0.31446373462677, 0.38593071699142456, -0.11043455451726913, -0.13149890303611755, 0.2350436896085739, -0.07594692707061768, -0.43805214762687683, 0.6446353197097778, -0.35475608706474304, 0.08844666928052902, -0.03848896920681, -0.16020001471042633, 0.2893033027648926, -0.17222878336906433, -0.11721540242433548, -0.3413540720939636, -0.42971038818359375, -0.01157083734869957, 0.040056608617305756, -0.16270451247692108, -0.2373936027288437, -0.10945045948028564, -0.04230378195643425, -0.09930280596017838, 0.5741395950317383, 0.11410269141197205, -0.07966385036706924, 0.10877763479948044, -0.36297544836997986, 0.08864666521549225, -0.01379737351089716, -0.11922973394393921, 0.2629792094230652, 0.008318937383592129, -0.15257225930690765, 0.35963720083236694, 0.14079533517360687, 0.10476137697696686, 0.1482752412557602, 0.12095552682876587, -0.4866599440574646, 0.17832167446613312, 0.2863868176937103, -0.10661571472883224, 0.29462194442749023, -0.2071358561515808, -0.15897223353385925, 0.3145362138748169, 0.004584236536175013, 0.1638030707836151, 0.34835150837898254, -0.061588745564222336, -0.5145134329795837, 0.591756284236908, -0.21472759544849396, -0.05891479551792145, -0.3019114136695862, -0.42220720648765564, -0.008983605541288853, 0.19732756912708282, -0.48474785685539246, -0.24469348788261414, 0.09372901916503906, 0.32367393374443054, 0.20872226357460022, 0.14300864934921265, -0.570732593536377, 0.03699132055044174, -0.07081864774227142, -0.03414730355143547, -0.17834067344665527, 0.6331691145896912, 0.16029095649719238, -0.45912620425224304, -0.099886454641819, -0.14058491587638855, 0.1664469987154007, -0.306517094373703, 0.39487868547439575, 0.3102259635925293, -0.09625889360904694, 0.09723642468452454, -0.19536004960536957, 0.2219160795211792, -0.7770016193389893, -0.35464903712272644, -0.15982501208782196, -0.08673148602247238, -0.06062733754515648, -0.14663363993167877, -0.2506707012653351, 0.18565909564495087, 0.16699934005737305, -0.14487949013710022, -0.07291215658187866, -0.3029562532901764, -0.10661657899618149, 0.1421416699886322, -0.2915872633457184, 0.2597067952156067, 0.22752505540847778, 0.010355555452406406, -0.22751739621162415, -0.37132129073143005, -0.17383237183094025, 0.15487754344940186, 0.17900851368904114, -0.33545631170272827, 0.0476994514465332, -0.1103639081120491, 0.22674590349197388, -0.15614083409309387, -0.005018425174057484, -0.16597811877727509, -0.2091587781906128, 0.08125220984220505, 0.14932237565517426, 0.25390681624412537, -0.2931545376777649, -0.09039468318223953, 0.31013017892837524, -0.03577592223882675, -0.18535026907920837, -0.042499590665102005, 0.3293417692184448, 0.3655892312526703, -0.267669141292572, 0.29911085963249207, 0.2988116145133972, 0.15073630213737488, -0.4910965859889984, -1.1151360273361206, -0.3367649018764496, 0.4118771553039551, 0.09970295429229736, 0.48260071873664856, -0.3682379126548767, 0.1234482154250145, 0.15121960639953613, 0.2437431812286377, 0.3359014689922333, 0.11481605470180511, 0.1719035655260086, -0.2500782907009125, -0.15832683444023132, 0.06658712774515152, 0.13901980221271515, 0.17476977407932281, 0.13043208420276642, 0.1107233464717865, 0.27498650550842285, -0.046099502593278885, 0.18114209175109863, 0.22569458186626434, -0.6723465323448181, 0.05183350667357445, 0.08561351895332336, 1.0248162746429443, -0.282675176858902, 0.12328929454088211, -0.06369371712207794, -0.20631106197834015, 0.080063097178936, -0.3156857490539551, -0.26248791813850403, 0.6542487740516663, 0.0999690592288971, 0.02623215690255165, 0.10432109981775284, -0.11868639290332794, -0.1720196008682251, -0.31883615255355835, -0.04039094224572182, 0.1554010957479477, -1.0498652458190918, -0.3123534619808197, 0.2525760233402252, 0.024036770686507225, -0.010572806932032108, -0.2684983015060425, -0.008659550920128822, 0.2506977617740631, 0.26092684268951416, 0.4047989547252655, -0.0509103387594223, -0.09037864208221436, 0.09327375143766403, -0.5377234220504761, 0.23322437703609467, -0.35111770033836365, 0.031821105629205704, 0.1915738582611084, -0.35353443026542664, -0.28355905413627625, -0.294537752866745, 0.46944108605384827, -0.019327308982610703, -0.14444294571876526, -0.04610217362642288, 0.06652795523405075, -0.07862433046102524, -0.1426495462656021, 0.4110599458217621, 0.28569987416267395, 0.01629190519452095, 0.32210487127304077, 0.1932811439037323, -0.11574021726846695, -0.18624486029148102, -0.3488905429840088, 0.20261025428771973, 0.28746095299720764, -0.05327346548438072, 0.1519123911857605, 0.18588809669017792, 0.0664796531200409, 0.3626510202884674, 0.23534110188484192, -0.01817244663834572, 0.2503950595855713, 0.2923268675804138, 0.14104999601840973, 0.14095158874988556, -0.319504976272583, -0.16502094268798828, 0.27957937121391296, -0.15271082520484924, -1.0089703798294067, 0.10284200310707092, 0.07599066942930222, 0.4605400860309601, 0.0351237952709198, 0.0923403948545456, 0.05945785343647003, -0.1335713118314743, 0.14369109272956848, 0.3067752718925476, -0.08586835116147995, 0.19756880402565002, 0.29974743723869324, -0.3600408136844635, -0.15941128134727478, 0.16472043097019196, 0.39242997765541077, -0.17578883469104767, 0.27020063996315, -0.07609879225492477, 0.10745523869991302, 0.29779767990112305, 1.138502597808838, -0.24513420462608337, -0.09303167462348938, 0.22724059224128723, -0.2933138608932495, -0.06823285669088364, 0.11603576689958572, 0.03413976728916168, -0.030974963679909706, 0.2340724915266037, 0.7875819206237793, -0.19331134855747223, 0.08170139044523239, 0.7532532811164856, -0.22288714349269867, -0.09018833190202713, 0.1914471834897995, 0.025042489171028137, 0.44240647554397583, -0.21521881222724915, -0.09432539343833923, -0.20786990225315094, 0.5489152073860168, -0.2510182857513428, 0.01680367812514305, -0.49730902910232544, -0.17058472335338593, 0.13859806954860687, -0.08264648169279099, -0.019839391112327576, -0.10589088499546051, 0.05588701367378235, -0.003782798768952489, 0.08624552935361862, 0.13575942814350128, -0.2017926424741745, -0.4213523864746094, -0.24268841743469238, 0.06787371635437012, -0.32226473093032837, 0.16880908608436584, 0.17189949750900269, -0.34251129627227783], "sparse_embedding": null}, {"id": "7d9bf2c964dd29dfe457920873ea2d27df399bb3877534d85d09b4c626e4077d", "content": "This paper mainly describes a unified system for hallucination detection of\nLLMs, which wins the second prize in the model-agnostic track of the\nSemEval-2024 Task 6, and also achieves considerable results in the model-aware\ntrack. This task aims to detect hallucination with LLMs for three different\ntext-generation tasks without labeled training data. We utilize prompt\nengineering and few-shot learning to verify the performance of different LLMs\non the validation data. Then we select the LLMs with better performance to\ngenerate high-quality weakly supervised training data, which not only satisfies\nthe consistency of different LLMs, but also satisfies the consistency of the\noptimal LLM with different sampling parameters. Furthermore, we finetune\ndifferent LLMs by using the constructed training data, and finding that a\nrelatively small LLM can achieve a competitive level of performance in\nhallucination detection, when compared to the large LLMs and the prompt-based\napproaches using GPT-4.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.12913v1", "title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data", "content": "http://arxiv.org/pdf/2402.12913v1", "datetime": "2024-02-20 11:01:39", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting News in AI Research \ud83d\ude80\n\nThrilled to share a groundbreaking paper on hallucination detection of Large Language Models (LLMs) that secured the second prize in the model-agnostic track of the SemEval-2024 Task 6! This unified system not only excelled in the model-aware track but also showcased impressive results in detecting hallucinations with LLMs across different text-generation tasks, all without the need for labeled training data.\n\nThe team leveraged prompt engineering and few-shot learning to evaluate various LLMs on validation data, ultimately identifying those with superior performance to generate high-quality weakly supervised training data. By fine-tuning different LLMs using this data, the study revealed that even smaller LLMs can achieve remarkable levels of performance in hallucination detection, rivaling larger LLMs and prompt-based approaches employing GPT-4.\n\nCurious to dive deeper into this cutting-edge research? Check out the full paper here: http://arxiv.org/abs/2402.12913v1\n\n#AI #LLMs #NLP #SemEval2024 #TechResearch #InnovationInAI", "x": "\ud83d\ude80 Exciting news in the field of LLMs! This paper presents a unified system for hallucination detection using LLMs, achieving remarkable results in the SemEval-2024 Task 6. Learn more about their innovative approach and findings at: http://arxiv.org/abs/2402.12913v1 #AI #NLP #LLMs #SemEval2024 \ud83e\udde0\ud83d\udd0d\ud83d\udcda", "source_id": "747ad4b286595f29a6f710cd4d0c9d3bf232d403960ab66568eb7a783a67bc1c", "page_number": 1}, "score": null, "embedding": [-0.19226832687854767, 0.08449019491672516, 0.14070087671279907, -0.031264107674360275, 0.11341431736946106, 0.09804486483335495, -0.19229251146316528, -0.04812152683734894, 0.32290416955947876, -0.34288573265075684, 0.11311943084001541, -0.29280486702919006, 0.12289931625127792, 0.39010632038116455, -0.05257369205355644, 0.2024051398038864, -0.07364103943109512, 0.3202001452445984, -0.01827096939086914, -0.006473100744187832, 0.2345947027206421, -0.006250663194805384, 0.14201627671718597, -0.03805328533053398, 0.005882936529815197, -0.12750686705112457, -0.08498121052980423, -0.11283331364393234, -0.5155237913131714, -1.4507036209106445, 0.05779721587896347, -0.4056026339530945, 0.2575875222682953, -0.005920517724007368, -0.03190208598971367, -0.13537082076072693, -0.21471938490867615, 0.03052174113690853, -0.13169395923614502, 0.15145766735076904, -0.010178152471780777, -0.014214945025742054, -0.14962846040725708, -0.21271000802516937, -0.025557957589626312, -0.5541932582855225, -0.16329067945480347, -0.37960848212242126, -0.5218876004219055, -0.049717847257852554, 0.014426523819565773, -0.22645966708660126, 0.12105102092027664, 0.16573423147201538, -0.0036310332361608744, 0.1626930981874466, 0.1252707988023758, 0.2234991490840912, 0.152227520942688, 0.16537898778915405, 0.2198939174413681, 0.5380845069885254, -0.9256199598312378, -0.012997809797525406, 0.1608048975467682, 0.08712070435285568, -0.3823831081390381, -0.06019652262330055, 0.17892171442508698, 0.21550074219703674, 0.018068823963403702, 0.13480032980442047, 0.19638238847255707, 0.1665716916322708, 0.23574569821357727, 0.3079301714897156, 0.05723755061626434, -0.08664095401763916, -0.025980940088629723, -0.06238682195544243, -0.044701799750328064, -0.05569007247686386, -0.02465195208787918, -0.27984023094177246, -0.13839589059352875, -0.09498244524002075, -0.2338941991329193, -0.10556543618440628, 0.2894521951675415, 0.052869416773319244, 0.08021747320890427, 0.12098073214292526, -0.2591094672679901, 0.1597871631383896, -0.2833584249019623, -0.08681205660104752, 0.12364217638969421, 0.16682817041873932, -0.01258342619985342, 0.6182922720909119, -0.2171241044998169, 0.046785593032836914, -0.0005390860023908317, 0.03829021006822586, 0.186305433511734, -0.06970951706171036, -0.3370993733406067, -0.13569903373718262, -0.30825895071029663, -0.042821791023015976, 0.09684626013040543, -0.06323044747114182, 0.06791873276233673, -0.16235673427581787, -0.06500811129808426, 0.25324371457099915, 0.6097241640090942, 0.047536835074424744, -0.07493019849061966, 0.07355791330337524, -0.18739064037799835, -0.05521587282419205, 0.19257515668869019, -0.4014669358730316, 0.20876921713352203, -0.10539861023426056, 0.15991900861263275, 0.37753573060035706, 0.3098835349082947, -0.007389785256236792, 0.17447498440742493, -0.012613167986273766, -0.6875450015068054, -0.18118323385715485, 0.03771340474486351, 0.1005864217877388, 0.19471822679042816, 0.04909071698784828, 0.14871177077293396, 0.15042729675769806, -0.3479422628879547, 0.17703986167907715, 0.48171311616897583, -0.11359380930662155, -0.4235970675945282, 0.3518143594264984, -0.18182459473609924, 0.16105282306671143, -0.2734341621398926, -0.3264961242675781, 0.03799056634306908, 0.12122896313667297, -0.20798589289188385, -0.3129255473613739, 0.3926191031932831, 0.30809152126312256, 0.2293200045824051, 0.16841654479503632, -0.2708466947078705, 0.048824891448020935, -0.2735865116119385, -0.010126234032213688, -0.07788970321416855, 1.2112478017807007, 0.08008699119091034, -0.3343859612941742, -0.22387483716011047, -0.09542476385831833, 0.22470077872276306, -0.32132095098495483, 0.2744612395763397, 0.07386514544487, -0.20055198669433594, -0.0842299833893776, -0.11174497753381729, -0.13957245647907257, -0.666354775428772, 0.24351803958415985, -0.21079833805561066, 0.14267244935035706, 0.16841383278369904, -0.3986714780330658, -0.31745266914367676, 0.21822528541088104, 0.06530687212944031, -0.2279052883386612, 0.11982014775276184, -0.3451586961746216, 0.2943874001502991, 0.2162247598171234, -0.2126200646162033, 0.12927430868148804, -0.016497667878866196, -0.10205140709877014, -0.08802619576454163, -0.1701533943414688, -0.13422322273254395, -0.22767728567123413, 0.17307455837726593, -0.13182853162288666, -0.09969007968902588, 0.07300575077533722, 0.1047128289937973, 0.018737390637397766, 0.15136970579624176, -0.12428770214319229, -0.08128242939710617, -0.13043949007987976, 0.11823736131191254, 0.32747122645378113, -0.08720893412828445, -0.1739736795425415, 0.3127349019050598, -0.3297490179538727, -0.04361613467335701, -0.0834226980805397, 0.2601083815097809, 0.2755430340766907, -0.17407192289829254, 0.2170938104391098, 0.11646194010972977, 0.27578452229499817, -0.2182357758283615, -1.2542282342910767, -0.2466905415058136, 0.2016458660364151, 0.16168609261512756, 0.31352171301841736, -0.296506404876709, -0.03335000202059746, -0.048324838280677795, 0.3202555477619171, 0.202693372964859, 0.45346951484680176, 0.2419673502445221, -0.2241635024547577, 0.14808982610702515, -0.2217094600200653, 0.3605829179286957, -0.021596157923340797, -0.007521977648139, -0.10173038393259048, 0.21553753316402435, -0.09055749326944351, 0.11078397184610367, 0.41757142543792725, -0.4104108512401581, -0.0775049701333046, -0.10399259626865387, 0.8616195917129517, -0.0651497021317482, 0.22574329376220703, 0.0407128632068634, -0.15115047991275787, 0.12101777642965317, -0.1186366006731987, -0.11378148943185806, 0.31817156076431274, 0.002815424697473645, -0.054665595293045044, -0.10481563955545425, 0.23913472890853882, -0.12310266494750977, -0.5246883034706116, -0.2082417756319046, 0.20270733535289764, -0.7506012320518494, -0.37022796273231506, -0.004371644463390112, 0.08851975202560425, 0.05825308337807655, -0.19345708191394806, 0.27037733793258667, 0.3989662826061249, -0.16443663835525513, 0.3900177478790283, -0.24669048190116882, -0.09304435551166534, -0.048559676855802536, -0.555216908454895, -0.09281232953071594, -0.35303810238838196, 0.03752913326025009, 0.061699945479631424, -0.15740908682346344, -0.008675284683704376, -0.23105613887310028, 0.28673526644706726, -0.05119773745536804, -0.19204038381576538, -0.03829734027385712, 0.276344895362854, -0.10705175250768661, 0.18255122005939484, 0.36977851390838623, 0.10312596708536148, -0.027668768540024757, 0.21690376102924347, 0.2346896380186081, 0.020031945779919624, -0.19070997834205627, -0.3423520028591156, 0.055279094725847244, 0.3817589282989502, 0.2457481026649475, -0.04927608370780945, 0.2546383738517761, 0.02619561180472374, 0.1602848470211029, 0.15245293080806732, 0.12596653401851654, 0.14969436824321747, 0.21581684052944183, 0.13810977339744568, -0.013407767750322819, -0.3992486894130707, 0.14361144602298737, 0.21427984535694122, 0.1669178456068039, -1.1160805225372314, -0.14844679832458496, 0.00934184342622757, 0.3081657886505127, -0.16622233390808105, -0.1043003648519516, 0.24409252405166626, -0.11276321858167648, 0.10980222374200821, 0.20538565516471863, -0.3235644996166229, 0.13621985912322998, 0.10376295447349548, -0.06088091805577278, 0.0375736765563488, 0.13626186549663544, 0.17956101894378662, -0.2778284549713135, -0.0033440894912928343, -0.29882940649986267, -0.17356151342391968, 0.2497188001871109, 1.0119158029556274, -0.008588263764977455, -0.07746114581823349, 0.24106326699256897, 0.02989487536251545, -0.01971231773495674, 0.18330013751983643, -0.06175011396408081, -0.06547124683856964, 0.15340693295001984, 0.7167600989341736, -0.008851833641529083, 0.03353775292634964, 0.7302902936935425, -0.265752375125885, -0.0975174605846405, 0.06384779512882233, -0.08393128961324692, 0.3189411461353302, 0.0015350650064647198, -0.11163029074668884, -0.36423611640930176, 0.6328303217887878, -0.05485696345567703, 0.024824656546115875, -0.2563048303127289, -0.02595693990588188, 0.10369791090488434, -0.054712146520614624, -0.030883345752954483, -0.044194966554641724, -0.1160961389541626, 0.25324103236198425, 0.4524971544742584, 0.10014408826828003, -0.12747572362422943, -0.10099397599697113, -0.16275538504123688, 0.22294048964977264, 0.11248867213726044, 0.13094870746135712, 0.15295548737049103, -0.19556498527526855], "sparse_embedding": null}, {"id": "7c032378e1ee4331b0969bed9248056ef0d6182e1d0341694da9622a4b9c9cfc", "content": "Large-Language Models (LLMs) have shifted the paradigm of natural language\ndata processing. However, their black-boxed and probabilistic characteristics\ncan lead to potential risks in the quality of outputs in diverse LLM\napplications. Recent studies have tested Quality Attributes (QAs), such as\nrobustness or fairness, of LLMs by generating adversarial input texts. However,\nexisting studies have limited their coverage of QAs and tasks in LLMs and are\ndifficult to extend. Additionally, these studies have only used one evaluation\nmetric, Attack Success Rate (ASR), to assess the effectiveness of their\napproaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL)\nframework to address these issues by applying Metamorphic Testing (MT)\ntechniques. This approach facilitates the systematic testing of LLM qualities\nby defining Metamorphic Relations (MRs), which serve as modularized evaluation\nmetrics. The METAL framework can automatically generate hundreds of MRs from\ntemplates that cover various QAs and tasks. In addition, we introduced novel\nmetrics that integrate the ASR method into the semantic qualities of text to\nassess the effectiveness of MRs accurately. Through the experiments conducted\nwith three prominent LLMs, we have confirmed that the METAL framework\neffectively evaluates essential QAs on primary LLM tasks and reveals the\nquality risks in LLMs. Moreover, the newly proposed metrics can guide the\noptimal MRs for testing each task and suggest the most effective method for\ngenerating MRs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2312.06056v1", "title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities", "content": "http://arxiv.org/pdf/2312.06056v1", "datetime": "2023-12-11 01:29:19", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting news in the world of Large-Language Models (LLMs)! \n\nRecent studies have introduced the METAL framework, a groundbreaking approach that utilizes Metamorphic Testing techniques to systematically evaluate the quality attributes of LLMs. By defining Metamorphic Relations (MRs) as modularized evaluation metrics, METAL can generate hundreds of MRs covering various qualities and tasks in LLMs.\n\nThe experiments conducted with three major LLMs have demonstrated that the METAL framework effectively identifies essential Quality Attributes, helping to uncover potential risks in LLM outputs. Additionally, novel metrics have been introduced to enhance the assessment accuracy by integrating semantic qualities of text.\n\nCurious to learn more about this innovative approach and its implications for the future of LLM testing? Check out the full study here: http://arxiv.org/abs/2312.06056v1\n\n#LLM #MetamorphicTesting #QualityAttributes #TechInnovation #AI #NLP #ResearchStudy #TechNews", "x": "\ud83d\ude80 Exciting new research alert! Discover how the METAL framework enhances the evaluation of Large-Language Models (LLMs) in diverse applications by utilizing Metamorphic Testing techniques. Find out more at: http://arxiv.org/abs/2312.06056v1 #AI #NLP #LLMs #METALframework #TechResearch", "source_id": "d5a5cafefb4bffd1efa4086305804ff389b1be2e1bda3f9ba42184cd931e4652", "page_number": 1}, "score": null, "embedding": [-0.44862136244773865, -0.003887264523655176, -0.057276513427495956, 0.15216973423957825, -0.12665779888629913, 0.12267513573169708, -0.20374487340450287, 0.20399202406406403, 0.06129244714975357, -0.26022395491600037, 0.04698053002357483, -0.17592871189117432, 0.4306776225566864, 0.2519679069519043, 0.33119329810142517, 0.25601187348365784, -0.02715110220015049, 0.07846935093402863, -0.026606326922774315, 0.1494811475276947, 0.25227147340774536, -0.13151392340660095, 0.19284585118293762, -0.09096179157495499, 0.12788523733615875, 0.1532965451478958, -0.03541148826479912, -0.11152315139770508, -0.46238261461257935, -1.5952022075653076, -0.02616523578763008, -0.041304413229227066, 0.3095153868198395, -0.1953619420528412, -0.23474660515785217, -0.05031308904290199, -0.42230334877967834, 0.18289321660995483, -0.030904730781912804, 0.037286870181560516, 0.1289556324481964, 0.09088652580976486, 0.13258425891399384, -0.1158476322889328, -0.22061686217784882, -0.3923918306827545, -0.1958693414926529, -0.16049157083034515, -0.47200313210487366, -0.019489411264657974, -0.2771731913089752, -0.21967388689517975, 0.11724293231964111, 0.30757448077201843, 0.2910309135913849, 0.13763423264026642, 0.33881640434265137, 0.09455367922782898, 0.04870014637708664, 0.3882448375225067, 0.16814038157463074, 0.20777614414691925, -0.9181529879570007, 0.17757649719715118, 0.018289802595973015, 0.3416176736354828, -0.17162925004959106, 0.08052943646907806, -0.11739727109670639, -0.017163092270493507, -0.21959826350212097, 0.05165405571460724, 0.16252866387367249, 0.5424884557723999, 0.24046766757965088, 0.28676027059555054, 0.13280247151851654, -0.22966888546943665, 0.21031394600868225, 0.2206287980079651, -0.031884919852018356, -0.14476041495800018, 0.08606791496276855, -0.06746204197406769, -0.18096399307250977, -0.02807031385600567, -0.15249444544315338, -0.046320103108882904, 0.06402229517698288, -0.05967709794640541, 0.001066010445356369, -0.1026415303349495, -0.2463945597410202, 0.1275160312652588, -0.04524323344230652, 0.017796827480196953, -0.05297032371163368, -0.14198152720928192, -0.12084195762872696, 0.6389903426170349, -0.2889401316642761, 0.1612221896648407, -0.0909830704331398, -0.16033904254436493, 0.2250562459230423, 0.02709319069981575, 0.016349604353308678, -0.2986266613006592, -0.16366416215896606, -0.04094858095049858, 0.09635350853204727, -0.06743905693292618, -0.1028800755739212, -0.22364453971385956, -0.16131015121936798, 0.06082778796553612, 0.17402711510658264, -0.0735366940498352, -0.0567045584321022, -0.03287400305271149, -0.5323379635810852, 0.27170753479003906, 0.0250942874699831, -0.2791004776954651, 0.19670549035072327, -0.11064424365758896, 0.07099185138940811, 0.33006393909454346, -0.0038227341137826443, 0.09282078593969345, 0.0994979590177536, -0.1250055730342865, -0.35805588960647583, -0.05770702660083771, 0.03372316434979439, -0.08665624260902405, -0.192057803273201, -0.15392406284809113, 0.3040333092212677, 0.34550541639328003, -0.20891731977462769, -0.27178359031677246, 0.011812759563326836, -0.14574463665485382, -0.5783891677856445, 0.6222267150878906, -0.025030503049492836, 0.0556061789393425, -0.3004368841648102, -0.11106814444065094, 0.2533093988895416, 0.2985181212425232, -0.25079816579818726, -0.17437544465065002, 0.33921724557876587, 0.19001276791095734, -0.2347683608531952, 0.28248757123947144, -0.2709367573261261, -0.1251632571220398, -0.1542227417230606, -0.163279727101326, -0.06703043729066849, 0.8640269041061401, -0.23064552247524261, -0.3588159680366516, -0.26586729288101196, 0.10907519608736038, 0.18340179324150085, -0.22069503366947174, 0.40206098556518555, 0.17044080793857574, -0.14483828842639923, 0.047231897711753845, -0.12099836021661758, -0.03999625891447067, -0.463498592376709, -0.2069508582353592, -0.2734498381614685, 0.1075170636177063, 0.12155894935131073, 0.06014729291200638, -0.11223598569631577, 0.2425937056541443, 0.1016821414232254, -0.0667734146118164, 0.03380217030644417, -0.2594144344329834, 0.2386208027601242, 0.009009627625346184, -0.17009103298187256, 0.13742342591285706, -0.01663149707019329, 0.06233225017786026, -0.08581292629241943, -0.18870000541210175, -0.2345420867204666, -0.020986264571547508, -0.04486769810318947, -0.40641096234321594, -0.02204078808426857, 0.1228494718670845, -0.17297086119651794, 0.06434227526187897, 0.1657465249300003, 0.025819027796387672, -0.04139873757958412, -0.06289274245500565, 0.15476343035697937, 0.3816664516925812, -0.15709134936332703, -0.22867052257061005, 0.2790849506855011, 0.16628222167491913, -0.043288569897413254, 0.007618818897753954, 0.33019161224365234, 0.23520956933498383, -0.1005796417593956, 0.35786527395248413, 0.15155693888664246, 0.1141793355345726, -0.369440495967865, -1.3270368576049805, -0.35675379633903503, 0.2381424456834793, 0.3395617604255676, 0.12765797972679138, -0.1594582051038742, -0.16808871924877167, -0.0037175659090280533, 0.3778091371059418, 0.3259049355983734, 0.21823656558990479, 0.2563237249851227, -0.3158321678638458, -0.02286938950419426, 0.0008066649315878749, 0.122217558324337, -0.16744156181812286, 0.01635410077869892, -0.20759396255016327, 0.20940403640270233, -0.2813006043434143, 0.187948077917099, 0.187308669090271, -0.3921899199485779, 0.10743625462055206, -0.13299354910850525, 0.7046514749526978, -0.3877696990966797, -0.05821661278605461, 0.05951734632253647, 0.2791883647441864, 0.40710294246673584, 0.07844474166631699, -0.22857710719108582, 0.3215855658054352, 0.09511825442314148, 0.11994534730911255, 0.18956705927848816, -0.06476302444934845, -0.20755498111248016, -0.27751219272613525, -0.03513190895318985, 0.14460404217243195, -0.7093726992607117, -0.1801828294992447, -0.13207955658435822, 0.2771385908126831, -0.06649079918861389, -0.3730887174606323, 0.3471057415008545, -0.023020828142762184, 0.10888873040676117, 0.5355595946311951, -0.09811671823263168, -0.24592247605323792, -0.030656075105071068, -0.4031168520450592, -0.023042866960167885, -0.0045112972147762775, -0.09913496673107147, 0.2657892405986786, 0.022805169224739075, 0.11871559917926788, -0.14326801896095276, 0.31178534030914307, -0.36148062348365784, -0.0960954949259758, 0.01732868328690529, 0.15422894060611725, -0.2455129325389862, -0.0985843688249588, 0.6128576993942261, -0.04991123080253601, -0.033309876918792725, 0.030209003016352654, 0.2850320637226105, -0.1602962464094162, -0.305959016084671, -0.08342273533344269, 0.013075878843665123, 0.47516709566116333, 0.28362521529197693, 0.07595940679311752, 0.003465761197730899, 0.13880184292793274, 0.09229777008295059, 0.13446101546287537, 0.17195776104927063, 0.1138521209359169, 0.08007735013961792, -0.11915241926908493, -0.15485166013240814, -0.2889847457408905, -0.009953346103429794, -0.019502123817801476, 0.1575019359588623, -1.034592628479004, -0.036411382257938385, -0.11535453796386719, 0.20554587244987488, -0.25872859358787537, -0.1759720891714096, 0.24285177886486053, -0.31861311197280884, 0.02120702527463436, 0.35707589983940125, 0.04504219442605972, 0.286618709564209, 0.3514505624771118, -0.23921696841716766, 0.11382044851779938, -0.19509220123291016, 0.34906238317489624, -0.27083057165145874, 0.2254759818315506, -0.38077595829963684, -0.0007783608161844313, 0.03465202823281288, 1.259720802307129, 0.013095369562506676, 0.008538306690752506, 0.21904443204402924, 0.12711910903453827, 0.12094305455684662, -0.13852918148040771, 0.03591957688331604, 0.17812244594097137, 0.04735651984810829, 0.7525471448898315, 0.15922965109348297, 0.03132542967796326, 0.34794068336486816, -0.08803877234458923, -0.08499506860971451, 0.17350144684314728, 0.041738297790288925, 0.515060305595398, 0.1360466331243515, -0.23893538117408752, -0.12074064463376999, 0.6504127979278564, -0.13676023483276367, 0.13811899721622467, -0.43281999230384827, 0.06191977858543396, 0.123152956366539, -0.1509311944246292, 0.11992807686328888, -0.18447336554527283, -0.06974820047616959, -0.015116470865905285, 0.19650432467460632, 0.0027182961348444223, 0.031427692621946335, -0.13629673421382904, -0.0018106758361682296, 0.09290409088134766, 0.04397806152701378, 0.04129273071885109, 0.365815669298172, -0.09084291756153107], "sparse_embedding": null}, {"id": "e59a93b980c1587952fd591081edb63485c9d8a67a6813d2640ac44585382908", "content": "Medical large language models (LLMs) have gained popularity recently due to\ntheir significant practical utility. However, most existing research focuses on\ngeneral medicine, and there is a need for in-depth study of LLMs in specific\nfields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese\nAnesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions\nhave three aspects: 1) The data, such as utilizing Self-Instruct, acquired from\ncurrent LLMs likely includes inaccuracies. Hypnos implements a cross-filtering\nstrategy to improve the data quality. This strategy involves using one LLM to\nassess the quality of the generated data from another LLM and filtering out the\ndata with low quality. 2) Hypnos employs a general-to-specific training\nstrategy that starts by fine-tuning LLMs using the general medicine data and\nsubsequently improving the fine-tuned LLMs using data specifically from\nAnesthesiology. The general medical data supplement the medical expertise in\nAnesthesiology and enhance the effectiveness of Hypnos' generation. 3) We\nintroduce a standardized benchmark for evaluating medical LLM in\nAnesthesiology. Our benchmark includes both publicly available instances from\nthe Internet and privately obtained cases from the Hospital. Hypnos outperforms\nother medical LLMs in anesthesiology in metrics, GPT-4, and human evaluation on\nthe benchmark dataset.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.02742v1", "title": "Towards Training A Chinese Large Language Model for Anesthesiology", "content": "http://arxiv.org/pdf/2403.02742v1", "datetime": "2024-03-05 07:53:49", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting news in the world of AI and healthcare! Introducing Hypnos, a groundbreaking Chinese Anesthesia model that pushes the boundaries of Large Language Models (LLMs) like never before. \n\n\ud83e\udde0 Hypnos addresses the gap in specialized medical fields by leveraging a unique approach. It enhances data quality by implementing a cross-filtering strategy, utilizes a general-to-specific training method, and introduces a standardized benchmark for evaluating medical LLMs in Anesthesiology.\n\n\ud83d\udcc8 The results speak for themselves - Hypnos outperforms other medical LLMs in anesthesiology in various metrics, including GPT-4 and human evaluation. \n\n\ud83d\udd17 Read more about Hypnos and its innovative contributions in the field of AI and healthcare in the research paper here: http://arxiv.org/abs/2403.02742v1\n\n#AI #HealthTech #LLMs #Anesthesiology #Innovation #HealthcareAI", "x": "\ud83d\ude80 Exciting news in the world of AI and healthcare! Introducing Hypnos, a Chinese Anesthesia model that outperforms other medical LLMs in anesthesiology. Learn about its innovative strategies and impressive results here: http://arxiv.org/abs/2403.02742v1 #AI #HealthTech #LLMs #Anesthesiology \ud83c\udfe5\ud83d\udca1", "source_id": "f889d467faa2a6c93993e1eeac8face161faa08fd2ae7cd382a1cb9b1853bf79", "page_number": 1}, "score": null, "embedding": [-0.16380997002124786, 0.14144715666770935, 0.11296781152486801, -0.2603379786014557, 0.03350844979286194, 0.019038667902350426, -0.14381258189678192, 0.23147737979888916, 0.15179327130317688, -0.2755107879638672, -0.11069023609161377, -0.1862451285123825, 0.08546710759401321, 0.27831658720970154, 0.00786342192441225, -0.011114707216620445, -0.128213033080101, -0.14966532588005066, -0.19773587584495544, 0.09062084555625916, 0.2315567582845688, -0.005986033007502556, 0.17864494025707245, 0.1538153737783432, 0.038075365126132965, -0.23023831844329834, -0.10692600160837173, 0.0011932315537706017, -0.27451977133750916, -1.290595531463623, -0.2741142213344574, -0.239643394947052, 0.6102573275566101, 0.11376385390758514, -0.37129536271095276, 0.16718997061252594, -0.15628217160701752, 0.2968719005584717, -0.09767076373100281, 0.3573473393917084, 0.15246453881263733, 0.06399495899677277, 0.006034447345882654, 0.041988156735897064, -0.1229390874505043, -0.377991646528244, -0.1250763237476349, -0.12994498014450073, -0.24748919904232025, 0.23186036944389343, 0.08725371956825256, -0.133107990026474, -0.01758481189608574, 0.26594915986061096, 0.12493113428354263, -0.08419381827116013, 0.007985681295394897, 0.07838950306177139, -0.2144749015569687, 0.22565396130084991, 0.13654747605323792, 0.36866191029548645, -1.0280745029449463, 0.31942373514175415, 0.04353383556008339, -0.0783529281616211, -0.08455580472946167, 0.04727737605571747, 0.08682528883218765, 0.13732831180095673, -0.038809746503829956, -0.17146600782871246, 0.21413621306419373, 0.3279273509979248, 0.021735653281211853, 0.10976137220859528, 0.17093874514102936, -0.04820314049720764, 0.1509355753660202, -0.022115541622042656, -0.00621413066983223, -0.14311638474464417, -0.0208747498691082, -0.4156600832939148, 0.105892613530159, -0.10937872529029846, -0.015930017456412315, -0.21531766653060913, -0.0566403791308403, -0.17612764239311218, -0.12556859850883484, -0.1118612214922905, -0.2011844515800476, 0.4682956337928772, 0.00899394042789936, -0.037120141088962555, 0.0391807034611702, -0.051348958164453506, -0.34470993280410767, 0.6696066856384277, -0.22737613320350647, 0.09616269916296005, 0.08024222403764725, -0.017850667238235474, 0.40273377299308777, -0.11682465672492981, -0.14152653515338898, -0.15714649856090546, 0.15944227576255798, 0.10264407843351364, 0.21738487482070923, 0.1564919799566269, 0.19897370040416718, -0.09565363824367523, -0.08872242271900177, 0.2228430062532425, 0.47138723731040955, 0.20380425453186035, 0.15679794549942017, 0.14543433487415314, -0.36506783962249756, -0.0335908867418766, 0.010554383508861065, -0.2721579372882843, 0.22036200761795044, -0.1449746936559677, 0.10452241450548172, 0.4277403652667999, 0.14986391365528107, 0.02143029123544693, 0.22608856856822968, 0.061006247997283936, -0.47955432534217834, -0.2563123106956482, 0.025747623294591904, -0.22185255587100983, 0.05056224390864372, -0.16594581305980682, 0.04081086441874504, 0.09076757729053497, -0.17928071320056915, -0.10420156270265579, 0.15836837887763977, -0.22867529094219208, -0.5559748411178589, 0.6233304738998413, -0.18960808217525482, 0.10291288048028946, -0.2881612181663513, -0.16995549201965332, 0.10142232477664948, 0.4232403337955475, -0.24696213006973267, -0.16113770008087158, 0.07636712491512299, 0.013002486899495125, 0.30509689450263977, 0.17695747315883636, -0.3964522182941437, -0.06615053862333298, -0.11306478828191757, -0.05173501372337341, -0.18342755734920502, 0.7024737596511841, 0.06510798633098602, -0.05705684423446655, -0.1631488800048828, 0.18814559280872345, 0.36563000082969666, 0.020371582359075546, 0.22958523035049438, 0.06289450824260712, -0.016740707680583, -0.04188578203320503, 0.07852792739868164, -0.1277296245098114, -0.48346829414367676, -0.04465385153889656, -0.21878579258918762, -0.1212506964802742, 0.30254894495010376, -0.08882597088813782, -0.34844130277633667, 0.13437208533287048, -0.15389609336853027, -0.10401538759469986, 0.0356847420334816, -0.15738238394260406, 0.10417089611291885, 0.17088432610034943, -0.22747085988521576, 0.31304022669792175, 0.10036824643611908, -0.0225540641695261, -0.08443745225667953, -0.33516716957092285, -0.3263337016105652, -0.035074688494205475, 0.1577330082654953, -0.2319086790084839, -0.16410425305366516, -0.13580825924873352, -0.00515696220099926, 0.11880427598953247, -0.02980850078165531, -0.06112363561987877, -0.10068239271640778, 0.017690518870949745, 0.37880849838256836, 0.30438774824142456, -0.30255231261253357, 0.2691407799720764, 0.2989950478076935, -0.19783169031143188, 0.2207709550857544, 0.026854487136006355, 0.2809637188911438, 0.2509591281414032, -0.24802207946777344, 0.24653741717338562, 0.0595087967813015, 0.08088462054729462, -0.29291796684265137, -1.1687960624694824, -0.14028018712997437, -0.02144673280417919, -0.04897769168019295, 0.0822325050830841, -0.25630268454551697, 0.16194403171539307, 0.04835484176874161, 0.09805957973003387, 0.3100586533546448, 0.26333218812942505, 0.16079652309417725, -0.43967556953430176, 0.1929570436477661, -0.17459633946418762, -0.0017291984986513853, 0.28524357080459595, 0.11118929833173752, 0.016939647495746613, 0.15557131171226501, 0.04775447025895119, 0.22684356570243835, 0.21672694385051727, -0.39426830410957336, 0.03727157041430473, -0.35159993171691895, 0.5520157814025879, -0.3522348403930664, 0.26320964097976685, -0.09789780527353287, 0.2503787875175476, 0.12352237105369568, 0.1072239950299263, -0.42648741602897644, 0.1957225799560547, 0.08899199217557907, -0.10043662786483765, 0.06386741250753403, 0.01475844718515873, -0.20776031911373138, -0.2722996175289154, -0.021657418459653854, 0.09666664898395538, -0.30029207468032837, -0.10519571602344513, 0.03469463437795639, -0.19154642522335052, -0.1534886211156845, -0.4904947280883789, 0.12060943245887756, 0.2374715656042099, -0.14929607510566711, 0.08414185792207718, 0.048353563994169235, -0.1563367247581482, -0.06769004464149475, -0.6735883951187134, -0.0701196938753128, -0.3845239579677582, 0.11645804345607758, -0.08508358150720596, -0.07210402190685272, -0.10416397452354431, -0.04814464971423149, 0.053804684430360794, -0.44577279686927795, 0.1968328207731247, 0.05895880237221718, 0.41971859335899353, -0.21273858845233917, -0.06431258469820023, 0.5440725088119507, -0.12515707314014435, 0.10504428297281265, 0.08044181764125824, 0.19203528761863708, -0.06250154972076416, -0.17116166651248932, -0.28994715213775635, 0.027219383046030998, 0.6983681321144104, 0.381621390581131, -0.003106148447841406, 0.3341061770915985, 0.16664378345012665, 0.08724608272314072, 0.19358070194721222, 0.20863112807273865, 0.007277259603142738, -0.1847730576992035, -0.07781147211790085, -0.10248669236898422, -0.2640155851840973, -0.13343095779418945, 0.006998496130108833, 0.10221356153488159, -0.9900136590003967, -0.12027408182621002, -0.20700857043266296, 0.3566543757915497, -0.0893401950597763, -0.24022819101810455, -0.1374678760766983, -0.2377362996339798, 0.2560942471027374, 0.06396600604057312, -0.027384893968701363, 0.24233362078666687, 0.34399527311325073, -0.24625670909881592, -0.072803795337677, -0.06271487474441528, 0.34788647294044495, -0.12288917601108551, 0.29915812611579895, -0.23712660372257233, -0.07033197581768036, -0.05072344094514847, 0.9036880731582642, -0.20687149465084076, -0.015393257141113281, 0.4476386308670044, -0.14071431756019592, -0.09018105268478394, -0.20322629809379578, 0.23868633806705475, 0.18365821242332458, 0.32476696372032166, 0.8764334321022034, -0.09696784615516663, 0.13412001729011536, 0.4224737286567688, -0.4078461229801178, 0.40614110231399536, 0.04330241680145264, 0.15708699822425842, 0.3197101652622223, -0.006727415136992931, -0.09937869012355804, -0.06450597196817398, 0.7363764643669128, -0.1793593019247055, 0.130429208278656, -0.32448023557662964, -0.025727834552526474, 0.1241467222571373, -0.17841359972953796, -0.01074424758553505, 0.12872493267059326, 0.0977337658405304, 0.21861165761947632, 0.2632065713405609, 0.0008160493453033268, -0.10372686386108398, 0.09997820854187012, -0.23871387541294098, 0.07574515789747238, -0.2876109182834625, -0.16384023427963257, 0.3359842300415039, -0.15511758625507355], "sparse_embedding": null}, {"id": "1ef23815fc6dbd4b967b14592e3987b6070ee34ec4401bca56f057b290db1918", "content": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.10049v1", "title": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "content": "http://arxiv.org/pdf/2310.10049v1", "datetime": "2023-10-16 04:17:13", "query": "data quality for LLMs", "linkedin": "\ud83c\udf1f Exciting News in the World of Large Language Models (LLMs)! \ud83c\udf1f\n\nLarge Language Models (LLMs) like ChatGPT, LLaMA, GLM, and PaLM have been making waves with their exceptional performances. However, two key challenges have hindered their widespread adoption: resource-intensive training and the need for vast amounts of high-quality data.\n\nIntroducing FATE-LLM - an industrial-grade federated learning framework designed to tackle these challenges head-on. FATE-LLM enables federated learning for large language models, promotes efficient training through parameter-efficient fine-tuning methods, safeguards intellectual property, and ensures data privacy during training and inference.\n\nExcited to learn more? Dive into the details and explore the code of FATE-LLM at: http://arxiv.org/abs/2310.10049v1\n\nLet's revolutionize the world of LLMs together! \ud83d\ude80\n#LLMs #FederatedLearning #AI #NLP #TechInnovation", "x": "\ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! Introducing FATE-LLM, an industrial-grade federated learning framework designed to tackle challenges faced by LLMs in real-world applications. Learn more about FATE-LLM and its features at: http://arxiv.org/abs/2310.10049v1 #AI #NLP #LLMs #FederatedLearning #TechResearch", "source_id": "2d749a5f8b60dd8c070cbc3661b189a9d1515b11fe9a6e447212902b188aff55", "page_number": 1}, "score": null, "embedding": [-0.23471888899803162, -0.06580410152673721, 0.0028351328801363707, -0.003322715638205409, 0.10198433697223663, -0.03184795752167702, -0.4144800305366516, -0.04468442499637604, 0.2833271324634552, -0.48113536834716797, -0.045940060168504715, -0.05504383519291878, 0.445708304643631, 0.317224383354187, 0.040875013917684555, 0.2695329487323761, -0.13202984631061554, 0.01448968518525362, -0.19532495737075806, -0.09772790968418121, 0.5428891777992249, -0.22791515290737152, -0.25334858894348145, 0.08987190574407578, -0.09636510163545609, 0.08727836608886719, -0.2077840268611908, -0.2158537209033966, -0.2706473767757416, -1.292435884475708, 0.0818934217095375, 0.3498823940753937, 0.05262192711234093, 0.06200699135661125, -0.11852626502513885, 0.11555292457342148, -0.18638277053833008, 0.18734592199325562, -0.04314328730106354, 0.27917006611824036, 0.10347484797239304, 0.18294140696525574, 0.01459482405334711, -0.06666684150695801, 0.14290040731430054, -0.5561235547065735, -0.039885975420475006, 0.03301092982292175, -0.5672246813774109, -0.24791660904884338, -0.058495134115219116, -0.3089756667613983, 0.01139603741466999, 0.18604563176631927, 0.08680161833763123, 0.06238337233662605, 0.05848206579685211, 0.32018476724624634, 0.1965094655752182, -0.05426644906401634, 0.10157336294651031, 0.21212774515151978, -0.945554256439209, 0.31187525391578674, -0.2172578126192093, 0.33264538645744324, -0.06436590850353241, 0.1752127856016159, 0.3356081247329712, 0.18884657323360443, 0.07351221144199371, 0.07159323990345001, 0.0963040441274643, 0.12940151989459991, 0.1785203218460083, 0.10597378760576248, 0.1095452755689621, -0.1758681833744049, 0.0894462838768959, 0.18222272396087646, 0.25929492712020874, -0.19505912065505981, 0.025184636935591698, -0.2724875509738922, -0.39824315905570984, -0.13762538135051727, -0.2037903368473053, -0.3542308211326599, 0.05566848814487457, -0.14887507259845734, -0.11862192302942276, -0.02436291053891182, -0.03866202384233475, 0.2911137342453003, -0.05138249695301056, -0.03365270420908928, 0.3134646415710449, -0.02693874202668667, -0.339369535446167, 0.5788727402687073, 0.03303322196006775, 0.016207203269004822, -0.19068118929862976, -0.12283018231391907, 0.30848437547683716, 0.05414114147424698, -0.1947402060031891, -0.15356026589870453, 0.1584351658821106, -0.103522889316082, -0.08393990993499756, 0.2215818613767624, -0.0001980509696295485, -0.02404475398361683, 0.01345809269696474, -0.007539872080087662, 0.34766948223114014, -0.12510019540786743, -0.18929998576641083, 0.18073798716068268, -0.22021767497062683, 0.16008758544921875, -0.03337505832314491, -0.010922624729573727, 0.0742458775639534, -0.20130863785743713, -0.0744595155119896, 0.46465569734573364, 0.2552105188369751, -0.10687252134084702, 0.16547299921512604, -0.2053270936012268, -0.4342738389968872, -0.30001893639564514, 0.3236221373081207, 0.061770081520080566, 0.18149100244045258, -0.11968722194433212, -0.06813809275627136, 0.13183613121509552, -0.15708933770656586, 0.022967087104916573, 0.32518714666366577, -0.19823913276195526, -0.3653622567653656, 0.6751784682273865, 0.2246687114238739, 0.08969414234161377, -0.5330057740211487, -0.09125078469514847, -0.004196664784103632, 0.12379371374845505, -0.007781706750392914, -0.31808945536613464, 0.30531632900238037, 0.19504685699939728, 0.4546857476234436, 0.33820241689682007, -0.41069433093070984, -0.000631653587333858, -0.2764831781387329, -0.04522736743092537, 0.17097753286361694, 0.805895984172821, -0.10938598215579987, -0.4937453269958496, -0.32376590371131897, 0.01394813321530819, 0.24107623100280762, -0.23834991455078125, 0.10589607059955597, 0.08880776911973953, -0.208647221326828, -0.03497791662812233, -0.13204409182071686, 0.1591816395521164, -0.544393002986908, 0.03261245787143707, -0.29341962933540344, 0.18855972588062286, 0.08810144662857056, -0.2346690148115158, 0.03662467747926712, 0.09100073575973511, -0.20598049461841583, -0.2346654236316681, 0.14915989339351654, -0.3281005620956421, 0.0021950784139335155, -0.3183444142341614, -0.5239880084991455, 0.4658708870410919, 0.18574397265911102, -0.03397975489497185, -0.10095670074224472, -0.4250486195087433, 0.028686126694083214, 0.09368844330310822, 0.11681923270225525, -0.388820081949234, -0.10752081871032715, 0.18903185427188873, -0.05657877027988434, -0.12376592308282852, 0.06361153721809387, -0.13473495841026306, -0.0996645987033844, 0.011659566313028336, 0.07736467570066452, 0.2746437191963196, -0.4863796830177307, 0.027324704453349113, 0.25090333819389343, -0.0027895967941731215, -0.056098371744155884, 0.12942984700202942, 0.40244242548942566, 0.1744505763053894, -0.028215546160936356, 0.17205555737018585, 0.23267462849617004, 0.32219573855400085, -0.2781890034675598, -1.1872144937515259, -0.3044856786727905, 0.06892700493335724, -0.0985829085111618, 0.05789879709482193, -0.21811647713184357, 0.16275550425052643, 0.06373581290245056, 0.22604136168956757, 0.4696057438850403, 0.17617446184158325, -0.08495794236660004, -0.26623213291168213, 0.180009126663208, 0.04450008645653725, -0.10177689790725708, -0.28554588556289673, 0.27226802706718445, -0.25046151876449585, 0.26986929774284363, -0.06967505067586899, 0.0341503769159317, 0.023503677919507027, -0.7677710652351379, -0.008866592310369015, 0.08547654002904892, 0.8110917806625366, -0.2757972478866577, -0.21308983862400055, -0.3741295337677002, -0.023515254259109497, 0.4694640040397644, -0.07956329733133316, -0.36103713512420654, 0.4632072150707245, -0.18915916979312897, -0.10149142146110535, -0.11606848239898682, 0.05309664458036423, -0.10262830555438995, -0.0917782112956047, -0.0653614029288292, -0.04425455257296562, -0.6968377828598022, -0.34151652455329895, 0.09008126705884933, -0.13194328546524048, -0.21457859873771667, -0.4394514262676239, 0.3026154637336731, 0.09003014862537384, 0.08401291072368622, 0.5465739965438843, 0.018821949139237404, -0.16082632541656494, 0.04778467118740082, -0.3026111125946045, 0.2441330999135971, -0.2047506719827652, 0.2146342396736145, 0.09827286005020142, -0.059742558747529984, -0.17683610320091248, -0.0894971564412117, 0.37363201379776, -0.08493426442146301, -0.02181348018348217, 0.018163060769438744, 0.2263944298028946, -0.3971238434314728, 0.09252654016017914, 0.4826383888721466, 0.10544141381978989, -0.07736627757549286, 0.18421845138072968, 0.1834501475095749, -0.03461518883705139, 0.004249093122780323, -0.4309717118740082, 0.0005742149078287184, 0.8566892147064209, 0.3134450316429138, 0.1418558955192566, 0.24987804889678955, -0.06792764365673065, 0.015513100661337376, 0.3038160502910614, 0.2364766150712967, 0.11268583685159683, 0.2431400567293167, -0.01692204363644123, 0.010874317027628422, -0.20453810691833496, 0.02341870218515396, 0.11098416149616241, 0.010459279641509056, -1.0844672918319702, 0.08991669863462448, 0.0002483503776602447, 0.4193617105484009, -0.1389329433441162, -0.02300393022596836, 0.24468347430229187, -0.028210921213030815, 0.1745387762784958, 0.24561730027198792, 0.11114377528429031, 0.1461714804172516, 0.2569650709629059, 0.052274081856012344, 0.1868838518857956, -0.2576254606246948, 0.4068780839443207, -0.07693902403116226, 0.022287094965577126, -0.4334598779678345, -0.05396378040313721, 0.17822779715061188, 0.8697600960731506, -0.001614842563867569, -0.09216853976249695, 0.1528555452823639, -0.104002445936203, 0.2712431848049164, -0.015438300557434559, -0.05174552649259567, -0.06061732769012451, 0.013597223907709122, 0.7298703789710999, -0.14445151388645172, 0.014188777655363083, 0.3589445948600769, -0.01453285850584507, 0.14885276556015015, 0.22537751495838165, 0.1240239068865776, 0.37716954946517944, -0.02400449849665165, 0.01943996548652649, -0.1615355759859085, 0.3993362486362457, 0.1059156209230423, 0.045624054968357086, -0.1861228048801422, -0.12884926795959473, 0.005947148893028498, -0.11846771091222763, -0.19985154271125793, 0.015513921156525612, 0.07457015663385391, 0.08530089259147644, 0.3066895306110382, 0.2147311419248581, -0.051408182829618454, -0.16366207599639893, -0.2423047125339508, 0.1610855609178543, -0.28829270601272583, -0.15261733531951904, 0.15710654854774475, -0.1812065839767456], "sparse_embedding": null}, {"id": "e086c20943b945085fe4dc6e8194d198338bb80cb3d0cc13d67125075c3af1a2", "content": "Making LLMs speak for different, especially minority groups of people, and\ngenerate statements supporting their diverse or even controversial perspectives\nis critical to creating an inclusive environment. However, existing LLMs lack\nsufficient controllability to the stance of their generated content, which\noften contains inconsistent, neutral, or biased statements. In this paper, we\nimprove the controllability of LLMs in generating statements supporting an\nargument the user defined in the prompt. We find that multi-round debates\nbetween two LLMs with opposite stances generate higher-quality and more salient\nstatements for each, which are important training data to improve the\ncontrollability of LLMs. Motivated by this, we develop a novel debate & tuning\n(DEBATUNE) pipeline finetuning LLMs to generate the statements obtained via\ndebate. To examine DEBATUNE, we curate the largest dataset of debate topics so\nfar, which covers 710 controversial topics and corresponding arguments for each\ntopic. Evaluations by the GPT-4 judge with a novel controversy controllability\nmetric show that LLMs' capability of generating diverse perspectives is\nsignificantly improved by DEBATUNE. Moreover, such controllability can be\ngeneralized to unseen topics, generating high-quality statements supporting\ncontroversial arguments.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.10614v2", "title": "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements", "content": "http://arxiv.org/pdf/2402.10614v2", "datetime": "2024-06-07 20:19:09", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Exciting News in AI and NLP Research! \ud83c\udf1f\n\nEnhancing the controllability of Language Models (LLMs) to generate statements supporting diverse and controversial perspectives is crucial for fostering inclusivity. A recent study introduces a novel approach, DEBATUNE, which leverages multi-round debates between LLMs with opposing stances to improve the quality and relevance of generated content.\n\nDiscover how DEBATUNE refines LLMs' ability to produce statements aligned with user-defined arguments by checking out the research paper here: http://arxiv.org/abs/2402.10614v2\n\n#AI #NLP #Inclusivity #Research #DEBATUNE #LLMs #TechInnovation\n\nLet's continue pushing the boundaries of AI for a more inclusive future! \ud83c\udf10\ud83d\udca1", "x": "\ud83c\udf1f Exciting research alert! Improving LLMs' controllability in generating diverse and controversial statements is key for inclusivity. Learn how multi-round debates enhance statement quality and how DEBATUNE refines LLMs in this new paper: http://arxiv.org/abs/2402.10614v2 #AI #NLP #LLMs #InclusivityResearch \ud83e\udd16\ud83d\udcda\ud83d\udd0d", "source_id": "d32151e9800cc7fc474bfd8f99aaf86461865ddfacf4b895c5c937b97f3a3091", "page_number": 1}, "score": null, "embedding": [-0.36887040734291077, 0.09145717322826385, 0.002854397287592292, -0.11141796410083771, -0.12638165056705475, 0.1944492608308792, -0.2796405255794525, 0.08200207352638245, 0.08633682131767273, 0.02875426597893238, -0.10505934059619904, -0.0746864303946495, 0.21298544108867645, 0.18823586404323578, 0.30989646911621094, 0.18985706567764282, 0.006967790890485048, 0.11487244814634323, -0.24388378858566284, 0.10258139669895172, 0.20412039756774902, -0.2333488017320633, 0.13405807316303253, -0.3261685371398926, 0.14953234791755676, -0.0465412363409996, -0.19719532132148743, -0.2592744529247284, -0.08899004757404327, -1.5437864065170288, 0.13822057843208313, 0.10533081740140915, 0.47530925273895264, -0.08331728726625443, -0.13903583586215973, -0.051539015024900436, -0.17009058594703674, 0.18076428771018982, -0.296718955039978, 0.31499597430229187, 0.08674221485853195, -0.06492584198713303, 0.1661497950553894, -0.38823986053466797, -0.25475186109542847, -0.1584635078907013, -0.19841672480106354, -0.039997346699237823, -0.5471550822257996, -0.07517756521701813, -0.0859929546713829, -0.4034268260002136, -0.16676194965839386, -0.0451047345995903, 0.11996538937091827, 0.2167654037475586, 0.02262641303241253, 0.17101691663265228, -0.09949283301830292, 0.1534804403781891, 0.19503772258758545, 0.2432229071855545, -0.9889740347862244, 0.31580850481987, 0.08982684463262558, 0.31520944833755493, -0.12531304359436035, 0.1969241499900818, -0.008653642609715462, 0.3428250551223755, -0.20397408306598663, 0.08447618037462234, 0.2539193630218506, 0.3148452937602997, 0.49939051270484924, 0.19186903536319733, -0.07642191648483276, -0.01872050203382969, 0.060056570917367935, -0.0361393541097641, -0.11575353145599365, 0.13248085975646973, -0.19713832437992096, -0.20485402643680573, -0.02271062694489956, -0.0946030542254448, -0.07899755984544754, -0.08832722902297974, 0.008724392391741276, 0.07757919281721115, 0.04209098964929581, -0.1806403398513794, 0.17971956729888916, 0.18988730013370514, -0.0860154777765274, -0.09455960243940353, 0.15436188876628876, -0.18868692219257355, -0.26649168133735657, 0.7016090154647827, -0.2226283848285675, 0.16926778852939606, -0.011550690978765488, -0.0031876275315880775, 0.008659489452838898, -0.08764344453811646, -0.06197093054652214, -0.26973482966423035, -0.2643846869468689, 0.013020872138440609, 0.09108499437570572, -0.08256343007087708, 0.011188410222530365, -0.12870191037654877, -0.17116761207580566, -0.019103480502963066, 0.3164055645465851, -0.053740307688713074, -0.07951819151639938, -0.0552479550242424, -0.2681100070476532, 0.24142970144748688, -0.2694348096847534, -0.3170769512653351, 0.33391672372817993, -0.01781931146979332, 0.049024518579244614, 0.4766271114349365, 0.17956189811229706, -0.08318521082401276, 0.04230998083949089, -0.056556154042482376, -0.3385016620159149, -0.08611974865198135, 0.06812513619661331, -0.3201165497303009, 0.2030603438615799, 0.05714773014187813, 0.1117684543132782, 0.4342810809612274, -0.0883062556385994, 0.11852027475833893, 0.04951371252536774, -0.2281520813703537, -0.13127310574054718, 0.5654851794242859, -0.337807834148407, -0.06532641500234604, -0.14840924739837646, 0.051920048892498016, -0.07324058562517166, 0.151633620262146, 0.04104088246822357, -0.28254440426826477, 0.43714988231658936, 0.18184103071689606, 0.17743344604969025, 0.2872789204120636, -0.27277010679244995, 0.13929538428783417, 0.08363856375217438, 0.07188662886619568, -0.037882331758737564, 0.9310935139656067, -0.06002911925315857, -0.3893132507801056, -0.34616464376449585, 0.0945785641670227, 0.2590271830558777, -0.03672533854842186, 0.18009380996227264, -0.008126932196319103, 0.10285743325948715, -0.09304866939783096, 0.0654713436961174, -0.13380910456180573, -0.7115526795387268, 0.08295214921236038, -0.11102041602134705, 0.2266184240579605, 0.46554556488990784, -0.21662591397762299, -0.3319582939147949, 0.12363780289888382, -0.30506259202957153, -0.3756169080734253, -0.014642774127423763, -0.15654043853282928, -0.10014954209327698, 0.3547380268573761, -0.3540038466453552, -0.0637291744351387, -0.02854059264063835, 0.08570247143507004, -0.21842768788337708, -0.3050749897956848, 0.009603657759726048, -0.11187738925218582, 0.0006654267199337482, -0.2819414734840393, -0.08256711810827255, 0.09602077305316925, -0.13526977598667145, 0.16582493484020233, -0.028554139658808708, -0.14601172506809235, -0.0639437884092331, -0.07074851542711258, 0.4057297706604004, 0.32632941007614136, -0.2766695022583008, -0.08582774549722672, 0.09710606932640076, -0.025965914130210876, -0.199819415807724, 0.13889680802822113, 0.24468950927257538, 0.21517901122570038, -0.12346281111240387, 0.09889445453882217, 0.007338393479585648, 0.19436390697956085, -0.42323625087738037, -1.2396202087402344, -0.19940119981765747, -0.011449971236288548, 0.15733906626701355, 0.4331549108028412, -0.11431696265935898, 0.14396323263645172, 0.015898501500487328, 0.03229331225156784, 0.3457914888858795, 0.14011414349079132, 0.15320612490177155, -0.3488723933696747, 0.0959826335310936, -0.08034993708133698, 0.03714095801115036, -0.2952132821083069, 0.05572592467069626, -0.0820523053407669, 0.21185360848903656, -0.2711225152015686, 0.09146837145090103, 0.3744669258594513, -0.41668206453323364, 0.09862932562828064, -0.16553665697574615, 0.7870863676071167, -0.05832971632480621, 0.1607951819896698, 0.12096918374300003, 0.02333703637123108, 0.21978624165058136, -0.07167935371398926, -0.4847854971885681, 0.412082701921463, 0.04019796848297119, -0.1742842048406601, -0.14061041176319122, 0.11113779991865158, 0.03681426867842674, 0.22217890620231628, 0.10228756070137024, 0.22650714218616486, -0.7298370599746704, -0.20170219242572784, -0.09725131839513779, -0.02905529923737049, 0.24411337077617645, -0.07376503944396973, 0.07592960447072983, 0.1233125701546669, -0.22247017920017242, 0.36810019612312317, 0.0505145862698555, 0.011762304231524467, 0.18076083064079285, -0.6836208701133728, 0.10680931806564331, -0.06942024827003479, 0.0926695466041565, 0.008363695815205574, -0.06055588275194168, -0.004081773106008768, -0.3082433342933655, 0.03289918974041939, 0.055962201207876205, -0.01880970411002636, -0.09675008058547974, 0.1407151073217392, -0.13308730721473694, 0.05008428916335106, 0.5048133134841919, -0.015986088663339615, 0.34560829401016235, -0.11464592069387436, 0.20364345610141754, -0.08415361493825912, -0.003222069703042507, -0.31628021597862244, -0.17123182117938995, 0.5807591676712036, 0.20412033796310425, 0.17119774222373962, 0.20665690302848816, -0.08456344157457352, -0.06317657977342606, 0.1509062945842743, 0.07786688208580017, 0.35079219937324524, -0.1483805775642395, -0.21749800443649292, 0.016749022528529167, -0.29570475220680237, -0.15948006510734558, -0.16298717260360718, 0.04475091025233269, -0.9369832873344421, -0.10419978201389313, -0.2545456290245056, 0.1215934306383133, -0.03646712377667427, 0.11803173273801804, 0.28809112310409546, -0.1539018750190735, -0.16156907379627228, 0.1753755360841751, 0.06733287870883942, 0.2667873799800873, 0.056407034397125244, -0.0951865017414093, 0.08754141628742218, 0.08031298220157623, 0.5006831288337708, -0.25110092759132385, 0.18278658390045166, -0.2964228689670563, 0.250068724155426, 0.12490501254796982, 0.920996904373169, 0.09507977217435837, 0.07640578597784042, 0.15705212950706482, -0.000902921543456614, 0.27616506814956665, -0.06459115445613861, 0.1112758070230484, -0.1002800241112709, 0.05230638012290001, 0.7529109120368958, -0.15146544575691223, -0.015067108906805515, 0.43552473187446594, -0.17572462558746338, -0.1406632363796234, 0.27159178256988525, -0.01873018406331539, -0.0792597234249115, -0.13960744440555573, 0.21100153028964996, 0.07802397012710571, 0.5526567101478577, -0.062177594751119614, 0.041186802089214325, -0.3820951581001282, 0.15866272151470184, -0.04285528138279915, -0.078436478972435, 0.14610350131988525, -0.2705245614051819, 0.14064504206180573, 0.3188668489456177, 0.027967456728219986, 0.08562915772199631, -0.11570653319358826, 0.10217556357383728, -0.20951391756534576, 0.049924302846193314, 0.04703530669212341, 0.14527994394302368, 0.05389435961842537, -0.020229754969477654], "sparse_embedding": null}, {"id": "916aef1d06cb486b5fbb74ea705493b75ef2447c67aed99b5a56a245ee6d3469", "content": "This paper explores the evolving relationship between clinician trust in\nLLMs, the transformation of data sources from predominantly human-generated to\nAI-generated content, and the subsequent impact on the precision of LLMs and\nclinician competence. One of the primary concerns identified is the potential\nfeedback loop that arises as LLMs become more reliant on their outputs for\nlearning, which may lead to a degradation in output quality and a reduction in\nclinician skills due to decreased engagement with fundamental diagnostic\nprocesses. While theoretical at this stage, this feedback loop poses a\nsignificant challenge as the integration of LLMs in healthcare deepens,\nemphasizing the need for proactive dialogue and strategic measures to ensure\nthe safe and effective use of LLM technology. A key takeaway from our\ninvestigation is the critical role of user expertise and the necessity for a\ndiscerning approach to trusting and validating LLM outputs. The paper\nhighlights how expert users, particularly clinicians, can leverage LLMs to\nenhance productivity by offloading routine tasks while maintaining a critical\noversight to identify and correct potential inaccuracies in AI-generated\ncontent. This balance of trust and skepticism is vital for ensuring that LLMs\naugment rather than undermine the quality of patient care. Moreover, we delve\ninto the potential risks associated with LLMs' self-referential learning loops\nand the deskilling of healthcare professionals. The risk of LLMs operating\nwithin an echo chamber, where AI-generated content feeds into the learning\nalgorithms, threatens the diversity and quality of the data pool, potentially\nentrenching biases and reducing the efficacy of LLMs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.14691v2", "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals", "content": "http://arxiv.org/pdf/2403.14691v2", "datetime": "2024-04-01 05:03:45", "query": "data quality for LLMs", "linkedin": "\ud83d\ude80 Just published! Dive into the complex dynamics between clinician trust in LLMs, evolving data sources, and the impact on precision and competence in healthcare. Explore the potential risks of feedback loops and deskilling, emphasizing the crucial role of proactive dialogue and user expertise. Stay informed with the full paper here: http://arxiv.org/abs/2403.14691v2 #AI #LLMs #Healthcare #TechInnovation \ud83d\udcc8\ud83d\udd0d", "x": "\ud83d\udd0d New research alert! Explore the impact of clinician trust in LLMs on data sources and clinician competence in healthcare. Uncover the risks of feedback loops and deskilling in AI integration. Proactive dialogue is key. Access the full study here: http://arxiv.org/abs/2403.14691v2 #AI #LLMs #healthtech #research", "source_id": "2ed2a02a48f7b78ba97364253e6552711a66e70cf6d530b5180530f4c18ae4ae", "page_number": 1}, "score": null, "embedding": [-0.2518070340156555, -0.05858746916055679, 0.10614371299743652, -0.18186797201633453, -0.029853081330657005, 0.18250782787799835, 0.14087119698524475, 0.24967876076698303, 0.26723378896713257, -0.2877890169620514, 0.0443594828248024, -0.022368615493178368, 0.13885170221328735, 0.38034772872924805, 0.04666551202535629, 0.09941086918115616, -0.2276500165462494, 0.12020758539438248, -0.027586424723267555, 0.20703177154064178, 0.046339284628629684, -0.21312344074249268, 0.2471676766872406, 0.04588284343481064, -0.24269238114356995, -0.1803765892982483, -0.1293284147977829, -0.46440333127975464, -0.609228789806366, -1.281346082687378, 0.12807612121105194, -0.11556136608123779, 0.42000633478164673, -0.015002824366092682, -0.06961296498775482, -0.003321365686133504, -0.14659252762794495, 0.14428389072418213, 0.05662880465388298, 0.06230301409959793, 0.061428654938936234, -0.2710394263267517, 0.1813000738620758, -0.1385529786348343, -0.11879606544971466, -0.3866497874259949, -0.12245931476354599, -0.1855463683605194, -0.4992547035217285, 0.08955943584442139, -0.05579465627670288, -0.08488237857818604, 0.009524919092655182, 0.49817171692848206, 0.18653543293476105, 0.0013196237850934267, 0.0972072035074234, 0.3061710000038147, -0.06158372014760971, 0.44961094856262207, 0.3128186762332916, 0.4154396951198578, -1.110813856124878, 0.18547172844409943, 0.07723486423492432, 0.20664039254188538, -0.23501841723918915, 0.04519964009523392, -0.011569324880838394, 0.10131862014532089, -0.16992564499378204, 0.07801496982574463, 0.21170449256896973, 0.17687828838825226, 0.28681379556655884, 0.5187333822250366, 0.09351308643817902, -0.1453198492527008, 0.3839348256587982, 0.07700087875127792, 0.1434827744960785, 0.1679304540157318, -0.09152716398239136, -0.25754299759864807, -0.261263906955719, -0.15784777700901031, -0.05286688357591629, -0.530547022819519, 0.05540873482823372, -0.14118973910808563, 0.020922737196087837, -0.0903145894408226, -0.5106379389762878, 0.3413289487361908, -0.015614707954227924, -0.26671481132507324, -0.1579207479953766, 0.21263079345226288, -0.4299061894416809, 0.35071200132369995, -0.31596073508262634, -0.02088734321296215, 0.07088849693536758, 0.02136639691889286, 0.2067529708147049, -0.2515471577644348, 0.04777080565690994, -0.03608604520559311, -0.009790644980967045, 0.059018004685640335, -0.008879536762833595, 0.10214201360940933, 0.13570377230644226, 0.013020950369536877, -0.05598125606775284, 0.43320780992507935, 0.282986581325531, 0.2537204623222351, -0.015313057228922844, -0.10305587947368622, -0.1582367718219757, 0.33010247349739075, -0.004914107732474804, -0.1445455551147461, 0.048371098935604095, -0.16675524413585663, 0.11700501292943954, 0.533988356590271, -0.05232562869787216, -0.19932270050048828, 0.16569742560386658, -0.07400158047676086, -0.2844865918159485, -0.024997349828481674, 0.13210873305797577, -0.2324085682630539, -0.1493828147649765, -0.35034501552581787, 0.09358419477939606, 0.34812670946121216, 0.030575549229979515, 0.12586280703544617, -0.04640667513012886, -0.22592315077781677, -0.4522656500339508, 0.6444905996322632, 0.21379868686199188, -0.09589323401451111, -0.2518956661224365, -0.12719638645648956, 0.2285356968641281, 0.2643805742263794, -0.26121005415916443, 0.06387145817279816, 0.3028562068939209, 0.09304405748844147, 0.10959897935390472, 0.20209939777851105, -0.3779467046260834, 0.06979778409004211, 0.06374819576740265, -0.30974406003952026, -0.3375333845615387, 0.7220012545585632, -0.051542919129133224, -0.3338749408721924, -0.20841924846172333, -0.04503070190548897, 0.36470845341682434, 0.038407102227211, 0.24342894554138184, -0.02782144583761692, -0.00537774246186018, -0.00794227421283722, 0.052532654255628586, -0.02154211327433586, -0.6464229822158813, -0.10243220627307892, 0.14369647204875946, 0.15047597885131836, 0.1511516273021698, -0.09560969471931458, -0.19143711030483246, 0.3107706606388092, 0.009087185375392437, 0.07222385704517365, 0.010453972965478897, -0.48464351892471313, 0.24388709664344788, 0.08918525278568268, -0.10357686132192612, 0.24288217723369598, -0.19622795283794403, -0.014363428577780724, -0.07088535279035568, -0.1377003788948059, -0.39439812302589417, -0.07309695333242416, -0.1291951835155487, 0.0480392687022686, -0.22482867538928986, -0.017415806651115417, 0.034698449075222015, 0.04865758493542671, -0.023139426484704018, 0.02809840440750122, -0.019234459847211838, 0.16083887219429016, 0.39719802141189575, 0.4836030900478363, -0.0890144556760788, 0.18081586062908173, 0.29030364751815796, -0.2455180138349533, -0.28821662068367004, 0.0026782811619341373, 0.20544114708900452, 0.10968148708343506, -0.20646485686302185, 0.27047473192214966, 0.2825499475002289, 0.5716554522514343, -0.16864924132823944, -1.1638249158859253, -0.2797439694404602, 0.006200277246534824, 0.26968568563461304, 0.15110066533088684, -0.20108160376548767, 0.07956377416849136, 0.08931504189968109, 0.011097213253378868, 0.13222193717956543, 0.4864805340766907, 0.2964116930961609, -0.41650429368019104, 0.10931681096553802, -0.061325185000896454, 0.029353197664022446, -0.07556752860546112, 0.03276454657316208, -0.21501967310905457, 0.18285346031188965, -0.39573249220848083, 0.2648495137691498, 0.17248640954494476, -0.7858923077583313, 0.1220337375998497, -0.1788361370563507, 0.8707525134086609, -0.20771047472953796, -0.16986557841300964, 0.24891571700572968, 0.033234287053346634, 0.3063076436519623, -0.2975478768348694, -0.653447687625885, 0.4060646593570709, 0.0021879300475120544, 0.2559760808944702, 0.1330680549144745, -0.2731839716434479, -0.1546509563922882, -0.19141505658626556, -0.14977037906646729, -0.09215937554836273, -0.42881345748901367, -0.36038273572921753, -0.08223818242549896, 0.03414744511246681, 0.21137289702892303, -0.31038975715637207, 0.23745891451835632, 0.09307728707790375, -0.04434053227305412, 0.14422717690467834, -0.0652136355638504, -0.14447665214538574, 0.050647955387830734, -0.25369930267333984, 0.3426980674266815, -0.11492098867893219, -0.10108304023742676, 0.057451605796813965, -0.04194997623562813, -0.0157037153840065, -0.398176908493042, 0.2958295941352844, -0.528755247592926, 0.07265359163284302, 0.35507437586784363, 0.4643683433532715, -0.28560107946395874, -0.07659979164600372, 0.6712638139724731, -0.25018253922462463, 0.09610738605260849, -0.039259519428014755, -0.04625380411744118, -0.1149473562836647, -0.244998499751091, -0.41565704345703125, -0.08588876575231552, 0.5099712610244751, 0.14051014184951782, -0.12273039668798447, 0.31674689054489136, 0.13161823153495789, -0.011019853875041008, 0.07285362482070923, 0.2670469880104065, 0.1903948038816452, 0.0070675089955329895, 0.03885917738080025, -0.1011144369840622, -0.5308569073677063, -0.3334164321422577, 0.06511082500219345, 0.010369870811700821, -1.1088244915008545, -0.029426276683807373, -0.37540772557258606, 0.4352754056453705, -0.3385220468044281, 0.03399287164211273, 0.041889823973178864, -0.11071610450744629, 0.08818167448043823, 0.11623768508434296, 0.1724366992712021, 0.04086139798164368, 0.11452925205230713, -0.06599119305610657, 0.05336446687579155, 0.1316961795091629, 0.46918410062789917, -0.3435412347316742, 0.17259199917316437, -0.27776673436164856, 0.024919098243117332, 0.18506336212158203, 0.9195558428764343, -0.15092763304710388, 0.019901221618056297, 0.3181740343570709, -0.1267983466386795, 0.19063153862953186, 0.14375674724578857, -0.06807908415794373, -0.07312613725662231, -0.0016228709137067199, 0.5585509538650513, 0.06730280071496964, 0.3197465240955353, 0.22979600727558136, -0.4073641896247864, -0.2030777484178543, 0.10233138501644135, -0.03843328356742859, 0.5010138750076294, -0.06796816736459732, 0.1392771452665329, 0.09406180679798126, 0.6356948018074036, -0.21131739020347595, 0.04351084306836128, -0.2958413064479828, 0.12066145241260529, -0.05059704929590225, 0.02039801888167858, -0.07384605705738068, 0.01615607738494873, 0.11287011206150055, 0.16519539058208466, 0.30307286977767944, 0.18981710076332092, -0.2607223093509674, 0.13891461491584778, -0.24804291129112244, 0.21121594309806824, -0.1102912425994873, 0.13835780322551727, 0.15299531817436218, -0.2904050648212433], "sparse_embedding": null}, {"id": "be9144b6323f1bb4e1c337a67266379297875b771915a34d4cc4a740aa1c6a6d", "content": "This research focuses on how Large Language Models (LLMs) can help with path\nplanning for mobile embodied agents such as robots, in a human-in-the-loop and\ninteractive manner. A novel framework named LLM A*, aims to leverage the\ncommonsense of LLMs, and the utility-optimal A* is proposed to facilitate\nfew-shot near-optimal path planning. Prompts are used to 1) provide LLMs with\nessential information like environment, cost, heuristics, etc.; 2) communicate\nhuman feedback to LLMs on intermediate planning results. This makes the whole\npath planning process a `white box' and human feedback guides LLM A* to\nconverge quickly compared to other data-driven methods such as reinforcement\nlearning-based (RL) path planning. In addition, it makes code-free path\nplanning practical, henceforth promoting the inclusiveness of artificial\nintelligence techniques. Comparative analysis against A* and RL shows that LLM\nA* is more efficient in terms of search space and achieves an on-a-par path\nwith A* and a better path than RL. The interactive nature of LLM A* also makes\nit a promising tool for deployment in collaborative human-robot tasks.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2312.01797v1", "title": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics", "content": "http://arxiv.org/pdf/2312.01797v1", "datetime": "2023-12-04 10:37:58", "query": "human feedback LLMs", "linkedin": "\ud83d\ude80 Exciting News in AI Research! \ud83e\udd16\n\nDelving into the realm of Large Language Models (LLMs), a groundbreaking study unveils the potential of LLMs in enhancing path planning for mobile robots. The introduction of LLM A* showcases a novel framework that merges LLMs' commonsense with the efficiency of A* to revolutionize few-shot near-optimal path planning. By integrating human feedback through prompts, LLM A* emerges as a 'white box' solution, outperforming RL-based methods and streamlining the convergence process.\n\nDiscover more about this cutting-edge research and the promising implications for collaborative human-robot tasks: [Read the full study here!](http://arxiv.org/abs/2312.01797v1)\n\n#AI #LLM #PathPlanning #Robotics #Research #Innovation #TechNews", "x": "Exciting research on leveraging Large Language Models (LLMs) for path planning in robots! The novel LLM A* framework combines LLM commonsense and A* for efficient few-shot path planning. Human feedback guides LLM A* to quick convergence, outperforming RL methods. Learn more at: http://arxiv.org/abs/2312.01797v1 #AI #LLMs #Robotics", "source_id": "e10c1736608e82901db135fb6ef34b6b4d3c5499e3e4182b4f26f62c4a119af8", "page_number": 1}, "score": null, "embedding": [-0.3949567973613739, -0.23553083837032318, 0.18434280157089233, 0.025874063372612, -0.24552173912525177, 0.1061396673321724, -0.2992689311504364, 0.2552465796470642, 0.20540957152843475, -0.13467822968959808, 0.17907243967056274, 0.08669358491897583, 0.3607248067855835, 0.44890326261520386, 0.2616037130355835, 0.1342865526676178, -0.27814000844955444, 0.2963617146015167, 0.09306619316339493, -0.05870657414197922, 0.388121098279953, -0.1188812404870987, 0.19556021690368652, -0.26746097207069397, -0.00806161854416132, 0.010768583044409752, -0.232258141040802, -0.23079919815063477, -0.34982240200042725, -1.1410443782806396, 0.26202821731567383, 0.06280434876680374, 0.28391823172569275, 0.021311435848474503, -0.2593673765659332, 0.23178799450397491, -0.2246152013540268, 0.32094627618789673, 0.02185223065316677, 0.12318318337202072, -0.11183243989944458, -0.010220292955636978, 0.19676333665847778, -0.039741165935993195, -0.05785534158349037, -0.4809769093990326, -0.07485683262348175, -0.22879572212696075, -0.6787469387054443, -0.3322455585002899, -0.12782461941242218, -0.30592188239097595, -0.037835780531167984, 0.03551770746707916, 0.2385873943567276, 0.19520193338394165, 0.08645451068878174, 0.41353631019592285, 0.1649874895811081, 0.037501562386751175, 0.32643401622772217, 0.2991939187049866, -0.8846519589424133, 0.4822888970375061, -0.03271324932575226, 0.17719076573848724, -0.17303849756717682, -0.004038310144096613, 0.22727161645889282, 0.21686136722564697, -0.21760058403015137, 0.014029320329427719, 0.2867414653301239, 0.23579718172550201, 0.2523062825202942, -0.06742957979440689, -0.009713810868561268, -0.3253570795059204, 0.18338827788829803, 0.009972368367016315, -0.014502221718430519, -0.005604750011116266, -0.27345335483551025, -0.2262173742055893, -0.1944892257452011, -0.024245277047157288, -0.1366778463125229, -0.35786816477775574, 0.06505866348743439, 0.09425715357065201, -0.06238890811800957, -0.06411691755056381, -0.2783433496952057, 0.32959985733032227, -0.14845620095729828, -0.12376266717910767, 0.04430834576487541, -0.08189969509840012, -0.45860767364501953, 0.7390052080154419, 0.011713901534676552, 0.21982213854789734, 0.09687406569719315, -0.016662968322634697, 0.14824122190475464, 0.04673595353960991, -0.28957080841064453, -0.2706943154335022, -0.20240923762321472, 0.21538186073303223, -0.13442489504814148, 0.02056352235376835, 0.030080130323767662, 0.029860222712159157, 0.103568434715271, 0.06447185575962067, 0.4085894823074341, 0.21201717853546143, -0.029353134334087372, -0.05386512354016304, -0.13626228272914886, 0.029254630208015442, 0.12307892739772797, -0.23078222572803497, -0.09217657148838043, -0.22199977934360504, 0.23518002033233643, 0.2927612066268921, -0.07537197321653366, 0.08130913227796555, 0.3609660267829895, -0.22627709805965424, -0.3377774655818939, -0.05957958102226257, 0.1274692416191101, 0.031875014305114746, 0.09366916120052338, -0.23936855792999268, -0.027363434433937073, 0.13338561356067657, -0.07132365554571152, 0.2447623908519745, 0.07552704215049744, -0.4809505045413971, -0.16428832709789276, 0.3891699016094208, 0.12902751564979553, -0.1899598240852356, -0.27303096652030945, -0.08302658796310425, 0.13000506162643433, 0.034746650606393814, 0.03321348875761032, -0.3068395256996155, -0.00300599099136889, 0.2066546231508255, 0.2541554570198059, 0.16861963272094727, -0.47334837913513184, 0.2033594697713852, -0.09176581352949142, -0.1386784017086029, -0.1423080414533615, 0.8405689001083374, -0.21771769225597382, -0.48931363224983215, -0.31702709197998047, -0.11040058732032776, 0.2958019971847534, -0.1292046308517456, 0.05407245457172394, -0.029202209785580635, -0.12526538968086243, -0.000817956926766783, 0.03902749717235565, 0.05031893029808998, -0.8331426978111267, 0.07616614550352097, -0.2912718951702118, 0.14543060958385468, -0.036190152168273926, -0.11736041307449341, 0.05978057160973549, -0.0542314313352108, -0.015194643288850784, 0.07055484503507614, -0.10985813289880753, -0.3601694405078888, 0.15513622760772705, -0.03260999545454979, -0.0732879489660263, 0.3866632580757141, 0.23975634574890137, -0.09531449526548386, -0.29141461849212646, -0.12793169915676117, 0.01522593293339014, -0.1278664916753769, -0.05098411440849304, -0.17665842175483704, -0.1120697557926178, 0.05312957242131233, -0.12556523084640503, 0.017993493005633354, -0.019376641139388084, -0.27328380942344666, -0.15763986110687256, -0.08396769315004349, 0.2523956000804901, 0.5045826435089111, -0.4545186758041382, -0.05030516907572746, 0.5068002939224243, -0.30088770389556885, -0.016919787973165512, 0.11069957166910172, 0.07197614014148712, -0.009485336020588875, 0.05723421648144722, 0.1575850546360016, 0.05279058963060379, 0.35572949051856995, -0.09839417785406113, -1.002472996711731, -0.0911717340350151, 0.09711624681949615, 0.13957449793815613, 0.03183189034461975, -0.37219271063804626, 0.06045054271817207, -0.10092616081237793, 0.0033522939775139093, 0.2766117453575134, 0.529574990272522, -0.09217160195112228, -0.13737830519676208, 0.363034725189209, 0.017316553741693497, 0.204721599817276, -0.12653127312660217, 0.121554896235466, -0.24872393906116486, 0.3957911431789398, -0.033153779804706573, -0.03444309160113335, 0.10988829284906387, -0.6810196042060852, 0.00788153987377882, -0.10327368974685669, 0.8803465366363525, -0.23149870336055756, 0.09279444813728333, 0.049122404307127, 0.07462507486343384, 0.3928163945674896, -0.0871531218290329, -0.3869229555130005, 0.126583993434906, -0.00699169747531414, 0.3333856761455536, -0.336398184299469, 0.16670583188533783, -0.1846081018447876, -0.447525292634964, -0.05856843665242195, 0.0065864017233252525, -0.42471033334732056, -0.5737501382827759, -0.041948843747377396, -0.16048583388328552, -0.059542786329984665, -0.03671295940876007, -0.04322485253214836, 0.05976005643606186, 0.03295249119400978, 0.13110408186912537, -0.22644445300102234, -0.22621023654937744, 0.06011711806058884, -0.08336257189512253, 0.005518075544387102, -0.2688473165035248, 0.22256338596343994, 0.1519278585910797, -0.041168540716171265, -0.13226117193698883, 0.041995495557785034, 0.35961785912513733, -0.3130528926849365, 0.24197155237197876, 0.07258086651563644, 0.1739960014820099, -0.09328492730855942, 0.07074447721242905, 0.647183895111084, 0.24834094941616058, 0.15250566601753235, 0.2695121467113495, -0.08361627161502838, 0.08355963975191116, 0.15511317551136017, -0.3974820673465729, -0.09631286561489105, 0.48584169149398804, 0.1405295431613922, 0.023536834865808487, 0.16690322756767273, 0.19892339408397675, -0.23475807905197144, 0.26181337237358093, -0.15950559079647064, 0.05525290593504906, 0.2726908326148987, 0.07898025214672089, -0.1661866158246994, -0.17062072455883026, -0.10346418619155884, 0.06880124658346176, -0.014868270605802536, -1.1613825559616089, 0.04551360011100769, 8.608818461652845e-06, 0.35929638147354126, -0.40369081497192383, 0.16170455515384674, 0.26367589831352234, -0.16338607668876648, 0.2705424726009369, 0.20503094792366028, -0.0018234038725495338, 0.2981172800064087, 0.17017178237438202, 0.15358085930347443, 0.4003848731517792, -0.4098375141620636, 0.45678916573524475, -0.11320246756076813, 0.0968550443649292, -0.6161516308784485, -0.0830458402633667, 0.2141391783952713, 1.1821215152740479, -0.24423879384994507, 0.21224097907543182, 0.3209899961948395, 0.02731393091380596, -0.1008790135383606, 0.1592094600200653, -0.1276208758354187, -0.10336577147245407, 0.19556181132793427, 0.6745429635047913, -0.30444467067718506, 0.20795677602291107, 0.2800484597682953, -0.1548764705657959, 0.2532949447631836, 0.12833821773529053, 0.0695146843791008, 0.12766623497009277, -0.04270101338624954, 0.38263049721717834, -0.04359832778573036, 0.4415036737918854, 0.16368915140628815, -0.07836108654737473, -0.2354167103767395, -0.05223318934440613, 0.13639479875564575, -0.12969262897968292, 0.016704777255654335, -0.24987269937992096, -0.12739218771457672, 0.1949203461408615, 0.12197566032409668, 0.2306389957666397, -0.3517530560493469, -0.3878192603588104, -0.18027587234973907, 0.16662734746932983, -0.1364077478647232, 0.3814350366592407, -0.11862926930189133, -0.026246653869748116], "sparse_embedding": null}, {"id": "8b58607e712ea934d601ff2b19265e37cfd367434e6f21591cb2a9b4f6a2d8b9", "content": "In recent years, Large Language Models (LLMs) have witnessed a remarkable\nsurge in prevalence, altering the landscape of natural language processing and\nmachine learning. One key factor in improving the performance of LLMs is\nalignment with humans achieved with Reinforcement Learning from Human Feedback\n(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies\nare investigating the replacement of human feedback with feedback from other\nLLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the\nbiases that come along with evaluating LLMs with other LLMs and take a closer\nlook into verbosity bias -- a bias where LLMs sometimes prefer more verbose\nanswers even if they have similar qualities. We see that in our problem\nsetting, GPT-4 prefers longer answers more than humans. We also propose a\nmetric to measure this bias.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.10076v1", "title": "Verbosity Bias in Preference Labeling by Large Language Models", "content": "http://arxiv.org/pdf/2310.10076v1", "datetime": "2023-10-16 05:19:02", "query": "human feedback LLMs", "linkedin": "\ud83d\ude80 Exciting advancements in Large Language Models (LLMs)! \ud83e\udd16\ud83e\udde0\n\nThe rise of LLMs has reshaped the field of natural language processing and machine learning, with Reinforcement Learning from Human Feedback (RLHF) playing a crucial role in enhancing their performance. Recent studies are delving into the realm of Reinforcement Learning from AI Feedback (RLAIF), exploring how LLMs can learn from one another.\n\n\ud83d\udd0d However, it's essential to consider biases that may arise when evaluating LLMs with other LLMs. A recent study highlighted the verbosity bias, where LLMs tend to favor longer answers, even if shorter responses are equally effective. For example, GPT-4 has shown a preference for lengthier outputs compared to human preferences.\n\nCurious to learn more about this bias and its implications? Check out the full study here: http://arxiv.org/abs/2310.10076v1\n\n#AI #NLP #LLMs #MachineLearning #RLHF #RLAIF #BiasInAI #TechResearch #GPT4 #Bard", "x": "\"\ud83e\udd16\ud83d\udd0d Exciting developments in the world of Large Language Models (LLMs)! Learn how Reinforcement Learning from AI Feedback is shaping the future of LLMs like GPT-4 and Bard. Discover insights on biases in evaluating LLMs and delve into the verbosity bias phenomenon. Check out the study here: http://arxiv.org/abs/2310.10076v1 #AI #NLP #LLMs\"", "source_id": "2cae5b65377ca9716ba6d2c019ad3f04c47ecc1b9f6c8e0ad3812adc1a3a673a", "page_number": 1}, "score": null, "embedding": [-0.26564982533454895, -0.17986281216144562, 0.06986391544342041, 0.03167108818888664, 0.0004490143619477749, 0.046978507190942764, -0.14880099892616272, 0.185377836227417, 0.22826716303825378, -0.25978678464889526, 0.17175379395484924, -0.05530812591314316, 0.034159574657678604, 0.3661377429962158, 0.19100619852542877, 0.09885506331920624, -0.260886013507843, 0.018353790044784546, -0.24642729759216309, -0.0789932906627655, 0.08982774615287781, -0.27733170986175537, 0.1478731632232666, -0.05294906347990036, -0.19702580571174622, -0.10130935162305832, -0.3631778359413147, -0.3189372718334198, -0.2549842298030853, -1.4175944328308105, 0.1879751980304718, 0.1163502037525177, 0.18149931728839874, -0.1327683925628662, -0.15029440820217133, 0.05948120728135109, -0.21185550093650818, 0.0422457791864872, -0.08086187392473221, 0.19507743418216705, -0.0017930414760485291, -0.03000626526772976, 0.017783960327506065, -0.18158181011676788, -0.02834833413362503, -0.2126038670539856, -0.2844342589378357, -0.1450147032737732, -0.7485988140106201, -0.03070482239127159, -0.13278740644454956, -0.2919284701347351, 0.09683456271886826, 0.25991949439048767, 0.2389773279428482, 0.2104378640651703, 0.3020280599594116, 0.3831726014614105, 0.07690560817718506, -0.0400690995156765, 0.22016550600528717, 0.3044167459011078, -0.9849215745925903, 0.46165236830711365, -0.1164047047495842, 0.28317704796791077, -0.02930801920592785, -0.016739564016461372, 0.013669684529304504, 0.2214251160621643, -0.2617320120334625, 0.2749296724796295, 0.2832792401313782, 0.15499211847782135, 0.12343092262744904, 0.36088916659355164, 0.19071921706199646, -0.20764635503292084, 0.35740169882774353, 0.08858225494623184, 0.2612524628639221, 0.06459884345531464, -0.12549203634262085, -0.4559553861618042, -0.10106305778026581, -0.24047209322452545, -0.08300343155860901, -0.02976500801742077, 0.0036813057959079742, 0.09100055694580078, 0.16046540439128876, -0.0432957224547863, -0.11414818465709686, 0.39438357949256897, -0.003353852080181241, 0.05624568834900856, 0.11049207299947739, -0.04008151963353157, -0.35899657011032104, 0.5224817991256714, -0.09265728294849396, 0.19631989300251007, -0.1624356359243393, 0.3372056186199188, 0.43097278475761414, 0.0024342851247638464, -0.2248726338148117, -0.045255620032548904, -0.23735730350017548, -0.08563292026519775, 0.027021974325180054, -0.05626912787556648, 0.211237370967865, 0.018047425895929337, 0.02976626344025135, -0.05030536651611328, 0.44281768798828125, 0.0175328329205513, 0.11530229449272156, 0.004579614382237196, -0.34072333574295044, 0.24673393368721008, 0.0015278207138180733, -0.29983243346214294, 0.10385942459106445, -0.10743624716997147, 0.15458627045154572, 0.4978378415107727, 0.04536037519574165, -0.07385173439979553, 0.2879686653614044, -0.39057058095932007, -0.3180847465991974, -0.2593521475791931, 0.1710381656885147, 0.14042098820209503, -0.06041542813181877, -0.28756505250930786, 0.19922497868537903, 0.2918596863746643, -0.1286466419696808, 0.0026980703696608543, 0.01685933768749237, -0.17117992043495178, -0.36556193232536316, 0.5288516283035278, -0.08313052356243134, -0.028871027752757072, -0.20428156852722168, 0.09325606375932693, 0.07089913636445999, 0.19972002506256104, -0.24594557285308838, -0.13588188588619232, 0.08422690629959106, 0.2840062081813812, 0.058843716979026794, 0.09022030234336853, -0.2938976585865021, 0.1077198013663292, 0.03190581500530243, -0.09428004175424576, -0.26756253838539124, 0.5941470861434937, 0.014254225417971611, -0.23434428870677948, -0.4407464265823364, -0.16815510392189026, 0.3227781653404236, 0.03944261744618416, 0.2803882956504822, -0.07662298530340195, -0.2755354642868042, 0.102876216173172, -0.12064819037914276, 0.05777022987604141, -0.6911365985870361, -0.0703849047422409, -0.16354399919509888, 0.19648431241512299, 0.29495424032211304, -0.16023117303848267, -0.07638665288686752, 0.07128274440765381, 0.04066501930356026, -0.18969124555587769, -0.05802367627620697, -0.4794169068336487, 0.1585424691438675, -0.04428675025701523, -0.28737902641296387, 0.15074287354946136, 0.061096519231796265, -0.05460597202181816, 0.03433453291654587, -0.33181333541870117, 0.00016359818982891738, -0.0705249160528183, -0.05735231935977936, 0.04187750443816185, -0.3057155907154083, 0.04467383399605751, 0.01769280806183815, -0.20512525737285614, 0.16805899143218994, 0.028924264013767242, -0.024430476129055023, 0.004456714261323214, 0.5206289291381836, 0.20836734771728516, -0.07723740488290787, -0.029515642672777176, 0.28285282850265503, 0.018229329958558083, -0.21245534718036652, 0.0770733430981636, 0.22441646456718445, 0.16716474294662476, -0.06978002190589905, 0.20073463022708893, 0.18654590845108032, 0.5083881616592407, -0.2099182903766632, -1.1188535690307617, -0.30196890234947205, 0.0016411484684795141, 0.21751649677753448, 0.3179209530353546, -0.3722465932369232, 0.21032926440238953, 0.2002030462026596, 0.2980383038520813, 0.5897817015647888, 0.07925243675708771, 0.15614086389541626, -0.21808183193206787, 0.1028522178530693, 0.1949475109577179, 0.2374335676431656, -0.20648248493671417, 0.24751363694667816, -0.3429558277130127, 0.19520564377307892, -0.39571794867515564, 0.1676093488931656, 0.07082001119852066, -0.5636262893676758, 0.19696547091007233, -0.25748902559280396, 0.7758721113204956, -0.20213192701339722, -0.036184947937726974, -0.02495582029223442, -0.020899934694170952, 0.42395228147506714, -0.08682313561439514, -0.4830128252506256, 0.5053815841674805, 0.24385270476341248, 0.14889149367809296, -0.00202290085144341, -0.15871746838092804, -0.09650127589702606, -0.21220046281814575, 0.05057156831026077, 0.09099233150482178, -0.467545747756958, -0.4038674235343933, 0.18621478974819183, 0.010760840028524399, 0.0045388564467430115, -0.5571538209915161, 0.11928176879882812, -0.07782601565122604, -0.14765487611293793, 0.14647668600082397, 0.009259543381631374, -0.18531587719917297, -0.12482081353664398, -0.5598397850990295, 0.11846251785755157, -0.15888676047325134, 0.14178703725337982, 0.35105857253074646, -0.165448397397995, -0.2568800449371338, -0.15816055238246918, 0.1726824790239334, -0.203876793384552, 0.10233921557664871, 0.06845536828041077, 0.28009822964668274, 0.21648618578910828, -0.09366519749164581, 0.5198644995689392, -0.04731001704931259, 0.010662466287612915, -0.04563611373305321, 0.07617384195327759, -0.002045617438852787, -0.18328627943992615, -0.4418128430843353, -0.1024441197514534, 0.5834807753562927, 0.0947960764169693, 0.03542166203260422, 0.04598623886704445, 0.16238299012184143, -0.12259726971387863, 0.06682842969894409, 0.010420277714729309, 0.20801077783107758, 0.08936862647533417, 0.10585496574640274, 0.025087634101510048, -0.3762770891189575, -0.27431991696357727, -0.053986310958862305, 0.13260871171951294, -1.05869460105896, -0.01800001785159111, -0.07280825823545456, 0.19075332581996918, -0.09175378829240799, 0.10131732374429703, 0.05010535940527916, -0.17441654205322266, 0.18248377740383148, -0.017167162150144577, -0.09440885484218597, 0.30495697259902954, 0.18324090540409088, -0.0793851986527443, 0.18262651562690735, -0.07953903079032898, 0.43138572573661804, -0.27009087800979614, 0.16707433760166168, -0.4652969241142273, -0.11517467349767685, 0.46647873520851135, 1.0566859245300293, -0.06388544291257858, 0.07940548658370972, 0.2176354080438614, -0.11048642545938492, -0.15869615972042084, -0.10256870836019516, -0.04479029402136803, -0.28913629055023193, 0.18117569386959076, 0.5723117589950562, -0.060004495084285736, 0.12300953269004822, 0.6157464981079102, -0.2692245543003082, -0.13998548686504364, 0.2979501187801361, 0.24402114748954773, 0.28836962580680847, 0.07958530634641647, 0.12345103174448013, 0.05070622265338898, 0.5715944170951843, 0.1816079020500183, 0.27110716700553894, -0.19952617585659027, -0.043922144919633865, 0.03128371387720108, -0.24752922356128693, 0.09999851882457733, -0.02414928749203682, 0.03720826283097267, -0.020143624395132065, 0.11466852575540543, -0.17178687453269958, -0.17300695180892944, 0.006535020191222429, -0.22543300688266754, 0.1380152851343155, -0.2495061606168747, 0.16718418896198273, 0.026813894510269165, -0.2812574803829193], "sparse_embedding": null}, {"id": "152fe63bae013960eb84a72664e18eaa9a9dd4cf2c2bdc2c59677191f006d956", "content": "Reinforcement Learning from Human Feedback (RLHF) is popular in large\nlanguage models (LLMs), whereas traditional Reinforcement Learning (RL) often\nfalls short. Current autonomous driving methods typically utilize either human\nfeedback in machine learning, including RL, or LLMs. Most feedback guides the\ncar agent's learning process (e.g., controlling the car). RLHF is usually\napplied in the fine-tuning step, requiring direct human \"preferences,\" which\nare not commonly used in optimizing autonomous driving models. In this\nresearch, we innovatively combine RLHF and LLMs to enhance autonomous driving\nsafety. Training a model with human guidance from scratch is inefficient. Our\nframework starts with a pre-trained autonomous car agent model and implements\nmultiple human-controlled agents, such as cars and pedestrians, to simulate\nreal-life road environments. The autonomous car model is not directly\ncontrolled by humans. We integrate both physical and physiological feedback to\nfine-tune the model, optimizing this process using LLMs. This multi-agent\ninteractive environment ensures safe, realistic interactions before real-world\napplication. Finally, we will validate our model using data gathered from\nreal-life testbeds located in New Jersey and New York City.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2406.04481v1", "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF", "content": "http://arxiv.org/pdf/2406.04481v1", "datetime": "2024-06-06 20:10:34", "query": "human feedback LLMs", "linkedin": "\ud83d\ude97 Exciting news in autonomous driving tech! \ud83d\ude97\n\nInterested in the latest research on enhancing autonomous driving safety using Reinforcement Learning from Human Feedback (RLHF) and Large Language Models (LLMs)? Check out this innovative approach that combines RLHF and LLMs to optimize autonomous driving models. \n\nWant to learn more about this cutting-edge research and how it can revolutionize autonomous driving technology? Dive into the details here: \nhttp://arxiv.org/abs/2406.04481v1\n\n#AutonomousDriving #RLHF #LLMs #Innovation #AI #TechResearch", "x": "\ud83d\ude97\ud83e\udd16 Exciting research combining Reinforcement Learning from Human Feedback (RLHF) and Large Language Models (LLMs) to enhance autonomous driving safety! Learn how this innovative approach optimizes the learning process and ensures realistic interactions: http://arxiv.org/abs/2406.04481v1 #AI #RLHF #LLMs #AutonomousDriving \ud83d\udee3\ufe0f\ud83d\udd2c", "source_id": "ec537cda8ca0c27c3733b2006f9190e9af427bddf8c829bbe2a2888f8dd5e1e5", "page_number": 1}, "score": null, "embedding": [-0.2860698401927948, 0.08299335837364197, -0.07273934036493301, 0.12564420700073242, -0.22492727637290955, 0.18949569761753082, -0.060499921441078186, 0.026039857417345047, 0.15097451210021973, -0.11882185935974121, 0.14205095171928406, 0.009626445360481739, 0.13989853858947754, 0.5505147576332092, 0.15806107223033905, -0.018367772921919823, -0.22277377545833588, 0.13367082178592682, -0.058904994279146194, -0.21475370228290558, 0.1998003125190735, -0.009556609205901623, -0.03634287789463997, -0.22243066132068634, -0.10232876241207123, 0.10448681563138962, -0.2974696159362793, -0.3173485994338989, -0.41905564069747925, -1.1259593963623047, 0.2638551592826843, 0.17475199699401855, 0.011145484633743763, 0.11458366364240646, -0.22468379139900208, 0.0721689835190773, -0.4169890284538269, 0.20140667259693146, 0.08844322711229324, -0.05604498088359833, -0.02542334422469139, -0.00654668128117919, 0.144831120967865, 0.12418871372938156, 0.1376001238822937, -0.0011479533277451992, -0.2206667959690094, 0.024639390408992767, -0.384382963180542, -0.23949117958545685, -0.08130232244729996, 0.032419025897979736, 0.01461892481893301, 0.11993913352489471, 0.2832871377468109, -0.17218126356601715, 0.2828255593776703, 0.3741984963417053, 0.22322781383991241, 0.07960233837366104, 0.15191462635993958, 0.4892774224281311, -1.0548110008239746, 0.35874977707862854, -0.05755344033241272, 0.23605529963970184, -0.14089733362197876, -0.06103062629699707, 0.18028725683689117, 0.08839535713195801, -0.26388120651245117, 0.07181461155414581, -0.018217703327536583, 0.06674571335315704, 0.24593211710453033, -0.11155933886766434, 0.10198444873094559, -0.42844516038894653, 0.22196945548057556, 0.031000863760709763, 0.17676527798175812, -0.20056959986686707, -0.30175888538360596, -0.26692497730255127, -0.07021202892065048, -0.15093453228473663, -0.015597693622112274, -0.40367403626441956, 0.026927653700113297, 0.15038451552391052, 0.08734557777643204, -0.1120765432715416, -0.0821433737874031, 0.20560933649539948, -0.10783439129590988, 0.1174052283167839, 0.06927063316106796, -0.20164041221141815, -0.3056786358356476, 0.5959320664405823, 0.10996971279382706, 0.3321189284324646, -0.1625598818063736, 0.1664288192987442, 0.03318815305829048, 0.03525825962424278, 0.014791649766266346, -0.06038285046815872, -0.3078751862049103, 0.03172830492258072, 0.037718869745731354, -0.0685114935040474, 0.23864179849624634, -0.07133793830871582, 0.046504244208335876, 0.07864877581596375, 0.26097822189331055, 0.28310704231262207, 0.20232073962688446, -0.18012097477912903, -0.10831592231988907, -0.17262360453605652, 0.3758242130279541, -0.21539561450481415, 0.1783851683139801, -0.22906659543514252, 0.117156982421875, 0.45098862051963806, -0.003012987319380045, 0.14438292384147644, 0.20120929181575775, -0.3866334855556488, -0.3597654402256012, -0.21196889877319336, 0.16261009871959686, 0.04115646332502365, -0.07892847806215286, -0.1756182461977005, -0.0545143187046051, 0.14272932708263397, -0.10229088366031647, 0.19226083159446716, -0.026686012744903564, -0.5167253017425537, -0.26810577511787415, 0.09224741905927658, -0.0989035964012146, 0.14520443975925446, -0.2671850025653839, 0.07246428728103638, 0.2533875107765198, 0.18013904988765717, -0.17109017074108124, -0.025264514610171318, 0.12827923893928528, 0.0479760468006134, 0.40116894245147705, -0.07963865995407104, -0.38363903760910034, 0.24061091244220734, -0.18540233373641968, -0.18422995507717133, -0.01328474935144186, 0.5532164573669434, -0.22481508553028107, -0.37515345215797424, -0.2936347424983978, -0.04012678191065788, 0.37319061160087585, 0.11035656929016113, 0.20155683159828186, 0.05557657778263092, -0.2130584716796875, 0.1428288221359253, -0.06682050973176956, 0.21097365021705627, -0.6993707418441772, 0.01944877579808235, -0.001217719167470932, 0.008472711779177189, -0.048395201563835144, -0.3364623188972473, -0.10928673297166824, -0.03246164321899414, 0.23061159253120422, 0.14871746301651, -0.12703701853752136, -0.5085767507553101, 0.005159119609743357, -0.08842411637306213, -0.027396388351917267, 0.2859795391559601, 0.1383320540189743, -0.0648023933172226, -0.05563775449991226, -0.2105480432510376, 0.09837102144956589, -0.3301841616630554, 0.031041311100125313, 0.06724187731742859, -0.07163030654191971, 0.25586065649986267, 0.11305330693721771, -0.09680275619029999, -0.0884101539850235, -0.17799009382724762, -0.03575770556926727, 0.008392944931983948, 0.4110243618488312, 0.14892838895320892, -0.4149768054485321, 0.013028991408646107, 0.19556966423988342, -0.15377868711948395, -0.2455151230096817, 0.21204480528831482, -0.004460319876670837, 0.003249794477596879, -0.06543367356061935, 0.11878258734941483, 0.21582260727882385, 0.43859633803367615, -0.06850269436836243, -1.0188628435134888, -0.1794937700033188, -0.12165584415197372, -0.01298513263463974, 0.27950045466423035, -0.21449854969978333, 0.2582077085971832, -0.023644177243113518, 0.1441178321838379, 0.3683437705039978, 0.43877071142196655, 0.18702751398086548, -0.22110433876514435, 0.14344452321529388, 0.0452001579105854, 0.1934501677751541, -0.17283451557159424, 0.1732424795627594, -0.2953987717628479, 0.290255069732666, -0.24212121963500977, -0.08976715058088303, -0.07355707883834839, -0.5415213108062744, 0.5527138113975525, -0.2587965130805969, 0.6944668292999268, -0.1634073704481125, 0.18963722884655, -0.014095993712544441, 0.03270318731665611, 0.4201807379722595, 0.01601972058415413, -0.3635183572769165, 0.22392050921916962, -0.2621541917324066, 0.27384838461875916, 0.01630188524723053, -0.014270485378801823, -0.32264986634254456, -0.10055355727672577, 0.06945407390594482, -0.15563534200191498, -0.5064764022827148, -0.4555289149284363, 0.04538925737142563, -0.34210795164108276, 0.1583857536315918, -0.40266501903533936, -0.00374100380577147, 0.14905063807964325, -0.14328114688396454, -0.02513921819627285, -0.11338456720113754, -0.2636934220790863, -0.2742755711078644, -0.2584703266620636, 0.05383705720305443, -0.2107226699590683, -0.018189318478107452, 0.1763286143541336, -0.05265426263213158, -0.02818240039050579, -0.25231340527534485, 0.325492262840271, -0.37574633955955505, 0.16477057337760925, -0.12188607454299927, 0.19858601689338684, 0.1327858567237854, -0.28392282128334045, 0.6981777548789978, 0.2051992565393448, 0.33267685770988464, 0.19373703002929688, 0.004967684857547283, -0.1747223287820816, -0.0900702178478241, -0.44212213158607483, 0.05527593195438385, 0.3297335207462311, 0.11096134036779404, 0.16448096930980682, 0.39630043506622314, 0.38016414642333984, -0.3113347589969635, 0.2099817842245102, -0.04699976369738579, 0.17957714200019836, 0.23236316442489624, 0.08529779314994812, 0.08741069585084915, -0.18339107930660248, -0.21137535572052002, 0.1688971370458603, -0.14652562141418457, -1.2696317434310913, -0.061146993190050125, -0.13786640763282776, 0.4200778007507324, -0.34565305709838867, -0.01626141183078289, 0.14875786006450653, -0.07452483475208282, -0.050895728170871735, -0.23124967515468597, -0.19284412264823914, 0.5062838196754456, 0.36384692788124084, 0.3635751008987427, 0.3239468038082123, -0.05517442151904106, 0.3024219572544098, 0.04698258265852928, 0.2508557140827179, -0.48053422570228577, -0.1251421421766281, 0.5628420114517212, 1.033884882926941, -0.20808511972427368, 0.4061337113380432, 0.32530829310417175, -0.33680015802383423, -0.2928779423236847, 0.06994976848363876, 0.034155964851379395, -0.10047582536935806, 0.19503672420978546, 0.41317296028137207, -0.22059297561645508, 0.1768311709165573, 0.3141389787197113, -0.21086476743221283, 0.08659578114748001, 0.24130521714687347, 0.2372516244649887, -0.03152700513601303, 0.11384396255016327, 0.4339693784713745, -0.0962679386138916, 0.36742115020751953, -0.015642283484339714, 0.2971228063106537, -0.2923344373703003, -0.2095939964056015, 0.21305233240127563, 0.04264334589242935, 0.06868153065443039, -0.04700329899787903, -0.16138137876987457, 0.1633749008178711, -0.037039097398519516, 0.20025038719177246, -0.3553796410560608, -0.09947805851697922, -0.2183218002319336, 0.3223910331726074, -0.029888642951846123, 0.32483428716659546, -0.08873949199914932, -0.10454673320055008], "sparse_embedding": null}, {"id": "ef7e94f530790fb26073eedbef0c2df58dd48ae9deca359707bc47cf4d040df5", "content": "Aligning large language models (LLMs) to human values has become increasingly\nimportant as it enables sophisticated steering of LLMs. However, it requires\nsignificant human demonstrations and feedback or distillation from proprietary\nLLMs such as ChatGPT. In this work, we propose a novel alignment learning\nframework with synthetic feedback not dependent on extensive human annotations\nand proprietary LLMs. First, we perform reward modeling (RM) with synthetic\nfeedback by contrasting responses from vanilla LLMs with various sizes and\nprompts. Then, we use the RM to simulate high-quality demonstrations to train a\nsupervised policy and further optimize the model with reinforcement learning.\nOur resulting model, Aligned Language Model with Synthetic Training dataset\n(ALMoST), outperforms recent open-sourced models, which are trained on the\noutputs of InstructGPT or human-annotated demonstrations, in alignment\nbenchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2,\n55.0% and 58.5% of the time, respectively. Further analyses demonstrate the\nefficacy and importance of synthetic feedback in our framework. The code is\navailable at https://github.com/naver-ai/almost", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2305.13735v2", "title": "Aligning Large Language Models through Synthetic Feedback", "content": "http://arxiv.org/pdf/2305.13735v2", "datetime": "2023-10-21 01:50:54", "query": "human feedback LLMs", "linkedin": "\ud83d\ude80 Exciting news in the world of large language models (LLMs)! A new alignment learning framework, ALMoST, has been introduced to enhance LLMs' alignment with human values without relying on extensive human annotations or proprietary LLMs like ChatGPT. \n\nBy utilizing synthetic feedback and innovative reward modeling techniques, ALMoST outperforms existing open-sourced models in alignment benchmarks. Human evaluation shows that ALMoST is preferred over Alpaca and Dolly-v2 a significant percentage of the time.\n\nRead the full research paper and access the code at: http://arxiv.org/abs/2305.13735v2\n\n#AI #NLP #LLMs #AlignmentLearning #Research #TechInnovation", "x": "\ud83d\ude80 Exciting new research alert! A novel alignment learning framework, ALMoST, outperforms recent models in alignment benchmarks without extensive human annotations or proprietary LLMs. Learn more at: http://arxiv.org/abs/2305.13735v2 #AI #NLP #LLMs #Tech #Research #ALMoST", "source_id": "db97548e9bb0cdb4335fd9fc0a64562b94b8a82b61ca06e7481ad73531e5efc8", "page_number": 1}, "score": null, "embedding": [-0.4038417637348175, 0.011938516981899738, 0.0019033316057175398, -0.11591311544179916, -0.13532830774784088, 0.12881772220134735, -0.40292710065841675, -0.0007623741985298693, 0.2611949145793915, -0.21460093557834625, 0.09952042251825333, -0.24450846016407013, 0.021617697551846504, 0.3543838858604431, 0.3276079297065735, 0.03649459034204483, -0.38929134607315063, 0.18420858681201935, -0.09647883474826813, 0.16308660805225372, 0.10783109068870544, -0.18304945528507233, -0.018059173598885536, -0.03641011565923691, -0.15286937355995178, -0.00750302616506815, -0.31151413917541504, -0.04862955957651138, -0.17043621838092804, -1.4761322736740112, 0.2088126540184021, 0.07311694324016571, 0.4482012093067169, -0.1217808872461319, -0.2564634382724762, 0.09871085733175278, -0.3047875463962555, 0.06306114047765732, -0.05853641405701637, 0.2907637655735016, -0.15237578749656677, -0.028056534007191658, 0.1327262967824936, -0.09113365411758423, 0.007806887850165367, -0.4070802628993988, -0.21240992844104767, -0.016288384795188904, -0.521349310874939, -0.06433138996362686, -0.09970523416996002, -0.24659651517868042, -0.018288504332304, 0.21927152574062347, 0.16192857921123505, 0.18858987092971802, 0.33898285031318665, 0.478515088558197, -0.12303552776575089, 0.06098774075508118, 0.1334831863641739, 0.15149854123592377, -1.020733118057251, 0.44801098108291626, -0.09922876954078674, 0.32545650005340576, -0.04766921326518059, 0.11087428033351898, 0.23661814630031586, 0.32038626074790955, -0.15300947427749634, 0.061499450355768204, 0.4086483418941498, 0.13183507323265076, 0.1787394881248474, 0.2913764417171478, 0.13683906197547913, -0.10688888281583786, 0.17979027330875397, -0.2324034422636032, 0.06652917712926865, -0.12346069514751434, -0.07492431998252869, -0.3053188920021057, 0.024677811190485954, -0.12604589760303497, 0.10572593659162521, -0.2570295035839081, -0.030985094606876373, 0.22240324318408966, -0.1566956490278244, -0.073521688580513, -0.22970838844776154, 0.21322141587734222, -0.31305548548698425, 0.08565928041934967, -0.01936584897339344, 0.15125328302383423, -0.4128734767436981, 0.6118663549423218, -0.02366376668214798, 0.37235042452812195, -0.29234713315963745, 0.09881507605314255, 0.4404008090496063, -0.06447120755910873, -0.30258846282958984, -0.00814127642661333, -0.13635917007923126, -0.21178574860095978, -0.10316936671733856, 0.2017197161912918, 0.0710720345377922, -0.3437458872795105, -0.19500775635242462, 0.11714513599872589, 0.43118008971214294, 0.22176788747310638, 0.1564761847257614, -0.1544707864522934, -0.09483557194471359, 0.3464375138282776, 0.04727515950798988, -0.09576915204524994, 0.10559558868408203, -0.01908787526190281, -0.05878886207938194, 0.3884296119213104, 0.1314026564359665, 0.08494853973388672, 0.21106112003326416, 0.08425111323595047, -0.25170236825942993, -0.1718534529209137, 0.11611105501651764, 0.04934389516711235, -0.15819746255874634, -0.2176758497953415, 0.3536069095134735, 0.340511292219162, -0.12860876321792603, 0.056962113827466965, 0.10054630041122437, -0.24578934907913208, -0.4901599884033203, 0.3515164852142334, -0.1195666715502739, 0.1780085414648056, -0.2379612922668457, 0.01666935347020626, 0.041264764964580536, 0.13775360584259033, -0.07926959544420242, -0.18768128752708435, 0.060630373656749725, 0.10404569655656815, 0.017019303515553474, 0.16468550264835358, -0.26507967710494995, -0.20501753687858582, -0.13998283445835114, -0.06551748514175415, -0.33735087513923645, 0.5336489081382751, -0.09444580972194672, -0.6839292645454407, -0.2636079788208008, 0.046846285462379456, 0.19269636273384094, -0.0485147200524807, 0.33071011304855347, -0.04388371482491493, 0.0013130634324625134, -0.0011052493937313557, 0.003193369833752513, 0.12384948879480362, -0.6817597150802612, 0.00043227116111665964, 0.15387365221977234, 0.40305444598197937, 0.13408854603767395, -0.3065008819103241, -0.2664056420326233, 0.2243325561285019, -0.07061130553483963, 0.015492989681661129, 0.010553193278610706, -0.25135019421577454, 0.20780548453330994, -0.20369009673595428, 0.07276187092065811, 0.09734507650136948, 0.03526740521192551, 0.047852229326963425, -0.1879815012216568, -0.394339919090271, 0.003464512759819627, -0.010725544765591621, 0.22224920988082886, -0.050742603838443756, -0.09842591732740402, 0.3873359262943268, -0.003152858931571245, -0.28896990418434143, -0.02543252520263195, 0.08103564381599426, -0.007732263300567865, 0.12551701068878174, 0.30624303221702576, 0.3003266155719757, -0.28355443477630615, 0.08408249169588089, 0.25006985664367676, 0.08382025361061096, -0.24211394786834717, 0.10103438049554825, 0.2782674729824066, 0.3098162114620209, -0.17990536987781525, 0.38697516918182373, 0.21426256000995636, 0.29908081889152527, -0.07558410614728928, -1.2325258255004883, 0.1123916283249855, 0.07376125454902649, 0.26307040452957153, 0.23217827081680298, -0.31444066762924194, 0.16850413382053375, -0.04616645723581314, 0.3004310429096222, 0.2441396415233612, 0.18264158070087433, 0.22687643766403198, -0.1636582314968109, 0.13384392857551575, -0.05567047744989395, 0.3102174401283264, -0.11455358564853668, 0.15025697648525238, -0.018557976931333542, 0.14203764498233795, -0.324375718832016, 0.07733674347400665, 0.015991035848855972, -0.5779620409011841, 0.3987550735473633, -0.2075546681880951, 0.8411884307861328, -0.05369602143764496, 0.0025111210998147726, 0.07751652598381042, 0.11711467057466507, 0.42828819155693054, 0.01953776367008686, -0.47223958373069763, 0.4865572154521942, 0.1605476588010788, 0.23897451162338257, 0.1059366762638092, 0.006372619885951281, -0.07960112392902374, -0.07510783523321152, -0.0716385617852211, 0.2446148842573166, -0.726006031036377, -0.24548041820526123, 0.26231539249420166, -0.3434205949306488, -0.1882384717464447, -0.38656407594680786, 0.07206129282712936, 0.10218418389558792, -0.11508586257696152, 0.2058655023574829, 0.031916409730911255, -0.4120071232318878, 0.04302817955613136, -0.5199397802352905, -0.13985542953014374, -0.3468717336654663, -0.13146033883094788, 0.2284943014383316, -0.03709648177027702, 0.033994730561971664, -0.11792311072349548, 0.04671251028776169, -0.23272547125816345, 0.009454739280045033, -0.07200261205434799, 0.23197196424007416, 0.04304804280400276, -0.03262360766530037, 0.527738630771637, -0.03139582648873329, 0.12586438655853271, 0.12244611978530884, 0.10105735808610916, -0.2307339310646057, -0.11703335493803024, -0.3498293459415436, -0.07265005260705948, 0.34107571840286255, 0.31688371300697327, 0.17345821857452393, 0.10505937784910202, 0.3150452673435211, -0.04716743528842926, 0.22051143646240234, -0.05436592549085617, 0.2483680546283722, 0.12143047153949738, 0.11248063296079636, 0.11196376383304596, -0.2833499312400818, -0.127131387591362, 0.036907874047756195, 0.15561512112617493, -1.1914764642715454, 0.03966697305440903, -0.29208239912986755, 0.3738499879837036, -0.22515755891799927, -0.06540011614561081, 0.31457239389419556, -0.28352564573287964, -0.05519913509488106, -0.04470805078744888, -0.16167400777339935, 0.2503569424152374, 0.32557931542396545, -0.02948952279984951, 0.12487397342920303, -0.23720623552799225, 0.36098557710647583, -0.2786235809326172, 0.2954070270061493, -0.8026198744773865, -0.01418804470449686, 0.22121195495128632, 1.0249770879745483, -0.03319886326789856, 0.18490351736545563, 0.1308993101119995, -0.1426260769367218, -0.14582350850105286, 0.3080695569515228, -0.11980248987674713, -0.2520735263824463, -0.00030160672031342983, 0.4463970363140106, 0.048982344567775726, 0.0893535166978836, 0.6105592846870422, -0.2993176579475403, -0.2517470717430115, 0.40187662839889526, 0.10410816222429276, 0.03436736762523651, 0.07606060057878494, 0.151140034198761, -0.12143456190824509, 0.43185073137283325, 0.1432020217180252, 0.1698254644870758, -0.12820738554000854, -0.1336032748222351, 0.019563740119338036, 0.04948391020298004, 0.07578450441360474, -0.01577152870595455, -0.16282081604003906, 0.1694498360157013, -0.02646002545952797, -0.05680375546216965, -0.1660049855709076, -0.2277037352323532, -0.16899573802947998, 0.23096926510334015, -0.15659655630588531, 0.06407961994409561, 0.03207797557115555, -0.48908427357673645], "sparse_embedding": null}, {"id": "d4a1ed36a75d69ff02912f6cc6ea4ece3b27a1956eb3c9e660709ce7dd363d3f", "content": "Human feedback is increasingly used to steer the behaviours of Large Language\nModels (LLMs). However, it is unclear how to collect and incorporate feedback\nin a way that is efficient, effective and unbiased, especially for highly\nsubjective human preferences and values. In this paper, we survey existing\napproaches for learning from human feedback, drawing on 95 papers primarily\nfrom the ACL and arXiv repositories.First, we summarise the past, pre-LLM\ntrends for integrating human feedback into language models. Second, we give an\noverview of present techniques and practices, as well as the motivations for\nusing feedback; conceptual frameworks for defining values and preferences; and\nhow feedback is collected and from whom. Finally, we encourage a better future\nof feedback learning in LLMs by raising five unresolved conceptual and\npractical challenges.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.07629v1", "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values", "content": "http://arxiv.org/pdf/2310.07629v1", "datetime": "2023-10-11 16:18:13", "query": "human feedback LLMs", "linkedin": "\ud83d\ude80 Exciting insights into the world of Large Language Models (LLMs) and human feedback! \ud83c\udf1f\n\nCurious about how human feedback shapes the behavior of LLMs? Dive into this comprehensive survey of existing approaches in the field, drawing on 95 papers from ACL and arXiv repositories. Learn about past trends, current techniques, and future challenges in integrating human feedback into language models.\n\nRead more about this fascinating research at: http://arxiv.org/abs/2310.07629v1\n\n#LLMs #HumanFeedback #AI #NLP #Research #TechInnovation", "x": "\ud83d\ude80 Exciting read! Check out this insightful paper on incorporating human feedback into Large Language Models (LLMs) efficiently and effectively. Learn about past trends, present techniques, and future challenges in feedback learning for LLMs. #AI #NLP\n\n\ud83d\udcc4 Read more: http://arxiv.org/abs/2310.07629v1", "source_id": "6e80b700e945a808d659028dff66003776794471492d84c1ab486776f1a83c80", "page_number": 1}, "score": null, "embedding": [-0.43994787335395813, 0.08101867884397507, -0.03349629044532776, -0.019613485783338547, -0.17725443840026855, 0.22770646214485168, -0.21207179129123688, 0.3234748840332031, 0.4100532829761505, -0.1530465930700302, 0.04006107524037361, -0.1425347477197647, 0.043779145926237106, 0.38064301013946533, 0.1301880031824112, 0.17459417879581451, -0.3390180766582489, -0.07899264246225357, -0.24896226823329926, -0.10301801562309265, 0.17449542880058289, -0.20830854773521423, 0.2660301923751831, -0.0765550434589386, -0.18243379890918732, -0.22078736126422882, -0.265163779258728, -0.20308037102222443, -0.32340604066848755, -1.4477065801620483, 0.24277359247207642, 0.07359249889850616, 0.44722795486450195, -0.05773839354515076, -0.33540481328964233, -0.03433072194457054, -0.2554731070995331, -0.05746020749211311, -0.017252372577786446, 0.34911176562309265, -0.12870892882347107, -0.005360232666134834, 0.1720724254846573, -0.34005871415138245, -0.19057677686214447, -0.3919253647327423, -0.09243457764387131, -0.2681366801261902, -0.6358102560043335, -0.02416278049349785, -0.24471382796764374, -0.3664829730987549, -0.054387375712394714, 0.2555733621120453, 0.31012263894081116, 0.0863284170627594, 0.3465741276741028, 0.18955181539058685, 0.08598645776510239, 0.27125176787376404, 0.48730406165122986, 0.15258531272411346, -1.1061550378799438, 0.5824915766716003, -0.19567444920539856, 0.46985796093940735, -0.06672771275043488, 0.2730260491371155, -0.205368310213089, 0.15703263878822327, -0.31704431772232056, 0.082986019551754, 0.22005271911621094, 0.3253025412559509, 0.3335842192173004, 0.06874874979257584, 0.1352299600839615, -0.34361428022384644, 0.2762308418750763, -0.17021819949150085, 0.0979854017496109, 0.21661119163036346, -0.04438883438706398, -0.5297004580497742, -0.018780026584863663, -0.23057419061660767, -0.11290563642978668, -0.3493228256702423, 0.12763433158397675, 0.07386482506990433, 0.22061312198638916, -0.10544885694980621, -0.24479837715625763, 0.3900546729564667, -0.12214313447475433, -0.2148209512233734, 0.10744072496891022, 0.06386173516511917, -0.5312289595603943, 0.646268367767334, -0.12766678631305695, 0.33747580647468567, 0.14373844861984253, -0.045123908668756485, 0.24597962200641632, -0.037237055599689484, -0.075649693608284, -0.06669709831476212, -0.01524001732468605, -0.0771021768450737, -0.17039033770561218, 0.03236773982644081, 0.1341189295053482, -0.21573297679424286, 0.1762949526309967, 0.038563668727874756, 0.39870062470436096, 0.15028658509254456, 0.25501376390457153, -0.2157147377729416, -0.1797877550125122, 0.12981273233890533, 0.14551539719104767, -0.2329237014055252, 0.12803760170936584, -0.11072929203510284, 0.12498185783624649, 0.4167681336402893, 0.12435134500265121, 0.07520927488803864, 0.29325976967811584, -0.13703985512256622, -0.4624811112880707, -0.2255934327840805, -0.030706122517585754, -0.024106435477733612, -0.09005896002054214, -0.41413596272468567, 0.19052034616470337, 0.3386692702770233, 0.12265107780694962, -0.080253005027771, -0.0629945695400238, -0.34952014684677124, -0.470821350812912, 0.4129803776741028, 0.0014989431947469711, -0.12582702934741974, -0.25087201595306396, 0.18834805488586426, 0.4445175230503082, 0.3900229036808014, -0.17986205220222473, -0.08351206034421921, 0.1526578962802887, 0.24440862238407135, 0.11512701213359833, -0.0006033299723640084, -0.2230161428451538, 0.0011927151354029775, -0.03416179120540619, -0.1928476244211197, -0.23212967813014984, 0.4865879714488983, -0.09885838627815247, -0.35036587715148926, -0.30534040927886963, -0.20203658938407898, 0.3080701231956482, 0.1213827133178711, 0.2669556140899658, -0.007199564017355442, -0.34299829602241516, 0.07314159721136093, 0.2880493104457855, 0.3298206925392151, -0.5640006065368652, -0.1550779491662979, 0.13967569172382355, 0.2530263066291809, 0.2684136629104614, -0.051544323563575745, -0.04697728157043457, 0.34647467732429504, 0.028570860624313354, -0.007014257833361626, -0.014671314507722855, -0.5813923478126526, 0.13976183533668518, 0.19770249724388123, -0.13569027185440063, -0.18637444078922272, 0.15017618238925934, 0.01953979581594467, -0.11869798600673676, -0.37732502818107605, -0.09556493163108826, -0.15806449949741364, 0.024706754833459854, -0.14074920117855072, -0.1763385534286499, 0.34382209181785583, -0.06680727005004883, -0.21279843151569366, 0.23023945093154907, 0.3159276247024536, 0.22007033228874207, 0.019651226699352264, 0.5479093790054321, 0.11685021966695786, -0.35290032625198364, 0.24102738499641418, 0.36136800050735474, -0.05359407886862755, -0.04847251623868942, 0.02550182305276394, 0.3779886066913605, 0.2308889478445053, -0.05107657238841057, 0.17932234704494476, 0.04596836864948273, 0.17384132742881775, -0.21017904579639435, -1.1006381511688232, -0.032196007668972015, 0.13507814705371857, 0.1412634253501892, 0.3375271260738373, -0.238374263048172, 0.23963648080825806, -0.13287808001041412, 0.24188126623630524, 0.44160062074661255, 0.0910114049911499, 0.07076389342546463, -0.2444581389427185, 0.24813111126422882, 0.030354583635926247, 0.08006948232650757, -0.43153053522109985, 0.10275470465421677, -0.33154118061065674, 0.36844402551651, -0.3987872004508972, -0.046928588300943375, 0.2216169238090515, -0.6866359114646912, 0.25324031710624695, -0.44533175230026245, 0.7994541525840759, -0.2729637026786804, -0.06835461407899857, 0.021728528663516045, 0.0802319347858429, 0.46906962990760803, -0.1337936371564865, -0.5909689664840698, 0.5526807308197021, 0.1376969814300537, 0.07983177900314331, 0.01796134188771248, -0.12213680148124695, -0.3190647065639496, 0.07601642608642578, 0.001864778809249401, -0.029550153762102127, -0.4471607506275177, -0.5188491344451904, 0.12337402999401093, -0.1663188934326172, -0.17466945946216583, -0.31310856342315674, 0.04871391877532005, -0.05918411165475845, 0.07098724693059921, 0.10783195495605469, -0.11268924176692963, -0.46751147508621216, 0.09403055906295776, -0.36387673020362854, 0.18094152212142944, 0.031196825206279755, 0.05328761786222458, 0.28973498940467834, -0.13255637884140015, -0.10756517946720123, -0.4417211413383484, 0.18141941726207733, -0.2941438853740692, 0.039982832968235016, 0.07156737148761749, 0.19801011681556702, 0.07998302578926086, -0.1426791101694107, 0.5453452467918396, 0.030442912131547928, -0.01740552857518196, 0.08142809569835663, 0.42334258556365967, -0.3037002384662628, -0.08797789365053177, -0.393051415681839, -0.2239847183227539, 0.4085325598716736, 0.14860117435455322, -0.02549441158771515, 0.15393753349781036, 0.3635997474193573, -0.13361434638500214, 0.0900726392865181, -0.04575055092573166, -0.05967656522989273, 0.10737991333007812, 0.13882069289684296, -0.044844865798950195, -0.19573582708835602, -0.02554403990507126, 0.05070970207452774, 0.08959978818893433, -1.0985225439071655, -0.03786109760403633, -0.13611815869808197, 0.5926840305328369, -0.05439949408173561, 0.3987956941127777, 0.13790440559387207, -0.12371666729450226, -0.060276925563812256, 0.10043725371360779, 0.04254510998725891, 0.09053346514701843, 0.28024402260780334, -0.17139166593551636, 0.08692902326583862, -0.18350303173065186, 0.3854057788848877, -0.2623339891433716, 0.33697840571403503, -0.4695994257926941, -0.02057698927819729, 0.23123805224895477, 1.0925995111465454, -0.1347247064113617, 0.16622298955917358, 0.20763733983039856, -0.156110480427742, -0.10635371506214142, 0.09335965663194656, 0.002434374997392297, -0.23520325124263763, -0.048924125730991364, 0.8185825943946838, -0.005201352294534445, -0.08967931568622589, 0.363340824842453, -0.3764651119709015, 0.04990275576710701, 0.20608533918857574, 0.00779340136796236, 0.18258899450302124, -0.0036335685290396214, -0.014809506945312023, -0.10341351479291916, 0.4830479621887207, 0.14352749288082123, 0.2970733642578125, -0.4343937337398529, -0.023043090477585793, -0.043669380247592926, -0.08224952220916748, -0.11028163135051727, 0.06975246965885162, 0.026538893580436707, 0.16910018026828766, 0.09056257456541061, 0.10671057552099228, -0.17491216957569122, -0.13738806545734406, -0.19224761426448822, 0.16503113508224487, 0.09294125437736511, 0.3607378602027893, -0.22619429230690002, -0.06072822958230972], "sparse_embedding": null}, {"id": "161a89c1138b09aabb101e4feecf966e5f4d3fe3522d432fc23de3329ab94cdf", "content": "To democratize large language models (LLMs) to most natural languages, it is\nimperative to make these models capable of understanding and generating texts\nin many languages, in particular low-resource ones. While recent multilingual\nLLMs demonstrate remarkable performance in such capabilities, these LLMs still\nsupport a limited number of human languages due to the lack of training data\nfor low-resource languages. Moreover, these LLMs are not yet aligned with human\npreference for downstream tasks, which is crucial for the success of LLMs in\nEnglish. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively\nxLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100\nlanguages. To do so, we construct two datasets: a multilingual instruction\ndataset including 100 languages, which represents the largest language coverage\nto date, and a cross-lingual human feedback dataset encompassing 30 languages.\nWe perform multilingual instruction tuning on the constructed instruction data\nand further align the LLMs with human feedback using the DPO algorithm on our\ncross-lingual human feedback dataset. We evaluate the multilingual\nunderstanding and generating capabilities of xLLMs-100 on five multilingual\nbenchmarks. Experimental results show that xLLMs-100 consistently outperforms\nits peers across the benchmarks by considerable margins, defining a new\nstate-of-the-art multilingual LLM that supports 100 languages.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2406.01771v1", "title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback", "content": "http://arxiv.org/pdf/2406.01771v1", "datetime": "2024-06-03 20:25:12", "query": "human feedback LLMs", "linkedin": "\ud83c\udf1f Exciting News in the World of Language Models! \ud83c\udf1f\n\nIn a recent paper, researchers have introduced xLLaMA-100 and xBLOOM-100 (xLLMs-100), scaling multilingual capabilities to an impressive 100 languages. By constructing datasets that include a multilingual instruction dataset with 100 languages and a cross-lingual human feedback dataset with 30 languages, these models have achieved a new state-of-the-art in multilingual language understanding and generation.\n\nRead the full paper here for details on how xLLMs-100 outperforms its peers across five multilingual benchmarks: http://arxiv.org/abs/2406.01771v1\n\n#LanguageModels #AI #NLP #Multilingual #TechInnovation", "x": "\ud83c\udf10 Exciting development in the world of multilingual large language models! Introducing xLLMs-100, a new state-of-the-art model supporting 100 languages. Learn more about its remarkable performance and capabilities in this research paper: http://arxiv.org/abs/2406.01771v1 #AI #NLP #LLMs #MultilingualModels \ud83d\ude80\ud83d\udcda", "source_id": "e931da95550d0289db07273dcd4d7cf03ea0b694c572fdbc7e59cf902c5ba134", "page_number": 1}, "score": null, "embedding": [-0.5196604132652283, -0.1119568720459938, 0.007342842407524586, -0.11906540393829346, -0.23409053683280945, 0.10401951521635056, -0.479577898979187, -0.03674796596169472, 0.2023611068725586, -0.20791314542293549, 0.05623722821474075, -0.11131434887647629, 0.166751891374588, 0.32344895601272583, 0.28674381971359253, 0.10143297910690308, -0.11856499314308167, -0.07207255810499191, -0.4398800730705261, -0.00906627532094717, 0.510588526725769, -0.1748855710029602, 0.06917697191238403, -0.05666976049542427, -0.0037011143285781145, 0.05950566753745079, -0.1362549513578415, -0.22799687087535858, -0.1758859008550644, -1.5025471448898315, 0.1886269450187683, 0.07645565271377563, 0.49078717827796936, -0.18708142638206482, -0.21561892330646515, -0.03461967781186104, -0.2513982057571411, -0.009769855998456478, 0.03333447501063347, 0.31000620126724243, -0.09908120334148407, 0.13016028702259064, 0.1275664120912552, -0.24686962366104126, -0.11292339861392975, -0.39932772517204285, -0.4548256993293762, 0.06186005100607872, -0.561299741268158, 0.06785949319601059, -0.20352984964847565, -0.4862355589866638, 0.057328056544065475, 0.21757161617279053, 0.22068101167678833, 0.005479898769408464, 0.07651206851005554, 0.42339491844177246, 0.04098254069685936, 0.1847565919160843, 0.3143252730369568, 0.23281845450401306, -0.9667407274246216, 0.5233902931213379, -0.20814275741577148, 0.2908550500869751, -0.12909257411956787, 0.11469286680221558, 0.029036998748779297, 0.041464515030384064, -0.16294564306735992, -0.09237051010131836, 0.3812657296657562, 0.24143721163272858, 0.3675772249698639, 0.14146092534065247, 0.07318738847970963, -0.22113215923309326, 0.2946262061595917, 0.039148759096860886, 0.27813857793807983, -0.005262080579996109, -0.10008357465267181, -0.2403762936592102, -0.09414143860340118, 0.01891210488975048, -0.18644675612449646, -0.1403859704732895, -0.23444171249866486, 0.005201688501983881, 0.01752736233174801, 0.11006799340248108, -0.13958348333835602, 0.31367993354797363, -0.15591013431549072, -0.1382095217704773, 0.17123836278915405, 0.04672275111079216, -0.17013773322105408, 0.5673287510871887, -0.1661396622657776, -0.04386225715279579, 0.13697084784507751, -0.054195646196603775, 0.31408074498176575, -0.04525689408183098, 0.00981923658400774, -0.22019851207733154, -0.2602371275424957, -0.32063621282577515, -0.05059283599257469, 0.032392580062150955, -0.0742025300860405, -0.0015248543350026011, 0.15488602221012115, -0.09913769364356995, 0.43435749411582947, -0.11246564984321594, -0.038605548441410065, 0.1981159895658493, -0.37447819113731384, -0.02252485230565071, 0.04877014458179474, -0.23543691635131836, 0.12749086320400238, -0.05567856505513191, 0.2621321380138397, 0.4402349591255188, 0.16062015295028687, 0.18931475281715393, 0.22395427525043488, 0.09460915625095367, -0.3615545928478241, -0.37635311484336853, 0.01330717746168375, -0.0722375139594078, 0.012873601168394089, -0.14055052399635315, 0.05884189158678055, 0.23103296756744385, -0.03270777314901352, 0.087295301258564, 0.1864331066608429, -0.2060965746641159, -0.36096861958503723, 0.6380276679992676, -0.09984299540519714, 0.01801387406885624, -0.2723698914051056, 0.11658285558223724, 0.01980714313685894, 0.23148278892040253, -0.1032944843173027, -0.36329716444015503, 0.3631625473499298, 0.17315202951431274, 0.1828295886516571, 0.11359592527151108, -0.2591451108455658, -0.09779642522335052, -0.0837068036198616, -0.20028860867023468, -0.08523747324943542, 0.6543172001838684, -0.2641770541667938, -0.35702750086784363, -0.3180297613143921, 0.10399431735277176, 0.1556389033794403, -0.04694252833724022, 0.35677069425582886, 0.10309650003910065, -0.3064272701740265, -0.011866808868944645, 0.0389849916100502, 0.1670055091381073, -0.8197076916694641, -0.08811870962381363, -0.07031622529029846, 0.41612207889556885, 0.1720169186592102, -0.1467839628458023, 0.06174132972955704, 0.14007800817489624, -0.027065856382250786, -0.06478594988584518, -0.07298737019300461, -0.19516097009181976, 0.22315169870853424, -0.01481117308139801, -0.20687510073184967, 0.2352416068315506, 0.14697539806365967, 0.013407316990196705, -0.06119425967335701, -0.1973210573196411, -0.028591008856892586, -0.09807285666465759, 0.2110608071088791, -0.17957229912281036, -0.06009511649608612, 0.3002011179924011, -0.14568328857421875, 0.027935463935136795, 0.24163731932640076, -0.13700099289417267, 0.24060386419296265, 0.0412130281329155, 0.4130628705024719, 0.22011488676071167, -0.10769610852003098, 0.21113362908363342, 0.4251786172389984, -0.09345286339521408, 0.11329787969589233, 0.15650857985019684, 0.18223147094249725, 0.29724642634391785, -0.14535926282405853, 0.1739187091588974, 0.1077399030327797, -0.08839596807956696, -0.39507079124450684, -1.2236860990524292, -0.3528318703174591, 0.23215597867965698, -0.027144847437739372, 0.028742250055074692, -0.1589759737253189, 0.0013692298671230674, 0.09475015103816986, 0.2708926498889923, 0.6163947582244873, 0.22104403376579285, 0.0730138048529625, -0.1061500683426857, 0.22322139143943787, -0.05344448983669281, 0.15936493873596191, -0.2839435636997223, 0.20320190489292145, -0.3665614724159241, 0.24187758564949036, 0.047772880643606186, -0.05054756999015808, -0.0009797819657251239, -0.5393181443214417, 0.07657519727945328, -0.16087736189365387, 0.7773930430412292, -0.15913626551628113, -0.01624726876616478, -0.004233817104250193, 0.07166706025600433, 0.5804957151412964, -0.24828559160232544, -0.4650799036026001, 0.4820605218410492, 0.12381874769926071, 0.0731375515460968, 0.045749664306640625, 0.11922554671764374, 0.10244276374578476, 0.011482208035886288, -0.007984669879078865, -0.00759673910215497, -0.5218888521194458, -0.20222142338752747, 0.07116276770830154, -0.06900869309902191, -0.3176726698875427, -0.35487598180770874, -0.13916021585464478, 0.05016485974192619, 0.053042221814394, 0.19074198603630066, 0.08765824884176254, -0.41168078780174255, 0.13628527522087097, -0.7880287170410156, 0.16330881416797638, -0.2149525135755539, 0.060973044484853745, 0.1759958267211914, -0.1425783634185791, -0.20358386635780334, -0.24943314492702484, 0.0591936931014061, -0.18948906660079956, -0.0031498614698648453, 0.07242918014526367, 0.058195810765028, 0.049074482172727585, -0.12336281687021255, 0.4469078481197357, -0.14921890199184418, 0.11562178283929825, -0.02528066374361515, 0.3572014272212982, -0.08058690279722214, -0.34904417395591736, -0.278465211391449, -0.010791366919875145, 0.6851659417152405, 0.08300043642520905, 0.10360103100538254, 0.23013374209403992, 0.29034045338630676, 0.08005563169717789, 0.4627174437046051, 0.23408344388008118, 0.035995546728372574, 0.226050466299057, -0.03459447622299194, -0.12287598103284836, -0.24141129851341248, -0.09854232519865036, -0.03274955227971077, 0.09811225533485413, -0.9701566696166992, 0.04178542643785477, -0.04800654575228691, 0.29426082968711853, -0.2156524360179901, 0.23462577164173126, 0.1730155646800995, -0.16296237707138062, 0.21039564907550812, 0.10191728174686432, -0.1408490389585495, 0.31690362095832825, 0.2690570056438446, -0.09356527030467987, 0.09394239634275436, -0.17008495330810547, 0.4765528738498688, 0.00047715878463350236, 0.06746751815080643, -0.14793869853019714, -0.13150402903556824, 0.2564419209957123, 0.8908613324165344, 0.03964463248848915, -0.16152068972587585, 0.1440383791923523, -0.2253321409225464, 0.04405401274561882, 0.0020974522922188044, 0.03054981678724289, -0.16635896265506744, 0.1922294944524765, 0.6807569265365601, -0.12171585112810135, -0.15842404961585999, 0.4160364866256714, -0.2347390055656433, -0.11908971518278122, 0.3434586524963379, 0.1654152125120163, 0.09336455166339874, 0.0014513683272525668, -0.12739154696464539, -0.2705584168434143, 0.37251660227775574, 0.2161661684513092, 0.2345261573791504, -0.3566431999206543, 0.004503051750361919, -0.05489550158381462, -0.11793392896652222, 0.11769440770149231, -0.13670474290847778, -0.021779891103506088, -0.017245126888155937, 0.22912433743476868, 0.17952097952365875, -0.2097998559474945, -0.0918121263384819, -0.16084560751914978, 0.10116802155971527, -0.3871050775051117, -0.12433342635631561, -0.05485125631093979, -0.1656711846590042], "sparse_embedding": null}, {"id": "a90c9c743f8dc92203477ae13fca98f9cc08811673b2b108bc03fca13e479eda", "content": "The recent surge of versatile large language models (LLMs) largely depends on\naligning increasingly capable foundation models with human intentions by\npreference learning, enhancing LLMs with excellent applicability and\neffectiveness in a wide range of contexts. Despite the numerous related studies\nconducted, a perspective on how human preferences are introduced into LLMs\nremains limited, which may prevent a deeper comprehension of the relationships\nbetween human preferences and LLMs as well as the realization of their\nlimitations. In this survey, we review the progress in exploring human\npreference learning for LLMs from a preference-centered perspective, covering\nthe sources and formats of preference feedback, the modeling and usage of\npreference signals, as well as the evaluation of the aligned LLMs. We first\ncategorize the human feedback according to data sources and formats. We then\nsummarize techniques for human preferences modeling and compare the advantages\nand disadvantages of different schools of models. Moreover, we present various\npreference usage methods sorted by the objectives to utilize human preference\nsignals. Finally, we summarize some prevailing approaches to evaluate LLMs in\nterms of alignment with human intentions and discuss our outlooks on the human\nintention alignment for LLMs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2406.11191v1", "title": "A Survey on Human Preference Learning for Large Language Models", "content": "http://arxiv.org/pdf/2406.11191v1", "datetime": "2024-06-17 03:52:51", "query": "human feedback LLMs", "linkedin": "\ud83d\ude80 Exciting developments in the world of AI and NLP! Check out this insightful survey on how human preferences are introduced into Large Language Models (LLMs) to enhance their effectiveness across diverse contexts. The survey categorizes human feedback sources, preference modeling techniques, usage methods, and evaluation approaches for aligned LLMs. Dive deeper into the relationship between human preferences and LLMs for a comprehensive understanding.\n\nRead the full survey here: [http://arxiv.org/abs/2406.11191v1](http://arxiv.org/abs/2406.11191v1)\n\n#AI #NLP #LLMs #PreferenceLearning #TechSurvey #HumanIntentions #Innovation #TechTrends", "x": "\ud83d\ude80 Exciting insights on aligning human preferences with Large Language Models (LLMs) for enhanced effectiveness! This survey delves into the sources, formats, modeling, and evaluation of human feedback in LLMs. Dive deeper into the realm of preference-centered learning here: http://arxiv.org/abs/2406.11191v1 #AI #NLP #LLMs #TechResearch", "source_id": "3476c35352e6ceba31e66ece3a5422fa3b8fb65cf8c026437a5523cb6bd252f9", "page_number": 1}, "score": null, "embedding": [-0.3799351751804352, -0.0100961709395051, 0.00974947214126587, -0.21100810170173645, -0.1343638002872467, 0.027789710089564323, -0.13836224377155304, 0.07693272829055786, 0.42821356654167175, -0.29398414492607117, 0.12091994285583496, -0.07867516577243805, -0.04313063994050026, 0.18225634098052979, 0.3030778467655182, 0.1421528309583664, -0.45346567034721375, 0.03288884088397026, -0.18493011593818665, 0.09897050261497498, 0.16626068949699402, -0.31059572100639343, 0.13228051364421844, -0.11290688812732697, -0.14959222078323364, 0.04957461357116699, -0.05642014741897583, 0.0030752995517104864, -0.0820925161242485, -1.3157362937927246, 0.4034128487110138, 0.37452423572540283, 0.517432689666748, -0.17476814985275269, -0.47397124767303467, 0.013001927174627781, -0.16247449815273285, 0.1028238907456398, -0.18016572296619415, 0.2570973336696625, -0.10767759382724762, -0.033160798251628876, 0.07027290761470795, -0.1372564435005188, -0.09541410207748413, -0.27794787287712097, -0.1812066286802292, -0.13608086109161377, -0.763504147529602, -0.15731573104858398, 0.026715582236647606, -0.4545944929122925, -0.07195643335580826, 0.34658336639404297, 0.26382455229759216, 0.3562905490398407, 0.26146113872528076, 0.24464990198612213, -0.1550905704498291, 0.16513077914714813, 0.41160115599632263, 0.20048077404499054, -1.1009106636047363, 0.5136821866035461, -0.12062445282936096, 0.2031525820493698, -0.08804283291101456, 0.13585300743579865, -0.18916715681552887, 0.1997670829296112, -0.30606740713119507, 0.1292237490415573, 0.32966580986976624, 0.18988816440105438, 0.28227072954177856, 0.34390538930892944, 0.07677371799945831, -0.2601190507411957, 0.19408634305000305, -0.1364031583070755, 0.14395645260810852, 0.21419470012187958, -0.06283456087112427, -0.368160605430603, -0.09601093828678131, -0.25458329916000366, -0.21803157031536102, -0.47760632634162903, -0.11175412684679031, -0.017293937504291534, 0.05136701464653015, -0.2783225476741791, -0.28221747279167175, 0.2303231656551361, -0.09452320635318756, -0.07567211240530014, 0.15652871131896973, 0.11396317929029465, -0.5258605480194092, 0.6639785170555115, -0.19191482663154602, 0.3867383301258087, 0.05272596701979637, 0.0453055277466774, 0.3573257625102997, 0.15300318598747253, -0.37935060262680054, -0.09556620568037033, 0.010641642846167088, -0.1859109103679657, -0.034518275409936905, 0.07957176864147186, -0.17098432779312134, -0.19220654666423798, -0.16269096732139587, -0.030108779668807983, 0.47731804847717285, 0.1021018773317337, 0.350419282913208, -0.19817237555980682, -0.32996007800102234, 0.30768096446990967, 0.09801099449396133, -0.17304880917072296, -0.08660667389631271, -0.15459026396274567, 0.12021864205598831, 0.48978349566459656, 0.051224157214164734, -0.10919879376888275, 0.342585027217865, -0.20196866989135742, -0.28273677825927734, -0.25331056118011475, 0.10021262615919113, 0.07512610405683517, -0.17142844200134277, -0.23421084880828857, 0.18119990825653076, 0.32957765460014343, 0.017264122143387794, -0.056214433163404465, 0.13320381939411163, -0.46494022011756897, -0.29496529698371887, 0.7940496802330017, -0.17402170598506927, -0.132097065448761, -0.26621490716934204, 0.20589037239551544, 0.31370753049850464, 0.09807411581277847, -0.12516121566295624, -0.04681963473558426, 0.1851959526538849, 0.22515957057476044, -0.018515193834900856, 0.04360149800777435, -0.32880809903144836, 0.05805771425366402, 0.005915514193475246, -0.09633304178714752, -0.31913527846336365, 0.5522058010101318, -0.038122791796922684, -0.5110398530960083, -0.23940104246139526, -0.0621660090982914, 0.22644583880901337, 0.16287395358085632, 0.3763807713985443, 0.06572311371564865, -0.2029065042734146, 0.2043800801038742, 0.007260452955961227, 0.47506028413772583, -0.44740068912506104, -0.22026026248931885, -0.05565144866704941, 0.325257271528244, 0.19914068281650543, -0.2555606961250305, -0.14136333763599396, 0.07573767006397247, -0.09314406663179398, 0.0606662891805172, -0.024978941306471825, -0.42886096239089966, 0.06158468499779701, -0.1104828417301178, -0.018679065629839897, 0.08223728090524673, 0.2738876938819885, -0.13104689121246338, -0.21754035353660583, -0.4055704176425934, -0.1578810065984726, -0.10061578452587128, -0.0055130235850811005, -0.1613035798072815, -0.19496095180511475, 0.34029433131217957, -0.18807800114154816, -0.29343679547309875, 0.10532950609922409, 0.34722819924354553, 0.14056052267551422, -0.010541231371462345, 0.6420748233795166, 0.1297224760055542, -0.45289623737335205, 0.17634762823581696, 0.4169778823852539, -0.1734301894903183, -0.14501982927322388, 0.10987064987421036, 0.47390297055244446, 0.37064823508262634, -0.04245498403906822, 0.3808020055294037, 0.16689924895763397, 0.2333495318889618, -0.1926618218421936, -1.0898644924163818, -0.09289921075105667, 0.04399362951517105, 0.16602225601673126, 0.09564048051834106, -0.28017500042915344, 0.16503828763961792, -0.17242421209812164, 0.40641486644744873, 0.4022336006164551, 0.20859786868095398, -0.0003317527880426496, -0.2595779597759247, 0.2763039171695709, 0.09534861892461777, 0.05268055200576782, -0.2580232620239258, 0.19498980045318604, -0.2744746506214142, 0.2282087504863739, -0.19661375880241394, 0.0018947615753859282, -0.0032692814711481333, -0.7909027934074402, 0.08387887477874756, -0.15562598407268524, 0.675560712814331, -0.0873863473534584, -0.2574503719806671, -0.060283832252025604, 0.17447678744792938, 0.5359846949577332, -0.002876357873901725, -0.49506837129592896, 0.2423897087574005, 0.3019571006298065, -0.037195876240730286, -0.2136649638414383, -0.08797294646501541, -0.30034929513931274, -0.3154507279396057, 0.0424729660153389, 0.08209803700447083, -0.4723023474216461, -0.5050961375236511, 0.15369568765163422, -0.038464952260255814, -0.04941479489207268, -0.3024318814277649, 0.2687636613845825, 0.09066387265920639, -0.22217820584774017, 0.2025536596775055, 0.017714401707053185, -0.3098820447921753, 0.11689651757478714, -0.3302643597126007, -0.014406923204660416, -0.12762735784053802, 0.14059863984584808, 0.2194635421037674, -0.08021791279315948, -0.09717710316181183, -0.3684641718864441, 0.08220355212688446, -0.28406280279159546, -0.012817174196243286, -0.004150900058448315, 0.1420285850763321, 0.1335640698671341, -0.19751955568790436, 0.4692970812320709, -0.13319453597068787, -0.013872476294636726, -0.04827670380473137, 0.2726285755634308, -0.029967624694108963, 0.04567299783229828, -0.28107890486717224, -0.13533402979373932, 0.585357666015625, 0.07186270505189896, 0.006305880844593048, 0.02490411326289177, 0.45594847202301025, -0.08118800818920135, 0.2959745228290558, -0.06146105006337166, 0.11082352697849274, 0.17394237220287323, 0.11317218095064163, -0.00132013950496912, -0.04070108011364937, -0.3247167766094208, 0.09967998415231705, 0.02791679836809635, -1.0822093486785889, 0.05488276854157448, -0.36678189039230347, 0.41318121552467346, -0.15467339754104614, 0.21742847561836243, 0.2543156147003174, -0.19435253739356995, 0.013973835855722427, -0.013270056806504726, -0.061294522136449814, 0.20488031208515167, 0.2477073222398758, -0.19439485669136047, 0.0848076343536377, -0.12422209978103638, 0.40417903661727905, -0.12414267659187317, 0.3003109395503998, -0.5216109752655029, 0.10053383558988571, 0.22755159437656403, 1.2133214473724365, -0.006570075172930956, 0.04098043963313103, 0.09904457628726959, -0.1518624722957611, -0.25925004482269287, 0.0032270937226712704, -0.09952990710735321, -0.19428102672100067, 0.04148910567164421, 0.6777359247207642, -0.07053251564502716, -0.037763193249702454, 0.5371408462524414, -0.2046184241771698, 0.007594247814267874, 0.41781628131866455, 0.33923372626304626, 0.18963013589382172, 0.06471965461969376, 0.03320294991135597, -0.07561434060335159, 0.72076815366745, 0.07305312156677246, 0.06612711399793625, -0.1628185659646988, 0.07744928449392319, -0.11619670689105988, 0.03349703922867775, -0.09173978865146637, -0.016341041773557663, -0.0010877382010221481, 0.2577179968357086, 0.1297355741262436, -0.10002636909484863, -0.13469800353050232, -0.1189081221818924, -0.13371652364730835, 0.2611842453479767, 0.06647265702486038, 0.1298670619726181, -0.0831979438662529, -0.2017955482006073], "sparse_embedding": null}, {"id": "4a7341f384539567332169b8690ba66532249f88f28a5bed368d04e5fae6bf96", "content": "Fine-tuning Large Language Models (LLMs) adapts a trained model to specific\ndownstream tasks, significantly improving task-specific performance. Supervised\nFine-Tuning (SFT) is a common approach, where an LLM is trained to produce\ndesired answers. However, LLMs trained with SFT sometimes make simple mistakes\nand result in hallucinations on reasoning tasks such as question-answering.\nWithout external feedback, it is difficult for SFT to learn a good mapping\nbetween the question and the desired answer, especially with a small dataset.\nThis paper introduces an alternative to SFT called Natural Language Feedback\nfor Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they\nwill receive from an annotator. We find that requiring such reflection can\nsignificantly improve the accuracy in in-domain question-answering tasks,\nproviding a promising direction for the application of natural language\nfeedback in the realm of SFT LLMs. Additional ablation studies show that the\nportion of human-annotated data in the annotated datasets affects the\nfine-tuning performance.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2401.00907v1", "title": "LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models", "content": "http://arxiv.org/pdf/2401.00907v1", "datetime": "2023-12-31 21:18:16", "query": "human feedback LLMs", "linkedin": "\ud83c\udf1f Exciting News in the World of Large Language Models (LLMs) \ud83c\udf1f\n\nFine-tuning LLMs for specific tasks has always been crucial for enhancing performance. However, Supervised Fine-Tuning (SFT) can sometimes lead to errors and hallucinations in reasoning tasks. But fear not, a new approach called LaFFi (Natural Language Feedback for Finetuning LLMs) is here to revolutionize the game!\n\nThis innovative method has LLMs predict the feedback they would receive from an annotator, leading to a significant boost in accuracy for in-domain question-answering tasks. The study also highlights the impact of human-annotated data on fine-tuning performance.\n\nExcited to delve deeper into this groundbreaking research? Check out the full paper here: http://arxiv.org/abs/2401.00907v1\n\n#LLMs #AI #NLP #LaFFi #TechInnovation #ResearchHighlight", "x": "\ud83d\ude80 Exciting research alert! Discover LaFFi, a new approach to fine-tuning Large Language Models for improved task performance in question-answering tasks. Learn more about how natural language feedback enhances accuracy and fine-tuning performance: http://arxiv.org/abs/2401.00907v1 #AI #NLP #LLMs #LaFFi #ResearchPub", "source_id": "7edf3e123d052c48e0fa2498502df883160c727523ff7609a0a3129335de1f09", "page_number": 1}, "score": null, "embedding": [-0.18881990015506744, -0.1742304563522339, 0.11113766580820084, 0.06942646950483322, 0.05730608105659485, -0.06252535432577133, -0.12784701585769653, 0.2739396393299103, 0.3939618766307831, -0.19010469317436218, 0.007498858962208033, -0.1807376742362976, 0.24801747500896454, 0.3255639374256134, 0.25762873888015747, 0.14028292894363403, -0.05249619111418724, 0.2677099108695984, -0.0469801239669323, 0.08368829637765884, 0.2633594274520874, 0.27752894163131714, -0.012388101778924465, 0.1118558794260025, -0.08607103675603867, -0.27659812569618225, -0.2063436657190323, -0.38443443179130554, -0.34549394249916077, -1.3743571043014526, 0.0014424658147618175, 0.08417098224163055, 0.19361142814159393, 0.02831849828362465, -0.302944540977478, 0.04373200610280037, -0.30210110545158386, -0.049863819032907486, -0.0015401156852021813, 0.1270216852426529, 0.1638987511396408, 0.1961647868156433, -0.05743657797574997, -0.1822182685136795, -0.049268484115600586, -0.3934713304042816, -0.17644937336444855, -0.24226486682891846, -0.6711851358413696, -0.01902605965733528, -0.2736741304397583, -0.4426096975803375, 0.00516262324526906, 0.15862496197223663, 0.14209401607513428, 0.1250549554824829, 0.06675805896520615, 0.23745761811733246, 0.27282971143722534, 0.15617457032203674, 0.08967683464288712, 0.42697563767433167, -1.026731014251709, 0.2976211607456207, -0.13642087578773499, 0.13663165271282196, -0.02203293703496456, 0.06147344037890434, 0.042503684759140015, 0.20100410282611847, -0.18673008680343628, 0.10775915533304214, 0.2617695927619934, 0.22334393858909607, 0.34503498673439026, 0.16489821672439575, 0.0076518915593624115, -0.075129933655262, 0.16670815646648407, 0.23059068620204926, 0.1740919053554535, 0.02645176462829113, -0.02195117063820362, -0.4174284040927887, -0.02900664322078228, -0.16436447203159332, -0.24981489777565002, -0.030627721920609474, 0.12543649971485138, 0.07345729321241379, 0.12151217460632324, -0.034978803247213364, -0.23727205395698547, 0.3005911409854889, 0.017644476145505905, -0.12802550196647644, 0.2489665448665619, 0.07822751253843307, -0.2774736285209656, 0.5862375497817993, -0.1200219988822937, 0.1515795737504959, -0.16853591799736023, -0.15699335932731628, 0.17807160317897797, -0.07558028399944305, -0.07776346057653427, -0.27215802669525146, -0.060520488768815994, -0.05851353704929352, -0.2020893394947052, -0.048817530274391174, 0.09770908951759338, -0.24505777657032013, -0.04722756892442703, 0.14123830199241638, 0.5006738901138306, 0.04866959899663925, 0.049685124307870865, 0.03014012798666954, 0.10165060311555862, 0.16774208843708038, 0.14029784500598907, -0.2188061773777008, 0.3690018355846405, 0.021277794614434242, 0.011510065756738186, 0.31228184700012207, 0.08016210049390793, 0.28666335344314575, 0.2671111524105072, -0.3947509527206421, -0.29535651206970215, -0.2548457980155945, 0.12310511618852615, 0.012942849658429623, 0.20842649042606354, -0.3907281458377838, 0.010610791854560375, 0.12960191071033478, 0.005962069611996412, -0.036557409912347794, 0.12969449162483215, -0.02568410150706768, -0.4631737470626831, 0.5950577259063721, -0.23266687989234924, -0.21967267990112305, -0.27287599444389343, -0.1895664632320404, 0.047076545655727386, 0.2900829315185547, -0.0930066928267479, -0.21191589534282684, 0.32391244173049927, 0.27130234241485596, 0.2712076008319855, -0.09055893868207932, -0.3687552213668823, -0.0499831847846508, -0.10706422477960587, -0.19611884653568268, -0.16917350888252258, 0.5200452208518982, -0.11475121974945068, -0.22017844021320343, -0.5411948561668396, -0.08833782374858856, 0.1604907214641571, -0.2938133478164673, 0.36025065183639526, -0.0609469898045063, -0.06170933321118355, 0.15242978930473328, -0.1584244966506958, 0.24137194454669952, -0.4934968948364258, 0.09764715284109116, -0.175085186958313, 0.19984164834022522, 0.1654299944639206, -0.30521851778030396, -0.026569582521915436, 0.12069852650165558, -0.13392768800258636, -0.32102206349372864, 0.010778238996863365, -0.28254425525665283, -0.08243996649980545, -0.17992927134037018, -0.22677384316921234, 0.0795757845044136, 0.029883598908782005, -0.035706814378499985, -0.06146902218461037, -0.17227411270141602, 0.08739177882671356, 0.14698761701583862, 0.044786348938941956, 0.022641105577349663, -0.04006722569465637, 0.14888805150985718, 0.020411832258105278, -0.08557039499282837, -0.11030115932226181, -0.13383124768733978, -0.07372556626796722, 0.0018689847784116864, 0.18253642320632935, 0.4502736032009125, -0.32818928360939026, 0.17055794596672058, 0.25192442536354065, 0.05890214443206787, -0.15024083852767944, 0.14592763781547546, 0.04325733706355095, 0.16368204355239868, -0.242481529712677, 0.1375504583120346, 0.3124679625034332, 0.20604436099529266, -0.35787433385849, -1.0474014282226562, -0.32684534788131714, -0.01017039269208908, 0.1790458709001541, 0.3229750692844391, -0.27475908398628235, 0.2629026174545288, 0.09657180309295654, 0.23776672780513763, 0.2769966125488281, -0.22840256989002228, 0.0052042268216609955, -0.24300995469093323, -0.02034865878522396, -0.02943223901093006, 0.04727102816104889, -0.05242830142378807, 0.14654803276062012, -0.20187222957611084, 0.15696431696414948, -0.19937734305858612, 0.3938389718532562, 0.22565391659736633, -0.7179184556007385, 0.11470027267932892, -0.06856303662061691, 0.72499680519104, -0.35376158356666565, 0.22031337022781372, -0.33958572149276733, 0.21617744863033295, 0.19415579736232758, -0.01541935559362173, -0.32555529475212097, 0.6525422930717468, -0.003733246121555567, 0.2356986105442047, 0.14598141610622406, 0.17261160910129547, 0.059090208262205124, -0.09641533344984055, -0.11459033191204071, -0.0874691903591156, -0.5085548162460327, -0.6074323654174805, 0.190103217959404, 0.010113496333360672, -0.3275963366031647, -0.3630777895450592, 0.19152572751045227, -0.024950707331299782, -0.09680957347154617, 0.21526561677455902, 0.04006417840719223, -0.10433440655469894, -0.1773797869682312, -0.5856269598007202, 0.1843520700931549, -0.24050162732601166, -0.03142831102013588, 0.20583046972751617, -0.023320503532886505, -0.2757153809070587, -0.2171315848827362, 0.08488038182258606, -0.08985135704278946, -0.03903364762663841, 0.06200362369418144, 0.1733284443616867, 0.055717337876558304, -0.11330325901508331, 0.5372888445854187, 0.15327610075473785, 0.18974831700325012, 0.040395088493824005, 0.247809499502182, -0.0788750946521759, -0.08069087564945221, -0.2454630583524704, -0.040178265422582626, 0.7091630697250366, 0.3323575556278229, 0.26623398065567017, 0.14034131169319153, -0.13080959022045135, -0.00420492934063077, 0.4759479761123657, -0.05843915417790413, 0.17664888501167297, 0.29412195086479187, 0.007728148251771927, 0.1553727239370346, -0.2687159478664398, 0.0017160787247121334, 0.15946021676063538, 0.2888474762439728, -1.0740751028060913, -0.02059890702366829, 0.17039142549037933, 0.029155828058719635, -0.11260442435741425, -0.01082220021635294, 0.011610375717282295, -0.16315416991710663, 0.1626356989145279, 0.24158969521522522, -0.23238202929496765, 0.16356846690177917, 0.06633954495191574, 0.14188417792320251, -0.14489038288593292, -0.09009833633899689, 0.41368937492370605, 0.007084965240210295, 0.42862388491630554, -0.4096318483352661, -0.13306424021720886, 0.29001688957214355, 1.0184909105300903, -0.12487716972827911, 0.1565215140581131, 0.06113152578473091, -0.1359071284532547, -0.1846306025981903, 0.3665546476840973, -0.17480586469173431, 0.14578665792942047, -0.01320378016680479, 0.7816169261932373, -0.06331896781921387, 0.06645162403583527, 0.3522666096687317, -0.15147371590137482, -0.08470199257135391, 0.11495544761419296, 0.07089179009199142, 0.38795900344848633, -0.0834883600473404, 0.1935742050409317, -0.245393306016922, 0.5077098608016968, 0.028867356479167938, 0.07974576205015182, -0.16843169927597046, -0.2915903925895691, -0.12565521895885468, 0.021983448415994644, 0.04384427145123482, 0.0687895268201828, -0.2241245061159134, 0.24219898879528046, 0.07715386897325516, 0.1215972751379013, -0.14747241139411926, -0.16336460411548615, -0.13976602256298065, 0.14477816224098206, -0.26079925894737244, 0.0556686632335186, 0.028104674071073532, -0.39358747005462646], "sparse_embedding": null}, {"id": "26fda650c80ea2b7e0b8008755b9c4501791c8d9746bcd00367481eee390a8a1", "content": "The reward model for Reinforcement Learning from Human Feedback (RLHF) has\nproven effective in fine-tuning Large Language Models (LLMs). Notably,\ncollecting human feedback for RLHF can be resource-intensive and lead to\nscalability issues for LLMs and complex tasks. Our proposed framework Proto-RM\nleverages prototypical networks to enhance reward models under limited human\nfeedback. By enabling stable and reliable structural learning from fewer\nsamples, Proto-RM significantly enhances LLMs' adaptability and accuracy in\ninterpreting human preferences. Extensive experiments on various datasets\ndemonstrate that Proto-RM significantly improves the performance of reward\nmodels and LLMs in human feedback tasks, achieving comparable and usually\nbetter results than traditional methods, while requiring significantly less\ndata. in data-limited scenarios. This research offers a promising direction for\nenhancing the efficiency of reward models and optimizing the fine-tuning of\nlanguage models under restricted feedback conditions.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2406.06606v1", "title": "Prototypical Reward Network for Data-Efficient RLHF", "content": "http://arxiv.org/pdf/2406.06606v1", "datetime": "2024-06-06 15:23:30", "query": "human feedback LLMs", "linkedin": "\ud83c\udf1f Exciting News in AI and NLP! \ud83c\udf1f\n\nThe latest research introduces Proto-RM, a cutting-edge framework leveraging prototypical networks to enhance reward models for Large Language Models (LLMs) in Reinforcement Learning from Human Feedback (RLHF) tasks. Proto-RM enables stable and reliable structural learning from limited human feedback, significantly boosting LLMs' adaptability and accuracy in interpreting human preferences.\n\n\ud83d\udd0d Research Results:\nExtensive experiments across various datasets demonstrate that Proto-RM outperforms traditional methods in human feedback tasks, achieving comparable or even superior results while requiring significantly less data in data-limited scenarios.\n\n\ud83d\ude80 Dive into the details of this groundbreaking research at:\nhttp://arxiv.org/abs/2406.06606v1\n\n#AI #NLP #LLMs #ProtoRM #HumanFeedback #ReinforcementLearning #Research #Innovation #TechBreakthrough", "x": "\ud83d\ude80 Exciting new research alert! Proto-RM framework enhances reward models for Large Language Models under limited human feedback, boosting adaptability and accuracy. \ud83e\udd16\ud83d\udcc8 Check out the results here: http://arxiv.org/abs/2406.06606v1 #AI #NLP #LLMs #ProtoRM #Research #TechInnovation", "source_id": "f5950674d60fdc0a8fd0612a5e8f82541d3fb6891e206c8ce81869bffb546b4c", "page_number": 1}, "score": null, "embedding": [-0.5076338052749634, 0.2112632840871811, -0.002375410171225667, 0.07235140353441238, -0.2344330996274948, 0.22247828543186188, -0.18212249875068665, 0.22744961082935333, 0.3288196623325348, -0.1590958833694458, 0.1614353358745575, -0.21745431423187256, 0.2836889922618866, 0.5909157395362854, 0.21602249145507812, 0.03658078983426094, -0.3552290201187134, 0.1585099995136261, -0.2469366192817688, -0.06878729909658432, 0.1023971438407898, -0.3610042631626129, 0.07093511521816254, -0.12505485117435455, -0.015475261956453323, -0.10996562242507935, -0.23131825029850006, -0.18087120354175568, -0.29097625613212585, -1.4042857885360718, 0.24323254823684692, -0.002054220763966441, 0.3271946310997009, -0.08146175742149353, -0.26940643787384033, 0.030101612210273743, -0.28084003925323486, -0.03583955764770508, 0.041861653327941895, 0.23942962288856506, 0.023372970521450043, 0.07076139748096466, 0.020492155104875565, -0.05456020310521126, -0.005865852348506451, -0.2942917048931122, -0.24887968599796295, -0.2091110497713089, -0.48744258284568787, 0.024844106286764145, -0.11758793145418167, -0.23592980206012726, 0.06888376921415329, 0.26961570978164673, 0.3335130512714386, 0.09955231100320816, 0.39719483256340027, 0.3896055519580841, 0.016602056100964546, 0.045215457677841187, 0.06388477236032486, 0.38966134190559387, -0.9556942582130432, 0.10631561279296875, -0.004120799247175455, 0.31368815898895264, -0.27433302998542786, 0.16624827682971954, -0.041388146579265594, 0.12787236273288727, -0.270302951335907, 0.1831977218389511, 0.3125191032886505, 0.1120915338397026, 0.21722090244293213, 0.20035095512866974, 0.0389239639043808, -0.40574410557746887, 0.1935698539018631, 0.07103798538446426, 0.1789695769548416, -0.27609017491340637, -0.12882855534553528, -0.3813292682170868, -0.017823947593569756, -0.07158162444829941, -0.24680086970329285, -0.38241297006607056, -0.008253256790339947, 0.21685004234313965, 0.1414862722158432, 0.10212580114603043, -0.18979009985923767, 0.3621658980846405, -0.4925491511821747, 0.08370795845985413, 0.19857290387153625, -0.17256873846054077, -0.4570589065551758, 0.635718584060669, 0.19235551357269287, 0.35345423221588135, -0.1245277002453804, -0.056617267429828644, 0.4299793243408203, 0.06642773002386093, 0.07455529272556305, 0.0900898352265358, -0.20357641577720642, -0.16306445002555847, -0.12778200209140778, 0.013756630010902882, 0.17509901523590088, -0.19080296158790588, 0.022553550079464912, 0.0626448467373848, 0.33885422348976135, 0.17884981632232666, 0.0899248868227005, -0.18447484076023102, -0.2822483479976654, 0.2080310434103012, 0.17938722670078278, -0.38439977169036865, 0.10657309740781784, -0.04759438708424568, 0.011752420105040073, 0.4924396872520447, 0.03168540447950363, 0.029428303241729736, 0.11418160796165466, -0.2840118706226349, -0.31394293904304504, -0.20310461521148682, 0.19012653827667236, 0.1762620210647583, -0.05347271263599396, -0.42913272976875305, 0.10730557888746262, 0.35685208439826965, 0.1179753914475441, 0.196011021733284, 0.043518830090761185, -0.49614807963371277, -0.4137841761112213, 0.32456067204475403, 0.1451730728149414, 0.07090311497449875, -0.25354400277137756, -0.0054267472587525845, 0.27817967534065247, 0.32538196444511414, -0.240451380610466, -0.11874008178710938, -0.03326734900474548, 0.06676933169364929, 0.07074600458145142, 0.04986920207738876, -0.26542824506759644, -0.061924614012241364, -0.14988167583942413, -0.19915616512298584, -0.08931464701890945, 0.5382533669471741, -0.1544029861688614, -0.4395756721496582, -0.4416220784187317, -0.06603996455669403, 0.05772338807582855, -0.15952268242835999, 0.25070610642433167, 0.14721602201461792, -0.2402869313955307, 0.06706300377845764, 0.04369184002280235, 0.12164818495512009, -0.9509038329124451, -0.07155822217464447, 0.09199745953083038, 0.10985150188207626, 0.14319705963134766, -0.27054330706596375, -0.07696917653083801, 0.00831315852701664, 0.14645132422447205, 0.09065566956996918, 0.15125899016857147, -0.3043593764305115, 0.18349558115005493, -0.22945265471935272, -0.2233743965625763, 0.19074547290802002, 0.0517553836107254, 0.035004064440727234, -0.030762070789933205, -0.4707503020763397, 0.11876250803470612, -0.19656406342983246, -0.019138259813189507, 0.11474430561065674, -0.0678940862417221, 0.29445943236351013, -0.04672032967209816, -0.11058865487575531, 0.08630819618701935, -0.004906354006379843, -0.04382605850696564, 0.1642490178346634, 0.4453088641166687, 0.1227584034204483, -0.22960805892944336, 0.05364328250288963, 0.3764004707336426, 0.030245350673794746, -0.2850564122200012, -0.006194363348186016, 0.3549390137195587, 0.17485466599464417, -0.10659002512693405, 0.12195882946252823, 0.26799917221069336, 0.3118063807487488, -0.22347255051136017, -1.122621774673462, -0.07187262177467346, 0.12366842478513718, 0.06225629150867462, 0.4161428213119507, -0.27669557929039, 0.3174901604652405, -0.14215616881847382, 0.3336968719959259, 0.14667047560214996, 0.24297834932804108, 0.2897587716579437, -0.43033865094184875, 0.14430390298366547, -0.02716725692152977, 0.0655336007475853, -0.0005438114167191088, -0.02674715779721737, -0.059013448655605316, 0.20369039475917816, -0.2700542211532593, 0.2324501872062683, 0.184856578707695, -0.6285195350646973, 0.2849278450012207, -0.2978830933570862, 0.7498520016670227, -0.21652226150035858, -0.14649903774261475, -0.17838676273822784, 0.08347612619400024, 0.5738422870635986, -0.012973633594810963, -0.4301255941390991, 0.3493445813655853, 0.12312726676464081, 0.17751267552375793, 0.11135866492986679, 0.0039598699659109116, -0.2026297003030777, -0.07603573054075241, -0.0216843094676733, -0.03765105456113815, -0.6289911270141602, -0.5212419629096985, 0.3163541555404663, -0.10115873068571091, -0.005264811217784882, -0.35621675848960876, 0.13225291669368744, 0.16996034979820251, 0.04348820075392723, 0.15203292667865753, -0.16253088414669037, -0.13940677046775818, -0.11729353666305542, -0.3999823033809662, 0.0471482053399086, -0.34120243787765503, 0.07611538469791412, 0.26689913868904114, -0.20870870351791382, -0.2409474402666092, -0.07607918977737427, 0.4297540783882141, -0.27984610199928284, -0.024331597611308098, 0.0029979730024933815, 0.14700688421726227, 0.034342944622039795, -0.01023548748344183, 0.48575279116630554, 0.051334936171770096, 0.33384260535240173, 0.16146360337734222, 0.13696277141571045, -0.3260931074619293, -0.14289644360542297, -0.45903754234313965, 0.04465712979435921, 0.491524875164032, 0.15665818750858307, 0.08477448672056198, 0.34186241030693054, 0.126070037484169, -0.10188842564821243, 0.3775225877761841, -0.012151663191616535, 0.054585497826337814, 0.1597834676504135, -0.06104007363319397, -0.014811898581683636, -0.13093559443950653, -0.24029406905174255, 0.05957707017660141, 0.13068079948425293, -1.2993826866149902, -0.011558099649846554, -0.14918896555900574, 0.186833918094635, -0.12976416945457458, 0.025744127109646797, 0.2755616009235382, -0.17525625228881836, -0.04635695740580559, 0.04670428857207298, 0.002714108908548951, 0.4422307014465332, 0.3101012408733368, 0.05298089236021042, 0.19795449078083038, 0.036885302513837814, 0.39912450313568115, -0.04762418568134308, 0.24381084740161896, -0.5455574989318848, -0.04202524572610855, 0.4968622326850891, 1.0919982194900513, -0.20846183598041534, 0.17896686494350433, 0.0905921682715416, -0.13249200582504272, -0.23923929035663605, 0.22440044581890106, 0.009589740075170994, -0.33582472801208496, 0.18024371564388275, 0.5016611814498901, -0.02863950841128826, 0.03491382300853729, 0.6648345589637756, -0.1731179803609848, -0.13266988098621368, 0.2857299745082855, -0.037493996322155, 0.14642761647701263, 0.16208136081695557, 0.046581167727708817, -0.0769919753074646, 0.42974168062210083, -0.06257515400648117, 0.30307716131210327, -0.31896984577178955, -0.17604313790798187, 0.04874114319682121, -0.09964033961296082, 0.08531763404607773, 0.10598807781934738, -0.16687741875648499, 0.02706722356379032, 0.10385994613170624, 0.07256396859884262, -0.2576060891151428, -0.23422110080718994, -0.2278885394334793, 0.16754399240016937, 0.24282622337341309, 0.21129803359508514, -0.004902644548565149, -0.3862050771713257], "sparse_embedding": null}, {"id": "63f77ec106ec48f8bdf1b500b894faddf3808367057f1138ce80396ce9681d20", "content": "With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.18341v1", "title": "IterAlign: Iterative Constitutional Alignment of Large Language Models", "content": "http://arxiv.org/pdf/2403.18341v1", "datetime": "2024-03-27 08:32:19", "query": "human feedback LLMs", "linkedin": "\ud83d\ude80 Exciting advancements in the field of Large Language Models (LLMs) are shaping the future of AI! Ensuring the alignment of LLMs with human values and societal norms is crucial for their reliability and safety. \n\nDiscover how IterAlign, a data-driven constitution discovery and self-alignment framework, is revolutionizing LLM alignment by automatically uncovering new constitutions to guide self-correction. This innovative approach leverages red teaming to identify weaknesses in LLMs and enhance their alignment without heavy human annotations or predefined constitutions.\n\nCheck out the research paper to learn more about IterAlign and its impressive results in improving LLM alignment by up to 13.5% in harmlessness: http://arxiv.org/abs/2403.18341v1\n\n#LLM #AIalignment #TechInnovation #AIethics #IterAlign #AIresearch #TechAdvancements", "x": "\ud83d\ude80 Exciting development in the realm of LLM alignment! Introducing IterAlign, a data-driven constitution discovery and self-alignment framework for large language models. Discover how IterAlign leverages red teaming to enhance LLM alignment by up to 13.5% in harmlessness! Check out the research at: http://arxiv.org/abs/2403.18341v1 #AI #LLM #Alignment #TechResearch", "source_id": "06013f7d20fc3af067350396c1dec456c3f4892b9bae5bd32652c44754edee6e", "page_number": 1}, "score": null, "embedding": [-0.4456183612346649, -0.1337900161743164, 0.17718903720378876, -0.23251330852508545, -0.21915486454963684, -0.008452564477920532, -0.16320158541202545, -0.1947324424982071, 0.09231594204902649, -0.3172624409198761, 0.1823442429304123, 0.010773969814181328, 0.03021608665585518, 0.17049472033977509, 0.2844201326370239, 0.08898360282182693, -0.27512720227241516, 0.32230380177497864, -0.2750456929206848, 0.020768681541085243, 0.2561083436012268, -0.21053177118301392, -0.146995410323143, 0.07562363892793655, 0.1787581443786621, 0.1163845956325531, -0.21281498670578003, -0.09184380620718002, -0.09219372272491455, -1.4064240455627441, 0.10215888917446136, 0.10516969114542007, 0.19333624839782715, -0.026084674522280693, -0.31090518832206726, -0.034643035382032394, -0.2694185972213745, 0.26832857728004456, 0.01495333481580019, 0.11808119714260101, -0.06166844069957733, 0.04386382922530174, 0.1805233359336853, -0.020769396796822548, -0.05550781264901161, -0.22598212957382202, -0.05375707522034645, 0.03610185906291008, -0.4008961319923401, 0.005411775317043066, -0.036053646355867386, -0.2993018329143524, 0.10917264223098755, 0.32058191299438477, -0.09939543157815933, 0.33238714933395386, 0.2754276394844055, 0.3060586452484131, 0.05427347496151924, 0.04424864053726196, 0.28752750158309937, 0.26122868061065674, -0.9698907136917114, 0.38084283471107483, 0.11210709065198898, 0.24658650159835815, 0.020637309178709984, -0.1840120255947113, 0.12669914960861206, 0.1725091189146042, 0.08861912786960602, 0.12355986982584, 0.3391585648059845, -0.015307075344026089, 0.23047487437725067, 0.36645829677581787, 0.21316154301166534, -0.12316937744617462, 0.22690771520137787, -0.17306795716285706, 0.027969930320978165, 0.11476979404687881, 0.05776999145746231, 0.026227178052067757, -0.22774529457092285, -0.13354746997356415, -0.3439910411834717, -0.4209158420562744, 0.17731530964374542, 0.0916806012392044, -0.015162641182541847, -0.18157996237277985, -0.10805212706327438, 0.2714301347732544, -0.37138983607292175, -0.10238722711801529, -0.07227615267038345, 0.017879804596304893, -0.39671579003334045, 0.4354056119918823, -0.10524550825357437, 0.2203190177679062, -0.2988198399543762, 0.1415443867444992, 0.4717518091201782, 0.11915078014135361, -0.2788591682910919, -0.35816383361816406, -0.12632402777671814, -0.26644060015678406, 0.09824448823928833, 0.07061944156885147, 0.09078457951545715, -0.2228436917066574, -0.09002046287059784, 0.06230618432164192, 0.17792633175849915, 0.15602104365825653, 0.09388495236635208, -0.08621478080749512, -0.002650573616847396, 0.21073707938194275, 0.05897519737482071, -0.0031990190036594868, 0.11852921545505524, -0.15981219708919525, 0.11417348682880402, 0.20066416263580322, 0.1548825353384018, -0.0306834876537323, 0.19153772294521332, 0.008888939395546913, -0.11119548976421356, -0.3055724501609802, 0.06120211258530617, -0.004685243126004934, -0.24952654540538788, -0.1278115063905716, 0.24208037555217743, 0.14225409924983978, -0.17482323944568634, 0.17853766679763794, 0.12301246076822281, -0.3174837529659271, -0.4934144616127014, 0.5085689425468445, 0.03998155891895294, 0.10841524600982666, -0.23923173546791077, 0.1693752259016037, 0.0473947711288929, 0.036710210144519806, -0.24717488884925842, -0.21192537248134613, -0.06371969729661942, 0.07915041595697403, 0.0021271007135510445, 0.23514825105667114, -0.24824541807174683, -0.07399682700634003, -0.06950286030769348, -0.07683522999286652, -0.2265663743019104, 0.7545191645622253, -0.18293587863445282, -0.47515547275543213, -0.24145357310771942, -0.030521370470523834, 0.09958511590957642, -0.01859932206571102, 0.18026648461818695, -0.02614576369524002, -0.09010486304759979, 0.08612783998250961, -0.12696489691734314, 0.23261842131614685, -0.47131091356277466, -0.13081972301006317, 0.08033370226621628, 0.314644455909729, 0.13939638435840607, -0.3065379559993744, -0.43796172738075256, 0.0323762521147728, -0.03665200620889664, 0.03659917041659355, -0.057723354548215866, -0.4100187122821808, 0.30613064765930176, -0.12866057455539703, -0.055943313986063004, 0.7064361572265625, -0.051145125180482864, 0.013591502793133259, -0.2855040729045868, -0.315266489982605, 0.005559223238378763, -0.03277949243783951, 0.25706690549850464, -0.007071386091411114, -0.16587664186954498, 0.2671453058719635, -0.008903972804546356, -0.034900963306427, 0.10375412553548813, 0.04734940826892853, -0.05941367149353027, 0.006077786907553673, 0.41958561539649963, 0.2962462306022644, -0.09542103111743927, 0.051855284720659256, 0.2356485277414322, -0.18100076913833618, -0.26341497898101807, 0.13609060645103455, 0.3759905993938446, 0.3960668742656708, -0.34199586510658264, 0.20263579487800598, 0.30391958355903625, 0.2420564889907837, -0.01818975992500782, -1.0963997840881348, -0.12247400730848312, -0.25594791769981384, 0.06198707967996597, 0.30620214343070984, -0.1200682520866394, 0.04164969548583031, -0.0598866306245327, 0.07581499218940735, 0.30782967805862427, 0.09763262420892715, 0.30147191882133484, -0.38061222434043884, 0.28161391615867615, -0.05947783961892128, 0.025713810697197914, 0.06171609088778496, 0.10262101143598557, -0.042807869613170624, 0.32817989587783813, -0.2608208954334259, 0.14392299950122833, -0.19683264195919037, -0.6728402376174927, 0.09113596379756927, -0.13394440710544586, 0.7753911018371582, -0.3291132152080536, -0.04108775779604912, 0.09053333103656769, 0.08410724252462387, 0.26557156443595886, -0.12288304418325424, -0.40210598707199097, 0.3495154082775116, 0.21298933029174805, -0.013680190779268742, 0.04356983304023743, -0.29040709137916565, -0.330029159784317, -0.24693411588668823, -0.11337614804506302, 0.2183859795331955, -0.5913443565368652, -0.3216803967952728, 0.23794016242027283, -0.25381433963775635, -0.05086066201329231, -0.38167497515678406, 0.1211250051856041, 0.28430402278900146, -0.050027698278427124, 0.04228726774454117, 0.1862417608499527, -0.3478364944458008, 0.11367128044366837, -0.36526238918304443, 0.030023809522390366, -0.2614377737045288, 0.1784108281135559, 0.24550101161003113, 0.1826881319284439, 0.04786817729473114, -0.04834303632378578, 0.186509907245636, -0.099151611328125, -0.05044655129313469, 0.07438045740127563, 0.20531383156776428, -0.20067301392555237, -0.058809276670217514, 0.4317375719547272, -0.13245390355587006, 0.13998699188232422, -0.13889944553375244, 0.1485704779624939, 0.006567893549799919, 0.09377425909042358, -0.18404728174209595, 0.1981908231973648, 0.3262089192867279, 0.09328148514032364, 0.09398295730352402, -0.22626101970672607, 0.14738377928733826, 0.05626224726438522, 0.21431082487106323, 0.20550870895385742, 0.24821095168590546, 0.02480270527303219, 0.030001172795891762, 0.10725170373916626, -0.1444602608680725, -0.18624049425125122, 0.0581628642976284, 0.052836205810308456, -1.0686684846878052, 0.18187707662582397, -0.37180912494659424, 0.38122332096099854, -0.3881549835205078, -0.008605193346738815, 0.22774638235569, -0.0871119275689125, 0.12174305319786072, -0.10447137802839279, -0.08546456694602966, 0.25378403067588806, 0.14760848879814148, -0.036113474518060684, 0.006371043156832457, -0.20377764105796814, 0.48252490162849426, -0.30331578850746155, 0.18554995954036713, -0.6464899182319641, -0.0621735081076622, 0.34611624479293823, 1.0567225217819214, -0.07151586562395096, -0.1596088707447052, 0.39413875341415405, 0.014925248920917511, -0.08016978949308395, -0.028186971321702003, 0.03302871808409691, -0.16253668069839478, -0.017545070499181747, 0.38016825914382935, 0.16173668205738068, -0.05931776016950607, 0.29076099395751953, -0.3352479636669159, -0.17762866616249084, 0.3080706000328064, 0.33657848834991455, -0.006836298853158951, 0.021014036610722542, 0.21403001248836517, -0.17416958510875702, 0.5315818190574646, 0.06492849439382553, 0.08792680501937866, -0.12256188690662384, 0.22966104745864868, 0.10292606055736542, -0.042752135545015335, -0.049810271710157394, -0.029202934354543686, 0.019817080348730087, 0.1264108121395111, 0.13276687264442444, 0.10683849453926086, -0.06462060660123825, -0.0512387678027153, -0.18312609195709229, 0.04121432453393936, 0.07969322800636292, 0.057050686329603195, 0.09066882729530334, -0.22312818467617035], "sparse_embedding": null}, {"id": "5ae61d4f0c8bb762479a22d3fb4c1155077d5b3ae5d1affb4f5fc8397ccc44ee", "content": "Although Large Language Models (LLMs) have shown strong performance in\nMulti-hop Question Answering (MHQA) tasks, their real reasoning ability remains\nexploration. Current LLM QA evaluation benchmarks have shown limitations,\nincluding 1) data contamination, the evaluation data are potentially exposed to\nLLMs during the pretraining stage; and 2) ignoration of the reasoning chain\nevaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA\nbenchmark based on the new, unprecedented knowledge by editing the\noff-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the\nreasoning chain in the form of sub-questions and intermediate answers\ncorresponding to the multi-hop questions. Specifically, based on the\nobservation, 1) LLMs show a performance gap between the original HotpotQA and\nour edited data, deeming that current MHQA benchmarks have the potential risk\nof data contamination that hard to evaluate LLMs' performance objectively and\nscientifically; 2) LLMs only get a small percentage of the right reasoning\nchain, e.g. GPT-4 only gets 36.3\\% right reasoning chain. We believe this new\nMulti-hop QA evaluation benchmark and novel evaluation methods will facilitate\nthe development of trustworthy LLM evaluation on the MHQA task.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.11924v2", "title": "MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition", "content": "http://arxiv.org/pdf/2402.11924v2", "datetime": "2024-03-03 02:23:19", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting news in the world of Large Language Models (LLMs) and Multi-hop Question Answering (MHQA) tasks! \ud83c\udf1f\n\nA recent study has shed light on the limitations of current LLM QA evaluation benchmarks, highlighting issues such as data contamination and the lack of evaluation of reasoning chains. To address these shortcomings, a new LLM MHQA evaluation benchmark has been introduced, based on the HotpotQA dataset. This benchmark includes annotated reasoning chains in the form of sub-questions and intermediate answers, providing a more comprehensive evaluation of LLM performance.\n\nKey findings from the study include a performance gap between the original HotpotQA dataset and the new benchmark, as well as LLMs achieving only a small percentage of correct reasoning chains. For example, GPT-4 scored 36.3% in this aspect.\n\nFor more details on this groundbreaking research and its implications for the development of trustworthy LLM evaluation in MHQA tasks, check out the full paper here: http://arxiv.org/abs/2402.11924v2\n\n#LLM #MHQA #AI #NLP #Research #Tech #Innovation\n\nLet's continue pushing the boundaries of AI technology together! \ud83d\ude80\ud83d\udca1\ud83d\udd0d", "x": "\ud83d\ude80 Exciting news in the world of Large Language Models (LLMs) and Multi-hop Question Answering (MHQA)! A new evaluation benchmark has been introduced to address current limitations and enhance the assessment of LLM reasoning abilities. Check out the details in the research paper here: http://arxiv.org/abs/2402.11924v2 #AI #NLP #LLMs #MHQA #TechResearch \ud83e\udd16\ud83d\udcda", "source_id": "933f7f41214fe8b2d81a6320cc902b05470cca310c2a9e5ef6153f9354f526cd", "page_number": 1}, "score": null, "embedding": [-0.4510345757007599, 0.07877463102340698, 0.029014965519309044, -0.03341623768210411, -0.012532229535281658, 0.037905771285295486, -0.13201020658016205, -0.06438560783863068, 0.18895627558231354, -0.22520996630191803, -0.12562929093837738, -0.19963276386260986, 0.31303688883781433, 0.15398092567920685, 0.27775025367736816, 0.3282786011695862, -0.1305798590183258, -0.005581144709140062, -0.02976047620177269, -0.08732379227876663, 0.03168277442455292, -0.26847800612449646, 0.04768375679850578, 0.01618763618171215, 0.25962236523628235, -0.07309085875749588, -0.3025330901145935, -0.12311918288469315, -0.3693424165248871, -1.550335168838501, 0.09657042473554611, -0.05332714691758156, 0.2494293600320816, -0.13618065416812897, -0.25774946808815, -0.04318748041987419, -0.07914729416370392, 0.1177758201956749, -0.042143240571022034, 0.06844047456979752, 0.4365294873714447, 0.17051738500595093, 0.011936286464333534, -0.17960777878761292, -0.06271728128194809, -0.3329763412475586, -0.06097450107336044, -0.05920539051294327, -0.547121524810791, -0.09941154718399048, -0.22463256120681763, -0.4028488099575043, 0.056495580822229385, 0.08220871537923813, 0.1825961172580719, 0.25046220421791077, 0.06784110516309738, 0.23432505130767822, -0.0018548918887972832, 0.2502700984477997, 0.31042009592056274, 0.2554238736629486, -0.9857354760169983, 0.3001272976398468, 0.22872617840766907, 0.03038719855248928, -0.0050478121265769005, -0.1341109424829483, 0.06717823445796967, 0.10306953638792038, -0.1254562884569168, 0.03632032871246338, 0.4087393283843994, 0.3672727644443512, 0.1820049285888672, 0.15122012794017792, 0.09298987686634064, -0.02463487721979618, -0.1260567456483841, 0.16392002999782562, -0.096173495054245, -0.06532912701368332, 0.012378395535051823, -0.2700120806694031, -0.23078548908233643, 0.07910721749067307, -0.14775219559669495, 0.09464185684919357, -0.005725144874304533, -0.2200767695903778, 0.2118358165025711, -0.18217866122722626, -0.20162832736968994, 0.21373550593852997, 0.049208372831344604, 0.045578278601169586, 0.22389650344848633, -0.05356587469577789, -0.08266458660364151, 0.6969869136810303, -0.2692742943763733, 0.15030522644519806, -0.26627692580223083, -0.2303231954574585, 0.21018657088279724, 0.11108884960412979, -0.06095217168331146, -0.30719152092933655, -0.1777680665254593, -0.033316731452941895, 0.18256278336048126, -0.06043236330151558, 0.3768160343170166, -0.11123280227184296, 0.05958590656518936, 0.18197977542877197, 0.40005993843078613, 0.07765629887580872, -0.24887001514434814, -0.06646525114774704, -0.3877575397491455, 0.12423519790172577, 0.2773834466934204, -0.241068497300148, -0.04608858376741409, -0.14841768145561218, 0.21032336354255676, 0.44537824392318726, 0.003500910010188818, -0.09508626908063889, 0.20536494255065918, -0.11968781054019928, -0.12923605740070343, -0.058595627546310425, 0.31592029333114624, -0.21582281589508057, 0.06236877292394638, 0.06432583928108215, 0.22981935739517212, 0.061074819415807724, -0.2319108247756958, -0.09732478111982346, 0.17360931634902954, -0.3153909146785736, -0.4551823139190674, 0.6288368105888367, -0.14175185561180115, -0.06793256103992462, -0.31240108609199524, -0.36322519183158875, 0.0826624259352684, 0.17149344086647034, -0.1197141483426094, -0.24989458918571472, 0.349045991897583, 0.263594388961792, -0.03595445677638054, 0.267411470413208, -0.1227884292602539, 0.03348689153790474, -0.15997454524040222, -0.20574402809143066, -0.2442445456981659, 0.7096574306488037, -0.3515494167804718, -0.23704169690608978, -0.09804535657167435, -0.1888589709997177, 0.25430217385292053, -0.29144546389579773, 0.30608391761779785, 0.16821083426475525, -0.21305835247039795, -0.06057509407401085, -0.21946576237678528, -0.2527220547199249, -0.5531917810440063, 0.11329887807369232, -0.02605096995830536, 0.05742093548178673, 0.048916179686784744, -0.19314081966876984, -0.1730240136384964, 0.21837346255779266, 0.1696726381778717, -0.269669771194458, -0.12251127511262894, -0.2038928121328354, 0.03186853975057602, 0.03373776748776436, -0.396512508392334, -0.05095218867063522, -0.2278793603181839, 0.20855332911014557, -0.10925909876823425, -0.34018567204475403, 0.01973893493413925, 0.09541528671979904, 0.24629278481006622, -0.04757273197174072, -0.08741941303014755, 0.25941798090934753, -0.10803202539682388, 0.07708952575922012, -0.1405404657125473, -0.06599817425012589, -0.12555818259716034, -0.07872378081083298, 0.255382776260376, 0.523687481880188, -0.2144872546195984, -0.1159459725022316, -0.11645997315645218, -0.07823862880468369, 0.15925230085849762, -0.15705335140228271, 0.26310303807258606, 0.09297547489404678, -0.2141684889793396, 0.18594183027744293, 0.1777406930923462, 0.13223184645175934, -0.0009235879988409579, -1.181640386581421, -0.12003304064273834, 0.3138250410556793, 0.05721375346183777, 0.24209877848625183, -0.22975504398345947, 0.11028177291154861, 0.06389907002449036, 0.1542321890592575, 0.2733025848865509, 0.3983202576637268, 0.15213501453399658, -0.1596936732530594, -0.018206510692834854, -0.06736253201961517, 0.14670978486537933, -0.029812566936016083, 0.17374835908412933, -0.29953518509864807, 0.3392433524131775, -0.22217844426631927, 0.1956472396850586, 0.025988617911934853, -0.5727569460868835, 0.13295260071754456, -0.20416536927223206, 0.8044275045394897, -0.1478603184223175, 0.05231848731637001, 0.1986982226371765, 0.357618123292923, 0.1052616760134697, -0.15317599475383759, -0.32261228561401367, 0.42401111125946045, 0.03154253587126732, -0.08212503790855408, 0.028574667870998383, 0.06355631351470947, -0.1601032167673111, -0.20028308033943176, -0.03798345476388931, 0.12472721934318542, -0.7512155175209045, -0.26146677136421204, 0.1372443586587906, 0.09286541491746902, 0.07745213806629181, -0.4179190993309021, 0.18366990983486176, 0.11629719287157059, -0.09957006573677063, 0.1603415608406067, 0.21868571639060974, -0.09906802326440811, -0.041178006678819656, -0.5686340928077698, 0.1264050155878067, -0.34942445158958435, 0.11745563894510269, 0.09993267059326172, 0.03555682301521301, -0.12292620539665222, 0.033354856073856354, 0.4113345146179199, -0.4099244475364685, -0.07413040846586227, 0.12604814767837524, 0.09407003968954086, -0.09420709311962128, -0.26758846640586853, 0.5535469055175781, 0.1364801824092865, 0.055754031985998154, 0.12176286429166794, 0.12189432978630066, 0.16600115597248077, -0.33045005798339844, 0.0061124395579099655, 0.037432026118040085, 0.32264086604118347, 0.22983627021312714, -0.13173657655715942, 0.23070074617862701, 0.043774910271167755, 0.13544870913028717, 0.18667615950107574, 0.25714218616485596, 0.048475369811058044, 0.26952072978019714, -0.0055452133528888226, -0.06807619333267212, -0.13378076255321503, -0.04188801348209381, -0.1068854033946991, 0.005092406179755926, -0.9548701047897339, -0.02510105073451996, -0.07644589990377426, 0.10652367025613785, -0.16692890226840973, -0.015993431210517883, 0.1828589290380478, -0.24511878192424774, 0.07344280928373337, 0.15852884948253632, -0.03415713086724281, 0.6248458623886108, 0.2337626814842224, -0.0812009945511818, -0.1283050924539566, -0.3450396955013275, 0.3140917718410492, -0.16255241632461548, -0.049214184284210205, -0.026113664731383324, 0.046104758977890015, 0.15965604782104492, 1.0018069744110107, 0.07625749707221985, 0.11530959606170654, 0.4596411883831024, -0.13398981094360352, 0.10016889125108719, -0.12268724292516708, 0.1273472160100937, 0.08925274014472961, -0.053398989140987396, 0.7062051296234131, -0.06510979682207108, 0.018923090770840645, 0.6363421678543091, -0.24956069886684418, 0.036000579595565796, 0.29900482296943665, 0.16738863289356232, 0.298777312040329, 0.01495955977588892, -0.16706573963165283, -0.11750191450119019, 0.6570332050323486, 0.11181136965751648, 0.014185134321451187, -0.4332244396209717, -0.06055634841322899, -0.08823544532060623, -0.1296580284833908, 0.11211377382278442, -0.07999522238969803, -0.08783084899187088, 0.18240804970264435, 0.20105552673339844, 0.15639232099056244, -0.08454003185033798, -0.22826695442199707, -0.09940384328365326, 0.21825797855854034, 0.0721696987748146, 0.11218144744634628, -0.010064118541777134, -0.28534069657325745], "sparse_embedding": null}, {"id": "3baee9d2fe77db6122aeb2d0f72b0b51f6393cde2676ea079c23ff2f1ba25539", "content": "The unprecedented performance of large language models (LLMs) requires\ncomprehensive and accurate evaluation. We argue that for LLMs evaluation,\nbenchmarks need to be comprehensive and systematic. To this end, we propose the\nZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional\nability coverage: We comprehensively evaluate LLMs across 7 ability dimensions\ncovering 51 tasks. Especially, we also propose a new benchmark that focuses on\nknowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration:\nWe use 3 different yet complementary evaluation methods to comprehensively\nevaluate LLMs, which can ensure the authority and accuracy of the evaluation\nresults. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering\nbenchmark that fully assesses LLMs in Chinese, while also providing equally\nrobust evaluation abilities in English. (4) Avoiding potential data leakage: To\navoid data leakage, we construct evaluation data specifically for 37 tasks. We\nevaluate 10 current mainstream LLMs and conduct an in-depth discussion and\nanalysis of their results. The ZhuJiu benchmark and open-participation\nleaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we\nalso provide a demo video at https://youtu.be/qypkJ89L1Ic.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2308.14353v1", "title": "ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models", "content": "http://arxiv.org/pdf/2308.14353v1", "datetime": "2023-08-28 06:56:44", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting news in the world of AI evaluation! Check out the ZhuJiu benchmark, a groundbreaking initiative for comprehensive and accurate evaluation of large language models (LLMs). This benchmark covers 51 tasks across 7 ability dimensions, with a focus on knowledge ability. Using 3 evaluation methods, ZhuJiu ensures thorough and precise assessment results. \n\nWhat makes ZhuJiu stand out? It's the first benchmark to fully evaluate LLMs in Chinese, alongside robust evaluation in English, while also addressing potential data leakage concerns. Want to dive deeper into the results and analysis? Explore the ZhuJiu benchmark and leaderboard at http://www.zhujiu-benchmark.com/ and watch the demo video at https://youtu.be/qypkJ89L1Ic. \n\nFor more details, check out the research paper at http://arxiv.org/abs/2308.14353v1. \ud83d\udcca\ud83d\udca1 #AI #LLMs #ZhuJiuBenchmark #TechInnovation", "x": "\ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! Check out the ZhuJiu benchmark - a comprehensive evaluation method covering 51 tasks across 7 dimensions, including a focus on knowledge ability. Dive into the details and results of evaluating 10 mainstream LLMs here: http://arxiv.org/abs/2308.14353v1 #AI #NLP #LLMs #ZhuJiuBenchmark \ud83d\udcca\ud83d\udd0d", "source_id": "9cf49df792a8dde03b46e5879736778ff631b714549baad12ba3f563a3b85293", "page_number": 1}, "score": null, "embedding": [-0.5254813432693481, -0.19407324492931366, 0.11575227975845337, -0.05539457127451897, 0.077775739133358, 0.11257738620042801, -0.09558919072151184, 0.15327084064483643, 0.21385852992534637, -0.20216695964336395, 0.1570969969034195, -0.3496587574481964, 0.275515079498291, 0.18597282469272614, 0.24753279983997345, 0.2210816591978073, 0.0038184768054634333, 0.044152986258268356, -0.05215543881058693, -0.04557619243860245, 0.21212948858737946, -0.25520724058151245, 0.07289327681064606, -0.05188968405127525, 0.25842928886413574, -0.057877734303474426, -0.14305010437965393, -0.33661511540412903, -0.013239970430731773, -1.4250504970550537, 0.027441365644335747, -0.1724560707807541, 0.12028884887695312, 0.0645657628774643, -0.16427099704742432, 0.006489710882306099, -0.16833113133907318, 0.03008113242685795, -0.022571073845028877, -0.020511414855718613, 0.2457699328660965, 0.32649433612823486, 0.07187646627426147, -0.06366419047117233, -0.0702386274933815, -0.38666924834251404, -0.175478994846344, -0.04677402600646019, -0.44555312395095825, 0.14040744304656982, 0.05780186131596565, -0.26242774724960327, 0.20542167127132416, 0.2352074831724167, 0.08005194365978241, 0.1531398594379425, 0.2813670039176941, 0.024252353236079216, -0.13285472989082336, 0.241615429520607, 0.38430339097976685, -0.03333669900894165, -0.9266743659973145, 0.2050633728504181, 0.11031228303909302, 0.14634251594543457, -0.10557545721530914, -0.17576278746128082, 0.1965014785528183, 0.12510670721530914, -0.14465855062007904, 0.050698913633823395, 0.3004155158996582, 0.33891561627388, 0.22837094962596893, 0.2658511698246002, 0.18377012014389038, -0.07513066381216049, -0.10105124115943909, 0.04793247580528259, -0.01272620353847742, -0.10292088985443115, -0.05654045566916466, -0.36818990111351013, -0.03553460165858269, 0.2308020442724228, -0.033417608588933945, 0.22078052163124084, -0.08200792968273163, -0.21238157153129578, 0.2021944522857666, -0.006508815102279186, -0.2744967043399811, 0.34293437004089355, -0.09586118906736374, -0.09836743026971817, 0.38911089301109314, 0.047945547848939896, -0.19909293949604034, 0.5807957053184509, -0.22072722017765045, -0.07208872586488724, -0.044582873582839966, -0.13983362913131714, 0.2448541522026062, -0.17308379709720612, -0.08344167470932007, -0.3424264192581177, -0.2220171093940735, -0.13333332538604736, 0.26945310831069946, -0.15844295918941498, -0.1305953413248062, 0.0014503116253763437, -0.012107747606933117, -0.02473222091794014, 0.38023561239242554, -0.016810303553938866, 0.06407994776964188, -0.15862363576889038, -0.2375851422548294, 0.11848092079162598, -0.060755811631679535, -0.4205142557621002, 0.09673277288675308, -0.1913089007139206, 0.1797059029340744, 0.28605416417121887, 0.2690418064594269, -0.07017561048269272, 0.19403915107250214, 0.0072777546010911465, -0.293766587972641, -0.11202648282051086, -0.038589462637901306, -0.10222546756267548, 0.008540129289031029, 0.14539919793605804, 0.24428662657737732, 0.10221090167760849, 0.015148396603763103, 0.05151502788066864, 0.26696959137916565, -0.2482796460390091, -0.30880483984947205, 0.9141479134559631, -0.21007487177848816, 0.14121364057064056, -0.07193344831466675, -0.12041912227869034, 0.14687711000442505, 0.114530548453331, 0.08316177129745483, -0.3154670298099518, -0.058781079947948456, -0.09202433377504349, 0.036358609795570374, 0.39927083253860474, -0.3014134466648102, 0.03665205463767052, -0.03146270662546158, -0.013045525178313255, -0.1478128582239151, 0.7424982786178589, -0.1719607561826706, -0.31648775935173035, -0.2378854751586914, -0.0063968622125685215, -0.03751212731003761, -0.3469069302082062, 0.025142444297671318, 0.31778234243392944, -0.46419021487236023, -0.11889385432004929, -0.1468457132577896, -0.17131881415843964, -0.6281183362007141, -0.08227667957544327, -0.16288188099861145, 0.05440357327461243, 0.11524583399295807, 0.04347451403737068, -0.1641276478767395, 0.005223223008215427, -0.0824040099978447, -0.2508578300476074, 0.028609134256839752, -0.11410592496395111, 0.10434387624263763, -0.11059915274381638, -0.5084174871444702, 0.26780271530151367, -0.03052401728928089, 0.24044129252433777, 0.04185623675584793, -0.28566449880599976, -0.14939287304878235, -0.15435513854026794, 0.12844984233379364, -0.23926788568496704, 0.2004241943359375, -0.13536463677883148, -0.13701573014259338, -0.12743385136127472, -0.0294828861951828, 0.02063133381307125, -0.15948618948459625, 0.11873982846736908, 0.22062218189239502, 0.4785326421260834, -0.3191228210926056, 0.18441981077194214, 0.2724103331565857, -0.002951430156826973, 0.13820509612560272, 0.19411073625087738, 0.36982056498527527, 0.2404116839170456, -0.07293540984392166, 0.25264766812324524, 0.3067716062068939, -0.10196112841367722, -0.23744530975818634, -1.2068164348602295, -0.40344417095184326, 0.27969586849212646, -0.02584998495876789, 0.383324533700943, -0.29328879714012146, 0.03786829486489296, 0.058026641607284546, 0.31925272941589355, 0.4926494359970093, 0.19306567311286926, 0.24641507863998413, -0.2678370475769043, 0.2308216094970703, -0.00608527846634388, 0.06021857261657715, -0.12341083586215973, 0.034355126321315765, -0.31095263361930847, 0.1431555449962616, -0.0632365494966507, 0.2599523961544037, 0.07402809709310532, -0.34446030855178833, 0.13284868001937866, -0.3927251398563385, 0.7287912368774414, -0.3610076904296875, 0.01003548875451088, -0.16668100655078888, 0.14086008071899414, 0.23798294365406036, 0.03191928565502167, -0.3581794798374176, 0.4952302575111389, 0.014613614417612553, -0.23171263933181763, -0.04715538024902344, 0.04397596791386604, 0.014573789201676846, 0.0021815369836986065, -0.13661013543605804, 0.1599501371383667, -0.6261675357818604, -0.2517816722393036, 0.2062111347913742, 0.18934687972068787, -0.15928976237773895, -0.5658222436904907, 0.09313717484474182, 0.2255495935678482, -0.14688707888126373, -0.030088666826486588, 0.11026071012020111, -0.2543945908546448, -0.07798859477043152, -0.6970648169517517, 0.358463853597641, -0.31546327471733093, 0.05253699794411659, -0.045773059129714966, -0.005676297936588526, -0.020637674257159233, -0.02257450297474861, 0.33003878593444824, -0.07909788936376572, -0.10003235936164856, 0.1290634572505951, 0.10721825808286667, -0.35563531517982483, -0.3057629466056824, 0.2693944573402405, -0.12376492470502853, -0.0036960775032639503, 0.21356447041034698, 0.007133154198527336, 0.20438824594020844, -0.1918746680021286, -0.04711417481303215, -0.11324036121368408, 0.5599570274353027, 0.14244945347309113, -0.07050463557243347, 0.3059748709201813, 0.17425468564033508, 0.0935179814696312, 0.2333274781703949, 0.1923528015613556, 0.06920208781957626, 0.055291805416345596, 0.11696205288171768, -0.09873563051223755, -0.2520166337490082, 0.040763016790151596, -0.02278028428554535, 0.20130157470703125, -1.0641894340515137, 0.0033017585519701242, -0.04745995253324509, 0.25877153873443604, -0.2391931116580963, -0.12537960708141327, 0.1611269861459732, -0.2208033800125122, 0.05661538243293762, 0.20663365721702576, 0.026513107120990753, 0.40141358971595764, 0.021456461399793625, -0.4107171297073364, 0.08218783885240555, -0.17528153955936432, 0.46551474928855896, -0.08433343470096588, 0.2696175277233124, -0.0932542160153389, 0.026503723114728928, 0.2258412390947342, 1.1407642364501953, -0.1568308174610138, -0.03959669917821884, 0.35554149746894836, -0.09825540333986282, -0.1486649215221405, -0.08047491312026978, -0.09489214420318604, -0.11295691132545471, -0.17260947823524475, 0.5792607665061951, 0.11463291198015213, -0.10155078768730164, 0.5796330571174622, -0.1306113749742508, 0.08393918722867966, -0.00045691028935834765, 0.1364125907421112, 0.5267270803451538, -0.12221118807792664, -0.052718691527843475, -0.04139263182878494, 0.7293500900268555, 0.1822565793991089, 0.10096516460180283, -0.3498977720737457, -0.009215516969561577, -0.08698882907629013, -0.17132316529750824, 0.048051249235868454, -0.13636089861392975, 0.16205337643623352, 0.2450421303510666, 0.3448292016983032, 0.09421339631080627, -0.062952421605587, -0.1496962606906891, -0.18369214236736298, 0.3607088625431061, -0.05211568996310234, -0.02299191989004612, 0.12726475298404694, -0.06294963508844376], "sparse_embedding": null}, {"id": "ae8e78e8f62414c5bc1398e6e7fa6d7ed5e92b5657940b40bde091648cf6a04e", "content": "The rapid development of Chinese large language models (LLMs) poses big\nchallenges for efficient LLM evaluation. While current initiatives have\nintroduced new benchmarks or evaluation platforms for assessing Chinese LLMs,\nmany of these focus primarily on capabilities, usually overlooking potential\nalignment and safety issues. To address this gap, we introduce OpenEval, an\nevaluation testbed that benchmarks Chinese LLMs across capability, alignment\nand safety. For capability assessment, we include 12 benchmark datasets to\nevaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge,\ncommonsense reasoning and mathematical reasoning. For alignment assessment,\nOpenEval contains 7 datasets that examines the bias, offensiveness and\nillegalness in the outputs yielded by Chinese LLMs. To evaluate safety,\nespecially anticipated risks (e.g., power-seeking, self-awareness) of advanced\nLLMs, we include 6 datasets. In addition to these benchmarks, we have\nimplemented a phased public evaluation and benchmark update strategy to ensure\nthat OpenEval is in line with the development of Chinese LLMs or even able to\nprovide cutting-edge benchmark datasets to guide the development of Chinese\nLLMs. In our first public evaluation, we have tested a range of Chinese LLMs,\nspanning from 7B to 72B parameters, including both open-source and proprietary\nmodels. Evaluation results indicate that while Chinese LLMs have shown\nimpressive performance in certain tasks, more attention should be directed\ntowards broader aspects such as commonsense reasoning, alignment, and safety.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2403.12316v1", "title": "OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety", "content": "http://arxiv.org/pdf/2403.12316v1", "datetime": "2024-03-18 23:21:37", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting News in the World of Chinese Large Language Models (LLMs) \ud83d\ude80\n\nThe fast-paced advancement of Chinese Large Language Models (LLMs) has brought about new challenges in their evaluation process. While existing benchmarks primarily focus on capabilities, the critical aspects of alignment and safety often get overlooked. To bridge this gap, we are thrilled to introduce OpenEval, a comprehensive evaluation testbed designed to assess Chinese LLMs across capability, alignment, and safety.\n\n\ud83d\udd0d Capability Assessment:\n- 12 benchmark datasets covering NLP tasks, disciplinary knowledge, commonsense reasoning, and mathematical reasoning.\n\n\ud83d\udccf Alignment Assessment:\n- 7 datasets examining bias, offensiveness, and illegalness in the outputs of Chinese LLMs.\n\n\ud83d\udee1\ufe0f Safety Evaluation:\n- 6 datasets focusing on anticipated risks like power-seeking and self-awareness in advanced LLMs.\n\nOur phased public evaluation and benchmark update strategy ensures that OpenEval stays aligned with the evolving landscape of Chinese LLMs, offering cutting-edge datasets for their development.\n\nIn our inaugural public evaluation, we tested various Chinese LLMs ranging from 7B to 72B parameters, including open-source and proprietary models. While these LLMs displayed remarkable performance in specific tasks, there is a clear need to shift focus towards broader aspects such as commonsense reasoning, alignment, and safety.\n\nFor more details, check out the research paper at: http://arxiv.org/abs/2403.12316v1\n\n#LLMs #ChineseLLMs #EvaluationTestbed #AIResearch #TechInnovation #OpenEval #NLP #SafetyEvaluation #AlignmentAssessment", "x": "\ud83d\ude80 Exciting news in the world of Chinese large language models (LLMs)! Introducing OpenEval, a comprehensive evaluation testbed for Chinese LLMs covering capability, alignment, and safety. Learn more about the benchmarks and evaluation results in the latest research paper: http://arxiv.org/abs/2403.12316v1 #AI #NLP #LLMs #OpenEval \ud83e\udde0\ud83c\udde8\ud83c\uddf3\ud83d\udd0d", "source_id": "fe308a71fd0e22212f43fa8a2547af91cd8efddee79f81ebea6aee4f89b281e7", "page_number": 1}, "score": null, "embedding": [-0.3953326642513275, -0.207266703248024, 0.2967738211154938, -0.17375250160694122, 0.14038950204849243, 0.20170849561691284, -0.14423348009586334, 0.0037126634269952774, 0.25598403811454773, -0.3642122447490692, 0.1870315670967102, -0.25437572598457336, 0.04550618305802345, 0.19673733413219452, 0.05670986697077751, 0.19787009060382843, -0.16895915567874908, 0.04685187339782715, -0.13003182411193848, 0.04749773442745209, 0.11718185991048813, -0.4108904302120209, 0.07412400841712952, -0.014919264242053032, 0.07054464519023895, 0.24313893914222717, -0.21348409354686737, -0.09171570837497711, -0.12607024610042572, -1.419578194618225, -0.06716075539588928, 0.014418534934520721, 0.28862786293029785, -0.1842544674873352, -0.16715486347675323, -0.1027139276266098, -0.15434731543064117, 0.04659727215766907, 0.08840807527303696, -0.21217527985572815, 0.21043221652507782, 0.1958836019039154, 0.055727917701005936, -0.07356666773557663, -0.020132068544626236, -0.37635475397109985, -0.17055237293243408, 0.056089580059051514, -0.5143883228302002, -0.09981795400381088, 0.15349921584129333, -0.44616639614105225, 0.22303073108196259, 0.21013569831848145, 0.06260688602924347, 0.14306414127349854, 0.15483655035495758, 0.03193064406514168, 0.027010420337319374, 0.13446615636348724, 0.40220192074775696, 0.08784319460391998, -0.8535656929016113, 0.259181410074234, 0.08326349407434464, 0.144196555018425, 0.0238815788179636, -0.019962793216109276, 0.15535789728164673, 0.10315844416618347, -0.1168864369392395, -0.006488049402832985, 0.34231051802635193, 0.25282037258148193, 0.03941894695162773, 0.4455338716506958, 0.29429399967193604, -0.10910436511039734, -0.15002083778381348, -0.09811316430568695, 0.2524532079696655, 0.027075739577412605, -0.1769004762172699, -0.11737097054719925, 0.06322131305932999, 0.07447047531604767, -0.09236457943916321, 0.11553611606359482, -0.04123079404234886, -0.10541113466024399, 0.06043759733438492, -0.20555025339126587, -0.18237009644508362, 0.31006279587745667, -0.23324976861476898, -0.08443429321050644, 0.2317269891500473, 0.05991077050566673, -0.4315069615840912, 0.657590389251709, -0.2644140124320984, 0.1963871717453003, -0.2905116379261017, -0.14048077166080475, 0.4605892598628998, -0.043494775891304016, -0.10698731988668442, -0.21277856826782227, -0.01733589731156826, -0.15473568439483643, 0.05047552287578583, -0.096109539270401, -0.01412249356508255, -0.07698068767786026, 0.04014169052243233, -0.04732954874634743, 0.2844920754432678, 0.05289153754711151, -0.11379877477884293, -0.18221689760684967, -0.1634468138217926, 0.20144402980804443, 0.07816854119300842, -0.3126303553581238, -0.005468579940497875, -0.09838633239269257, 0.20294824242591858, 0.3373015224933624, 0.22166314721107483, -0.07862371951341629, 0.16160058975219727, -0.14168402552604675, -0.18357959389686584, -0.2761484980583191, 0.0030200956389307976, -0.0524681881070137, -0.020268574357032776, 0.031327344477176666, 0.3234540522098541, 0.30517834424972534, -0.02316630445420742, -0.013595068827271461, 0.2930179238319397, -0.14079631865024567, -0.3478792905807495, 0.70212721824646, -0.26090630888938904, 0.11433915793895721, -0.2226596623659134, -0.1626957505941391, 0.29986700415611267, 0.18281371891498566, 0.06555600464344025, -0.178357794880867, 0.3042895793914795, 0.01518276147544384, -0.056197457015514374, 0.22735929489135742, -0.1730785220861435, 0.006525182165205479, 0.02095724083483219, 0.045631568878889084, -0.43405431509017944, 0.8220444321632385, -0.16172614693641663, -0.27459025382995605, -0.2104482203722, 0.004148297011852264, 0.043821919709444046, -0.17583094537258148, 0.26816049218177795, 0.14659073948860168, -0.16529326140880585, 0.032122205942869186, -0.1625693142414093, -0.08884671330451965, -0.4109714925289154, -0.3104737102985382, -0.10978074371814728, 0.04623683542013168, 0.09343480318784714, -0.17101404070854187, -0.20792214572429657, 0.1741306334733963, -0.03329857066273689, -0.09686009585857391, -0.12956984341144562, -0.18448121845722198, 0.06139887869358063, 0.007654194720089436, -0.5375470519065857, 0.40467923879623413, -0.05377870798110962, 0.14003998041152954, -0.12748783826828003, -0.30813068151474, -0.03400290012359619, 0.019445322453975677, 0.28631144762039185, -0.23589268326759338, 0.06727378070354462, 0.03587361052632332, 0.0398547500371933, -0.15121842920780182, -0.03245304152369499, -0.154309943318367, -0.004167501349002123, 0.06982465088367462, 0.07284209877252579, 0.4232691824436188, -0.20473049581050873, 0.09507233649492264, 0.10149330645799637, 0.02901296131312847, -0.04957322031259537, 0.06016677990555763, 0.23301294445991516, 0.22533223032951355, -0.4433281719684601, 0.3866822421550751, 0.26517876982688904, 0.16085807979106903, -0.10340498387813568, -1.1436913013458252, -0.16638575494289398, 0.1500495821237564, -0.03616378456354141, 0.13972561061382294, -0.11744365841150284, 0.014116121456027031, -0.03556865453720093, 0.21594470739364624, 0.3984990417957306, 0.2948869466781616, 0.18185095489025116, -0.31959953904151917, 0.19552643597126007, 0.04790973290801048, -0.008566789329051971, -0.29346129298210144, 0.0068429699167609215, -0.2289935052394867, 0.3507012724876404, 0.021848564967513084, 0.2879394292831421, -0.18133598566055298, -0.36986756324768066, 0.021522412076592445, -0.2746630907058716, 0.6525723934173584, -0.4260629713535309, 0.038895364850759506, -0.20368404686450958, 0.23493292927742004, 0.044738415628671646, 0.1052379459142685, -0.3516317307949066, 0.5142809748649597, 0.25162914395332336, -0.08797231316566467, 0.17647741734981537, -0.04213386029005051, 0.12795868515968323, 0.01779472641646862, -0.3057478666305542, 0.3420381247997284, -0.7137708067893982, -0.12532663345336914, 0.15378327667713165, -0.0436817929148674, 0.026195142418146133, -0.278348833322525, 0.20706355571746826, 0.020553452894091606, -0.2233467996120453, 0.0759933739900589, 0.23607133328914642, -0.12996192276477814, -0.25581860542297363, -0.7086097002029419, 0.16475528478622437, -0.03820795565843582, -0.21847151219844818, 0.008775554597377777, 0.0054004136472940445, -0.08500427007675171, 0.0073676942847669125, 0.2929851710796356, -0.262773722410202, -0.09239904582500458, -0.03655592352151871, 0.08358417451381683, -0.34925028681755066, -0.45168155431747437, 0.4915757179260254, -0.0162844005972147, 0.11673568189144135, 0.09360424429178238, 0.10931939631700516, -0.06293529272079468, -0.18039369583129883, -0.1028117686510086, -0.031026992946863174, 0.4730548560619354, 0.07090918719768524, -0.3057389259338379, 0.08525911718606949, 0.0769820511341095, -0.12506446242332458, 0.228797048330307, 0.24277587234973907, 0.1993374526500702, 0.07524274289608002, -0.03449206426739693, -0.06643695384263992, -0.22855953872203827, -0.16014795005321503, 0.043902184814214706, 0.16261263191699982, -0.9260091185569763, 0.08887092769145966, -0.15666784346103668, 0.10696238279342651, -0.0551149845123291, -0.20939674973487854, 0.22172902524471283, -0.18132030963897705, 0.117957703769207, 0.02510068006813526, -0.09564943611621857, 0.4423830509185791, 0.006045484449714422, -0.29911378026008606, 0.15602755546569824, -0.24479861557483673, 0.3375554382801056, -0.2628163993358612, 0.2668100893497467, -0.228033646941185, 0.08481768518686295, 0.296662300825119, 1.2046695947647095, -0.0065689715556800365, -0.01833556406199932, 0.21824420988559723, -0.04645600914955139, -0.18339264392852783, -0.04346850514411926, -0.0852450504899025, 0.06135745346546173, -0.06509929150342941, 0.5443581938743591, 0.0315057747066021, -0.003099599154666066, 0.7397636771202087, -0.11209525913000107, -0.06717674434185028, 0.29579100012779236, 0.21585814654827118, 0.45932498574256897, -0.197554811835289, -0.023414667695760727, 0.01661440171301365, 0.6913977861404419, 0.1171940267086029, -0.0535280816257, -0.28293243050575256, 0.15109150111675262, -0.017146574333310127, -0.048743266612291336, -0.042998507618904114, -0.06972283869981766, 0.25926563143730164, 0.1877295821905136, 0.06094227358698845, 0.3618936240673065, 0.03100050613284111, -0.07507805526256561, -0.19540388882160187, 0.23427820205688477, -0.10420417040586472, 0.0440690703690052, 0.054717279970645905, -0.07128793746232986], "sparse_embedding": null}, {"id": "4f2938c0796c1cb7c6deb232cf489bd37c86fec5b6949321d1f09450eab2ba6a", "content": "The versatility of large language models (LLMs) led to the creation of\ndiverse benchmarks that thoroughly test a variety of language models'\nabilities. These benchmarks consist of tens of thousands of examples making\nevaluation of LLMs very expensive. In this paper, we investigate strategies to\nreduce the number of evaluations needed to assess the performance of an LLM on\nseveral key benchmarks. For example, we show that to accurately estimate the\nperformance of an LLM on MMLU, a popular multiple-choice QA benchmark\nconsisting of 14K examples, it is sufficient to evaluate this LLM on 100\ncurated examples. We release evaluation tools and tiny versions of popular\nbenchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical\nanalysis demonstrates that these tools and tiny benchmarks are sufficient to\nreliably and efficiently reproduce the original evaluation results.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2402.14992v2", "title": "tinyBenchmarks: evaluating LLMs with fewer examples", "content": "http://arxiv.org/pdf/2402.14992v2", "datetime": "2024-05-26 22:27:23", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting news in the world of AI and NLP! A recent study explores strategies to streamline the evaluation of large language models (LLMs) on various benchmarks. By reducing the number of evaluations required, assessing LLM performance becomes more efficient and cost-effective.\n\nOne striking finding from the research is that evaluating an LLM on just 100 carefully selected examples can accurately estimate its performance on a benchmark with tens of thousands of examples. The study also introduces evaluation tools and compact versions of popular benchmarks like MMLU and HELM.\n\nFor further insights into this groundbreaking research, check out the full paper at: http://arxiv.org/abs/2402.14992v2\n\n#AI #NLP #LLMs #Research #Tech #Innovation", "x": "\ud83d\ude80 Exciting research on optimizing evaluation of Large Language Models (LLMs)! Find out how to assess LLM performance on key benchmarks more efficiently. Check out the paper here: http://arxiv.org/abs/2402.14992v2 #AI #NLP #LLMs #TechResearch", "source_id": "5640ce8cf27a219762186c7b42de8315edeff835eaf62c07eb8a69eb1256f9a2", "page_number": 1}, "score": null, "embedding": [-0.5979765057563782, -0.07626982033252716, 0.12684860825538635, -0.017023524269461632, -0.11009546369314194, 0.04269687831401825, -0.33191996812820435, 0.12486345320940018, 0.23664410412311554, -0.3747338354587555, 0.015948178246617317, -0.12412280589342117, 0.05843336135149002, 0.06444358080625534, 0.3063032925128937, 0.20811320841312408, -0.032914724200963974, 0.01837926171720028, -0.15017764270305634, -0.04930510371923447, 0.28434306383132935, -0.023918963968753815, 0.0676618292927742, -0.11489815264940262, 0.14134562015533447, -0.09544278681278229, -0.0871036946773529, -0.1071431115269661, -0.15276947617530823, -1.48312246799469, 0.051527805626392365, -0.2655266225337982, 0.40585869550704956, 0.04151512682437897, -0.21823324263095856, 0.010867569595575333, -0.24516426026821136, 0.06794841587543488, -0.04640696942806244, 0.061173953115940094, 0.19206774234771729, 0.2897893190383911, 0.002842481015250087, -0.12512865662574768, -0.049628179520368576, -0.3907391130924225, -0.20131608843803406, -0.09547553211450577, -0.5115360021591187, 0.12268278002738953, -0.04125849902629852, -0.4002077281475067, 0.15812364220619202, 0.1396716833114624, 0.143110990524292, 0.12974251806735992, 0.05848368629813194, 0.20020164549350739, -0.13514813780784607, 0.0623813159763813, 0.36584872007369995, 0.2194662094116211, -0.9056204557418823, 0.27727407217025757, 0.09955716878175735, 0.11198020726442337, 0.02422342635691166, 0.06345560401678085, -0.06425540149211884, 0.2633591890335083, -0.18130993843078613, 0.14686790108680725, 0.449249804019928, 0.3436796963214874, 0.27977970242500305, 0.19065314531326294, 0.08306701481342316, -0.12729251384735107, 0.022226503118872643, 0.0535895861685276, -0.03023446910083294, -0.2013658583164215, -0.1359289288520813, -0.4229026734828949, -0.19070488214492798, 0.15230609476566315, -0.010655942372977734, 0.13415881991386414, 0.03643143177032471, -0.2759915590286255, 0.07952962070703506, 0.1179061233997345, -0.20860248804092407, 0.3439445197582245, -0.2047586739063263, 0.10403052717447281, 0.2242743968963623, 0.09721828997135162, -0.1667427122592926, 0.6047024726867676, -0.036454472690820694, 0.11880339682102203, -0.01185437198728323, -0.23027880489826202, 0.29979437589645386, -0.0008859009249135852, -0.09684338420629501, -0.25330695509910583, -0.014751731418073177, -0.1036238968372345, 0.17987041175365448, 0.11071379482746124, 0.06963486969470978, -0.17901816964149475, -0.13251307606697083, -0.15865781903266907, 0.4047785699367523, -0.07078614085912704, -0.006638173945248127, 0.02694680728018284, -0.4469127953052521, 0.009255340322852135, 3.30685252265539e-05, -0.4337705969810486, 0.11025334149599075, -0.07810506224632263, 0.2586824595928192, 0.5169491767883301, 0.22935518622398376, 0.024930760264396667, 0.10224908590316772, -0.16474173963069916, -0.2769801616668701, -0.1467348039150238, 0.09339925646781921, -0.18748128414154053, 0.08444865792989731, 0.043216340243816376, 0.41390538215637207, 0.19554440677165985, -0.05838272348046303, -0.08349098265171051, 0.11657949537038803, -0.20430126786231995, -0.38495948910713196, 0.80913907289505, -0.028700005263090134, 0.02076093852519989, -0.23080365359783173, -0.05854950472712517, 0.19535477459430695, 0.3083571493625641, -0.09909672290086746, -0.22553713619709015, 0.28318333625793457, 0.14033010601997375, 0.12558595836162567, 0.3623267710208893, -0.22123989462852478, -0.1228676438331604, -0.17580783367156982, -0.07281716167926788, -0.21758437156677246, 0.7011088132858276, -0.35926252603530884, -0.24780969321727753, -0.330037385225296, -0.04306730255484581, 0.08424259722232819, -0.2600976824760437, 0.158331960439682, 0.290410578250885, -0.4137360751628876, 0.04727960750460625, -0.1195744201540947, -0.22806809842586517, -0.6998981237411499, -0.22063452005386353, -0.03781886398792267, 0.025859978049993515, 0.11631511896848679, 0.08885709941387177, -0.23976664245128632, 0.15675725042819977, 0.024449771270155907, -0.06109315901994705, 0.09024233371019363, -0.1659434735774994, 0.023820674046874046, 0.10109170526266098, -0.4249226450920105, 0.1828795075416565, -0.09906648844480515, 0.18290166556835175, -0.12991444766521454, -0.27922865748405457, -0.11406752467155457, -0.19631743431091309, 0.36154142022132874, -0.19392995536327362, -0.054653529077768326, 0.1406981647014618, -0.07194258272647858, 0.03250700235366821, 0.03792957589030266, 0.0024778353981673717, -0.1498250663280487, 0.014765207655727863, 0.3913593292236328, 0.5692574977874756, -0.3201815187931061, 0.014188638888299465, 0.14682702720165253, -0.055329110473394394, -0.0647815391421318, -0.16128556430339813, 0.42225009202957153, 0.28943437337875366, -0.10094370692968369, 0.24057044088840485, 0.19930142164230347, -0.07971590757369995, -0.3319004476070404, -1.2332656383514404, -0.16244012117385864, 0.255129873752594, 0.09941525012254715, 0.46732619404792786, -0.32725659012794495, -0.001421603956259787, 0.04655047506093979, 0.335071325302124, 0.33273375034332275, 0.01100093312561512, -0.05566404014825821, -0.18721266090869904, 0.26352182030677795, -0.07657390087842941, 0.073824942111969, -0.5252194404602051, 0.11821047216653824, -0.20813880860805511, 0.2591210901737213, 0.06363461911678314, 0.07285883277654648, 0.12303733080625534, -0.514683723449707, 0.15162061154842377, -0.29519981145858765, 0.6705631613731384, -0.3163009285926819, -0.017422284930944443, -0.08914582431316376, 0.23896068334579468, 0.24145491421222687, 0.06111765280365944, -0.07365590333938599, 0.24789302051067352, 0.20211100578308105, -0.03592783585190773, 0.09994527697563171, 0.12467679381370544, 0.1470884382724762, -0.13415665924549103, -0.10585751384496689, 0.2340674102306366, -0.5401084423065186, -0.3898090422153473, 0.28408122062683105, 0.14693106710910797, -0.12263274192810059, -0.4119431972503662, 0.14048254489898682, -0.016867535188794136, -0.05262725427746773, 0.2575032413005829, 0.03563990443944931, -0.31823626160621643, -0.23918764293193817, -0.5600870847702026, 0.1377786248922348, -0.1690753698348999, 0.03207336366176605, 0.09462929517030716, -0.1598190814256668, -0.0981811061501503, -0.16112172603607178, 0.47127944231033325, -0.22085681557655334, -0.23518463969230652, -0.11336515098810196, -0.01158105581998825, -0.2366628646850586, -0.20474368333816528, 0.2847752273082733, -0.01410982757806778, -0.07718879729509354, 0.24110808968544006, 0.12906192243099213, 0.09061956405639648, -0.1795312464237213, -0.10481176525354385, -0.08297714591026306, 0.34727710485458374, 0.25889983773231506, -0.13571631908416748, 0.12994913756847382, 0.04668737202882767, 0.09312695264816284, 0.4197109341621399, 0.158601775765419, -0.19127988815307617, 0.09286663681268692, -0.07503703236579895, -0.0542055182158947, -0.23896156251430511, 0.1792791783809662, -0.14142367243766785, 0.11102514714002609, -1.0117467641830444, 0.15081393718719482, 0.09470587223768234, 0.36093267798423767, -0.17154766619205475, -0.02758123353123665, -0.15433737635612488, -0.09860008209943771, 0.15445967018604279, 0.28190159797668457, 0.18127979338169098, 0.3918246924877167, 0.13813337683677673, -0.2603745460510254, 0.08693277090787888, -0.15798470377922058, 0.2870468199253082, -0.1676667183637619, 0.42910337448120117, -0.35487791895866394, -0.08157502114772797, 0.041936393827199936, 1.1001039743423462, -0.2049214094877243, -0.05879988893866539, 0.46143603324890137, -0.16021732985973358, 0.13265199959278107, 0.06559557467699051, 0.0456060916185379, 0.12252050638198853, -0.09140179306268692, 0.850178599357605, -0.07842111587524414, -0.1096017062664032, 0.6431864500045776, -0.23344217240810394, -0.023355627432465553, 0.27670153975486755, 0.2185346633195877, 0.6115668416023254, -0.07611944526433945, -0.22176992893218994, -0.16371247172355652, 0.7279178500175476, 0.01433833222836256, 0.16494864225387573, -0.48606380820274353, -0.15535904467105865, 0.012649517506361008, -0.15186753869056702, 0.034434884786605835, -0.24148114025592804, 0.05422505736351013, 0.28482285141944885, 0.23658323287963867, 0.016083013266324997, -0.013411632739007473, -0.15836654603481293, -0.11441250890493393, 0.28527751564979553, -0.011170939542353153, 0.06570210307836533, -0.007253335323184729, -0.09327174723148346], "sparse_embedding": null}, {"id": "28d24a1dcc1e849173edb68238fd4ce42ee6bdedf6789b8d43d038142d42c5f1", "content": "In recent years, AI has demonstrated remarkable capabilities in simulating\nhuman behaviors, particularly those implemented with large language models\n(LLMs). However, due to the lack of systematic evaluation of LLMs' simulated\nbehaviors, the believability of LLMs among humans remains ambiguous, i.e., it\nis unclear which behaviors of LLMs are convincingly human-like and which need\nfurther improvements. In this work, we design SimulateBench to evaluate the\nbelievability of LLMs when simulating human behaviors. In specific, we evaluate\nthe believability of LLMs based on two critical dimensions: 1) consistency: the\nextent to which LLMs can behave consistently with the given information of a\nhuman to simulate; and 2) robustness: the ability of LLMs' simulated behaviors\nto remain robust when faced with perturbations. SimulateBench includes 65\ncharacter profiles and a total of 8,400 questions to examine LLMs' simulated\nbehaviors. Based on SimulateBench, we evaluate the performances of 10 widely\nused LLMs when simulating characters. The experimental results reveal that\ncurrent LLMs struggle to align their behaviors with assigned characters and are\nvulnerable to perturbations in certain factors.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2312.17115v2", "title": "How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation", "content": "http://arxiv.org/pdf/2312.17115v2", "datetime": "2024-06-15 14:08:30", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting developments in the world of AI and Large Language Models (LLMs) are continuously pushing boundaries, but what about the believability of their simulated behaviors?\n\nA recent study introduces SimulateBench, a novel framework evaluating LLMs' believability in simulating human behaviors. This evaluation is based on two crucial dimensions: consistency and robustness. By assessing 10 popular LLMs using 65 character profiles and 8,400 questions, the results shed light on the current challenges faced by LLMs in aligning behaviors with assigned characters and their susceptibility to perturbations.\n\nCurious to dive deeper into the findings? Check out the full study at: http://arxiv.org/abs/2312.17115v2\n\n#AI #NLP #LLMs #ArtificialIntelligence #TechResearch #Innovation #SimulateBench #BelievabilityEvaluation #TechStudy", "x": "\ud83e\udd16\ud83d\udcca Exciting insights on evaluating believability of Language Models in simulating human behaviors! Check out how SimulateBench assesses LLMs based on consistency and robustness. Discover the challenges and results of 10 popular LLMs in character simulation here: http://arxiv.org/abs/2312.17115v2 #AI #NLP #LLMs #SimulateBench", "source_id": "ed6c1c363a1492d4575287697df2be6ce59a8c64c8402796d930efa03a86b725", "page_number": 1}, "score": null, "embedding": [-0.5062265396118164, -0.048588238656520844, 0.04727547988295555, -0.03825955092906952, -0.33402761816978455, 0.1457633525133133, -0.013021470047533512, -0.04724342003464699, 0.22746221721172333, -0.3583959639072418, -0.04181047901511192, -0.22677071392536163, 0.2530266344547272, 0.03776276484131813, 0.06619390845298767, -0.02796199917793274, -0.19806067645549774, 0.023276425898075104, -0.22548650205135345, 0.22197142243385315, 0.3751783072948456, -0.04547877982258797, 0.07228550314903259, 0.05393167585134506, -0.08823370188474655, 0.012696131132543087, -0.011255396530032158, -0.026551131159067154, -0.042576637119054794, -1.5702732801437378, 0.13434350490570068, 0.07716558128595352, 0.4379645586013794, -0.11783665418624878, -0.17177313566207886, 0.061641305685043335, -0.23501239717006683, 0.27880391478538513, -0.04775962978601456, 0.12870247662067413, -0.007192076183855534, -0.008138158358633518, 0.11025518923997879, -0.1639060229063034, 0.08362113684415817, -0.27719974517822266, -0.11892713606357574, -0.1956617832183838, -0.5356594920158386, 0.009049906395375729, 0.0006872183876112103, -0.28281569480895996, 0.16709257662296295, 0.35170310735702515, 0.29872798919677734, 0.10379105061292648, 0.3644810616970062, 0.35705888271331787, 0.04822736606001854, 0.27053090929985046, -0.00808979943394661, 0.225054532289505, -0.8657597899436951, 0.33583587408065796, 0.05338103324174881, 0.10857314616441727, 0.036245908588171005, -0.19320811331272125, 0.21866759657859802, 0.268061101436615, -0.1343812346458435, 0.11278460174798965, 0.30408063530921936, 0.38808673620224, 0.3670588731765747, 0.09680350124835968, 0.12799395620822906, -0.19451791048049927, 0.4004421532154083, 0.0508347786962986, -0.1745114028453827, -0.14591418206691742, -0.05702788010239601, -0.08823634684085846, -0.33989328145980835, 0.06702537089586258, 0.030560914427042007, -0.19467774033546448, -0.250070720911026, -0.044620122760534286, -0.14228734374046326, 0.027587801218032837, -0.11671066284179688, 0.264196515083313, -0.29162701964378357, -0.1800466924905777, 0.1209251806139946, -0.06212000548839569, -0.05046972632408142, 0.602118730545044, -0.1955437958240509, 0.051669757813215256, 0.05212095007300377, 0.18243852257728577, 0.35923123359680176, -0.03124857135117054, -0.14384892582893372, -0.23937217891216278, -0.25339165329933167, 0.03941687196493149, -0.26079699397087097, 0.08868975192308426, 0.24885843694210052, 0.030868861824274063, -0.23112645745277405, 0.05195603892207146, 0.5343323945999146, -0.1015147790312767, 0.08603507280349731, -0.2127409130334854, -0.1645158976316452, 0.5177484154701233, 0.07561033964157104, -0.2403825968503952, 0.1380423903465271, -0.3370830714702606, 0.10243766754865646, 0.4649207890033722, -0.026182854548096657, -0.15980343520641327, 0.3333737254142761, -0.048629727214574814, -0.05700333043932915, -0.05303250625729561, 0.02458503469824791, -0.1371767669916153, -0.3099166452884674, 0.04329214245080948, -0.04591865465044975, 0.17255012691020966, -0.10029375553131104, 0.03264126181602478, 0.2206239253282547, -0.2880699336528778, -0.3839191496372223, 0.29367101192474365, -0.043675608932971954, 0.11099809408187866, -0.18327918648719788, 0.05344696715474129, 0.10198616236448288, 0.23260149359703064, -0.018191641196608543, -0.19102013111114502, 0.35126522183418274, 0.15345238149166107, 0.026867488399147987, 0.21956025063991547, -0.4060523808002472, 0.036113329231739044, -0.07814296334981918, -0.07012888789176941, -0.11492833495140076, 0.5175449252128601, -0.2642141580581665, 0.00715290242806077, -0.07262737303972244, 0.3162335157394409, 0.4993043541908264, -0.02984805591404438, 0.43203508853912354, -0.013334170915186405, -0.49424466490745544, 0.09261207282543182, -0.11281038075685501, 0.10942773520946503, -0.6826735138893127, -0.00143820745870471, -0.06128459423780441, 0.19827789068222046, 0.2460501790046692, -0.2800275683403015, -0.15596570074558258, 0.11412901431322098, -0.010251590982079506, -0.14746588468551636, -0.16092802584171295, -0.1918916255235672, 0.3455624580383301, -0.19518685340881348, 0.07330112904310226, -0.012207340449094772, -0.013814221136271954, 0.09760995954275131, -0.2728526294231415, -0.2919548749923706, -0.10650798678398132, -0.20684503018856049, -0.16471773386001587, -0.1844911426305771, 0.08587365597486496, -0.03992389887571335, -0.2173919677734375, -0.11459390819072723, 0.3166560232639313, -0.05833195522427559, 0.11307010054588318, -0.06102127954363823, 0.35963499546051025, 0.3026008903980255, -0.3239034414291382, -0.04854504019021988, 0.1537855863571167, -0.19600442051887512, -0.11757197976112366, 0.1898345649242401, 0.29548171162605286, 0.2843815088272095, 0.13444069027900696, 0.2466854602098465, 0.2249874770641327, 0.26211223006248474, -0.23224054276943207, -1.2658567428588867, -0.2791716158390045, 0.07983443140983582, 0.25216177105903625, 0.18745501339435577, -0.3084765672683716, 0.18575049936771393, -0.22876310348510742, 0.4259653687477112, 0.2809597849845886, -0.015498215332627296, 0.08052798360586166, -0.22706055641174316, 0.14607137441635132, 0.14272233843803406, 0.09967219829559326, -0.28764328360557556, 0.06421630084514618, -0.3331316411495209, 0.2703845202922821, -0.3657607138156891, 0.13727083802223206, -0.18061381578445435, -0.674980640411377, -0.09376366436481476, -0.013597194105386734, 0.9004976153373718, -0.30463558435440063, -0.05886401608586311, 0.025494005531072617, -0.15041963756084442, 0.516018271446228, -0.05207137390971184, -0.3670920431613922, 0.4514630436897278, 0.18787817656993866, -0.14095498621463776, -0.13423208892345428, 0.07557366788387299, 0.0006989863468334079, -0.43811097741127014, 0.07758042961359024, -0.028063060715794563, -0.7205224633216858, -0.46731677651405334, 0.10603732615709305, -0.018313689157366753, 0.22812560200691223, -0.5336476564407349, -0.00015997113951016217, 0.3124602735042572, 0.07020057737827301, 0.08764739334583282, -0.19440492987632751, -0.34887874126434326, -0.21275188028812408, -0.3580664396286011, 0.08210644870996475, 0.03800030052661896, 0.242331862449646, -0.026056217029690742, -0.17280025780200958, -0.1596905142068863, -0.1267997771501541, 0.4960964620113373, -0.3624955117702484, 0.05313301458954811, -0.1075301542878151, 0.3643958270549774, -0.06019119545817375, -0.12913741171360016, 0.6177809238433838, 0.10416367650032043, 0.0006394415395334363, 0.29369431734085083, -0.0164516381919384, -0.10260938107967377, -0.10041984915733337, -0.1423254907131195, 0.11573398113250732, 0.476567804813385, 0.06555818766355515, 0.0021354840137064457, 0.09296709299087524, 0.24531464278697968, 0.0312562957406044, 0.014382866211235523, 0.03680133447051048, 0.21565040946006775, 0.12087756395339966, -0.03052377514541149, 0.10308102518320084, -0.3015538156032562, -0.20847193896770477, -0.12028639018535614, 0.10031221061944962, -1.256784200668335, -0.10561338067054749, -0.1304120570421219, 0.3409503698348999, -0.17492008209228516, -0.02998625673353672, 0.19641916453838348, -0.19458016753196716, 0.10537705570459366, -0.09048120677471161, -0.06628970056772232, 0.11834598332643509, 0.2933056056499481, 0.127484530210495, 0.20175844430923462, -0.10623489320278168, 0.3323555290699005, -0.2516753375530243, 0.32500097155570984, -0.6081444621086121, -0.03668238967657089, 0.07494794577360153, 1.3081068992614746, -0.19326359033584595, 0.19326353073120117, 0.19292518496513367, -0.07231967896223068, 0.004611674230545759, -0.18494807183742523, -0.09251837432384491, -0.216699481010437, -0.03327419236302376, 0.9094654321670532, -0.24134524166584015, -0.016923071816563606, 0.4398593008518219, -0.39500460028648376, -0.00870345626026392, 0.3305012285709381, 0.026757605373859406, 0.1421709805727005, 0.2897998094558716, 0.17396731674671173, 0.009951998479664326, 0.6814603805541992, 0.008591912686824799, -0.036379918456077576, 0.08302256464958191, 0.09642527997493744, 0.11333788931369781, -0.2005409300327301, -0.09343480318784714, 0.018377041444182396, -0.2796942889690399, 0.20938625931739807, 0.3172471523284912, 0.15655098855495453, -0.049455028027296066, 0.03889935463666916, -0.023331018164753914, 0.07247336208820343, 0.14390607178211212, 0.06668394804000854, 0.12628760933876038, -0.3640329837799072], "sparse_embedding": null}, {"id": "1b2b11a90753f096660e31ed0f2544b4479500627a8415268d94e33f3915e85b", "content": "Despite the utility of Large Language Models (LLMs) across a wide range of\ntasks and scenarios, developing a method for reliably evaluating LLMs across\nvaried contexts continues to be challenging. Modern evaluation approaches often\nuse LLMs to assess responses generated by LLMs. However, the meta-evaluation\nconducted to assess the effectiveness of these LLMs as evaluators is typically\nconstrained by the coverage of existing benchmarks or requires extensive human\nannotation. This underscores the urgency of methods for scalable\nmeta-evaluation that can effectively, reliably, and efficiently evaluate the\nperformance of LLMs as evaluators across diverse tasks and scenarios,\nparticularly in potentially new, user-defined scenarios. To fill this gap, we\npropose ScaleEval, an agent-debate-assisted meta-evaluation framework that\nleverages the capabilities of multiple communicative LLM agents. This framework\nsupports multi-round discussions to assist human annotators in discerning the\nmost capable LLMs as evaluators, which significantly eases their workload in\ncases that used to require large-scale annotations during meta-evaluation. We\nrelease the code for our framework, which is publicly available at:\n\\url{https://github.com/GAIR-NLP/scaleeval}.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2401.16788v1", "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate", "content": "http://arxiv.org/pdf/2401.16788v1", "datetime": "2024-01-30 07:03:32", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! \n\nEvaluating LLMs across diverse tasks and scenarios is crucial yet challenging. To address this, we introduce ScaleEval, a cutting-edge meta-evaluation framework that employs agent-debate assistance to assess the effectiveness of LLMs as evaluators efficiently and reliably. Our framework facilitates multi-round discussions, aiding human annotators in identifying the most adept LLM evaluators.\n\nCurious to learn more about ScaleEval and how it enhances the evaluation of LLMs? Dive into the details and access our framework's code at: [http://arxiv.org/abs/2401.16788v1](http://arxiv.org/abs/2401.16788v1)\n\n#AI #NLP #LLMs #TechInnovation #ScaleEval", "x": "\ud83d\ude80 Exciting news in the world of Large Language Models (LLMs)! Check out ScaleEval, a cutting-edge meta-evaluation framework for assessing LLM performance across diverse tasks and scenarios. Developed by experts, this framework leverages multiple communicative LLM agents to streamline evaluation processes. Dive into the details and access the code here: http://arxiv.org/abs/2401.16788v1 #AI #NLP #LLMs #ScaleEval", "source_id": "a88cf2e76a40175d3157fdfe457c1a23b2050d7df8ef54ac8ed58b65f4c7d111", "page_number": 1}, "score": null, "embedding": [-0.4113856256008148, -0.13939660787582397, 0.09658589214086533, -0.06795631349086761, 0.026100151240825653, -0.1799076348543167, -0.10328281670808792, 0.11130659282207489, 0.23679909110069275, -0.35471829771995544, 0.028201604261994362, -0.04351197928190231, 0.266198992729187, 0.154860720038414, 0.3791503310203552, 0.17235808074474335, -0.1626303493976593, 0.3608263432979584, -0.2472020387649536, -0.25966086983680725, 0.31164732575416565, 0.13331234455108643, 0.10199730098247528, 0.013046701438724995, 0.0011500061955302954, 0.07388851791620255, -0.36686038970947266, -0.12360353767871857, -0.45449092984199524, -1.451030969619751, 0.3482292890548706, 0.024337900802493095, 0.3642583191394806, 0.051853641867637634, -0.22842998802661896, 0.10271763801574707, -0.14340023696422577, 0.007154315710067749, 0.051897089928388596, 0.04837404191493988, 0.12962356209754944, 0.29252007603645325, 0.07979186624288559, -0.08459781855344772, -0.21896100044250488, -0.41393500566482544, -0.388384610414505, 0.005279751028865576, -0.8246546983718872, -0.01121384184807539, -0.08365797996520996, -0.4286155700683594, 0.10974772274494171, 0.174152672290802, -0.03811043128371239, 0.06611035764217377, 0.092326320707798, 0.1810285449028015, 0.11656836420297623, 0.07832275331020355, 0.3409877121448517, 0.28984615206718445, -0.9549306035041809, 0.3626282513141632, -0.031615953892469406, 0.38050153851509094, -0.15499082207679749, 0.0011308451648801565, 0.07018992304801941, -0.04557591676712036, -0.16065987944602966, -0.14305615425109863, 0.30278196930885315, 0.34953978657722473, 0.4324968457221985, 0.4177497327327728, -0.02559433877468109, -0.19108347594738007, 0.36328744888305664, -0.10477466136217117, 0.2687133550643921, -0.08238092809915543, 0.0428062342107296, -0.11474073678255081, -0.017394447699189186, 0.012499107047915459, -0.21293208003044128, -0.009808347560465336, 0.19429220259189606, -0.18311478197574615, 0.092554472386837, -0.0393725261092186, -0.23301419615745544, 0.13958294689655304, -0.1779097020626068, -0.23372459411621094, 0.1333649605512619, 0.09172583371400833, -0.33735281229019165, 0.7714200019836426, -0.08614419400691986, 0.21482247114181519, 0.11827187240123749, -0.11093151569366455, 0.3183252513408661, -0.2648225724697113, -0.085340715944767, -0.44044992327690125, 0.007012960966676474, -0.0814160704612732, 0.03929629921913147, 0.02467573992908001, -0.02293337509036064, -0.2902066111564636, -0.008138777688145638, 0.03498249128460884, 0.24890466034412384, 0.12721368670463562, -0.0946454107761383, -0.2113298922777176, -0.1700362265110016, 0.08694718778133392, -0.05934567376971245, -0.16789716482162476, 0.23230725526809692, -0.06405599415302277, 0.13027061522006989, 0.6178827881813049, 0.182485893368721, 0.061055343598127365, 0.22439488768577576, -0.10210764408111572, -0.2565312683582306, 0.02023104391992092, 0.27166199684143066, -0.1879788637161255, -0.13567501306533813, -0.16949310898780823, 0.09937135875225067, 0.6112372875213623, -0.006636098958551884, -0.19798919558525085, -0.03884079307317734, -0.2498340606689453, -0.41374504566192627, 0.8167868852615356, 0.007793199736624956, -0.02230307273566723, -0.20769144594669342, -0.10850509256124496, 0.08163245022296906, 0.22149837017059326, -0.17062368988990784, -0.15766960382461548, 0.282683402299881, 0.1413470208644867, 0.08660466969013214, 0.25865787267684937, -0.38182172179222107, -0.0656936839222908, -0.06841907650232315, 0.08278045058250427, -0.32329246401786804, 0.7610704302787781, -0.4379187822341919, -0.48132428526878357, -0.3232550323009491, -0.11570411175489426, 0.08765017986297607, 0.11561771482229233, 0.3182530999183655, -0.05688414350152016, -0.16001801192760468, -0.15571345388889313, -0.04230817034840584, 0.011472254991531372, -0.44534924626350403, -0.020486406981945038, -0.04776258021593094, 0.04272297024726868, 0.009677832946181297, -0.133751779794693, -0.08926267921924591, 0.3832249641418457, 0.13262546062469482, -0.15019163489341736, 0.01670216955244541, -0.36480188369750977, 0.05080356076359749, 0.025453312322497368, -0.4521402418613434, 0.18966516852378845, -0.023615291342139244, 0.01729344017803669, -0.19541585445404053, -0.16359293460845947, -0.1634557545185089, -0.218939870595932, 0.06335332989692688, -0.26138001680374146, 0.12049642950296402, 0.25704270601272583, 0.20111051201820374, 0.010655040852725506, 0.30200889706611633, 0.18644969165325165, -0.2868499159812927, 0.10064995288848877, 0.1240563839673996, 0.2541428506374359, -0.32127630710601807, -0.02594846859574318, 0.37798482179641724, 0.21686923503875732, -0.24003934860229492, 0.15369851887226105, 0.33412009477615356, 0.1180480569601059, -0.027494363486766815, 0.18250861763954163, 0.15704122185707092, 0.07730742543935776, -0.08309738337993622, -1.1605297327041626, -0.1586768627166748, 0.2999863624572754, -0.0834910050034523, 0.14839492738246918, -0.4145159125328064, 0.01980799436569214, -0.12435294687747955, 0.13575312495231628, 0.40592941641807556, 0.23770835995674133, -0.001884861383587122, -0.14808902144432068, 0.12530001997947693, 0.09278728067874908, 0.21197038888931274, -0.24982427060604095, 0.28638267517089844, -0.26599305868148804, 0.3473144769668579, 0.2638178765773773, -0.15085168182849884, 0.2353946417570114, -0.4790511131286621, 0.03288042172789574, -0.13514059782028198, 0.9190763235092163, -0.06094077602028847, -0.31044280529022217, 0.06159017235040665, 0.16883867979049683, 0.29006072878837585, -0.03623698651790619, -0.33846035599708557, 0.49579155445098877, -0.11971119791269302, 0.18230682611465454, -0.1225188598036766, 0.1967473030090332, -0.06512356549501419, -0.1245516911149025, -0.13542264699935913, 0.2320239245891571, -0.7400951981544495, -0.23900483548641205, 0.08667638152837753, 0.08542807400226593, -0.020293598994612694, -0.2960023581981659, 0.15856339037418365, 0.11685645580291748, -0.1100938618183136, 0.16088230907917023, -0.19987799227237701, -0.24231648445129395, -0.10715463757514954, -0.4098261594772339, 0.24157026410102844, -0.14272889494895935, 0.11410919576883316, 0.20914487540721893, 0.12835440039634705, -0.026162510737776756, -0.03470347821712494, 0.2820952236652374, -0.17747190594673157, 0.00024936709087342024, 0.09242238849401474, -0.013862146064639091, -0.4274763762950897, -0.16158172488212585, 0.5451864004135132, -0.3240003287792206, 0.14879587292671204, -0.0756944939494133, 0.3127478361129761, 0.003621560288593173, -0.29371803998947144, -0.2112559974193573, 0.06543318927288055, 0.41474273800849915, 0.22563223540782928, -0.005265698302537203, 0.08231443911790848, 0.19866560399532318, -0.056050851941108704, 0.14034566283226013, -0.0290779247879982, 0.14383046329021454, 0.018833059817552567, 0.018388904631137848, 0.00503989402204752, -0.4655684530735016, 0.10882546007633209, -0.051850516349077225, 0.06579075753688812, -1.0425063371658325, 0.121683768928051, 0.16288788616657257, 0.14360816776752472, -0.046342313289642334, 0.02361251227557659, 0.050680797547101974, -0.23314855992794037, -0.3291742205619812, -0.1347447782754898, 0.13371685147285461, 0.2760041058063507, 0.20267222821712494, -0.00919607188552618, 0.33174246549606323, -0.20850475132465363, 0.22529618442058563, -0.13698320090770721, 0.17360854148864746, -0.6227824687957764, 0.009781417436897755, 0.13747906684875488, 1.1931232213974, -0.1174032911658287, -0.0586065910756588, 0.31073519587516785, 0.04524108022451401, -0.24295373260974884, -0.17142540216445923, -0.10254194587469101, -0.0620107464492321, 0.03944811224937439, 0.7150098085403442, -0.1130729466676712, 0.08430680632591248, 0.2615048885345459, -0.22281326353549957, -0.1747107207775116, 0.38872846961021423, 0.10757426172494888, 0.5102976560592651, 0.015045450069010258, 0.1762857586145401, -0.11930596828460693, 0.764721691608429, 0.12480395287275314, 0.13797922432422638, -0.45583608746528625, -0.17411409318447113, -0.02945912443101406, -0.17991933226585388, -0.05543876066803932, -0.09924832731485367, -0.09434859454631805, 0.28112924098968506, 0.0020693112164735794, 0.10003066807985306, 0.11526096612215042, -0.2931137979030609, -0.20022915303707123, 0.009889395907521248, -0.08812098205089569, 0.2613798677921295, 0.036591291427612305, -0.058023445308208466], "sparse_embedding": null}, {"id": "c617cda5156276c2c0de16e3276b09b3d3e6652cda052a4c704a2411fce87e9d", "content": "Large Language Models (LLMs) are essential tools to collaborate with users on\ndifferent tasks. Evaluating their performance to serve users' needs in\nreal-world scenarios is important. While many benchmarks have been created,\nthey mainly focus on specific predefined model abilities. Few have covered the\nintended utilization of LLMs by real users. To address this oversight, we\npropose benchmarking LLMs from a user perspective in both dataset construction\nand evaluation designs. We first collect 1846 real-world use cases with 15 LLMs\nfrom a user study with 712 participants from 23 countries. These self-reported\ncases form the User Reported Scenarios(URS) dataset with a categorization of 7\nuser intents. Secondly, on this authentic multi-cultural dataset, we benchmark\n10 LLM services on their efficacy in satisfying user needs. Thirdly, we show\nthat our benchmark scores align well with user-reported experience in LLM\ninteractions across diverse intents, both of which emphasize the overlook of\nsubjective scenarios. In conclusion, our study proposes to benchmark LLMs from\na user-centric perspective, aiming to facilitate evaluations that better\nreflect real user needs. The benchmark dataset and code are available at\nhttps://github.com/Alice1998/URS.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2404.13940v2", "title": "A User-Centric Benchmark for Evaluating Large Language Models", "content": "http://arxiv.org/pdf/2404.13940v2", "datetime": "2024-04-23 01:58:24", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting News in AI and NLP! \ud83d\ude80\n\nWhen it comes to evaluating Large Language Models (LLMs), understanding real user needs and experiences is key. A recent study has proposed a groundbreaking approach to benchmarking LLMs from a user-centric perspective. \n\nBy collecting 1846 real-world use cases from 712 participants across 23 countries, the User Reported Scenarios (URS) dataset was created to categorize user intents. This dataset was then used to benchmark 10 LLM services on their effectiveness in meeting user needs, with results aligning closely with user-reported experiences. \n\nThis innovative approach sheds light on the importance of considering subjective scenarios in evaluating LLMs and aims to better reflect real user needs. If you're interested in learning more or accessing the benchmark dataset and code, check out the study at: [http://arxiv.org/abs/2404.13940v2](http://arxiv.org/abs/2404.13940v2) \n\n#AI #NLP #LLMs #UserExperience #Benchmarking #TechInnovation", "x": "\ud83d\ude80 Exciting new research on Large Language Models (LLMs) evaluation! A user-centric benchmarking approach was taken to assess 10 LLM services using 1846 real-world use cases from 712 participants across 23 countries. Results show alignment between benchmark scores and user-reported experiences. Check out the study here: http://arxiv.org/abs/2404.13940v2 #AI #NLP #LLMs #TechResearch", "source_id": "f329ead47d99660390f72aa84c54776cccf81c04c170968d454263a70bc50499", "page_number": 1}, "score": null, "embedding": [-0.30405765771865845, 0.025179266929626465, 0.11974958330392838, -0.21843591332435608, 0.12403629720211029, 0.09268375486135483, -0.10207024216651917, 0.14039407670497894, 0.37733185291290283, -0.348863810300827, 0.0069135637022554874, -0.047796811908483505, 0.15957902371883392, 0.19680428504943848, 0.2500034272670746, 0.13219551742076874, -0.1725103110074997, -0.05260494723916054, -0.06974539905786514, 0.03625890985131264, 0.30245521664619446, -0.21128304302692413, 0.11294516921043396, -0.023976758122444153, -0.053319524973630905, 0.01945449970662594, -0.33184418082237244, -0.04486220329999924, -0.24819529056549072, -1.2738152742385864, -0.03483596444129944, -0.22387467324733734, 0.33434054255485535, 0.09943637251853943, -0.286468505859375, -0.07485011965036392, 0.0032130402978509665, 0.18339814245700836, 0.15873770415782928, 0.09484606236219406, 0.10403003543615341, 0.08205823600292206, 0.1953601837158203, -0.0060981158167123795, -0.19774170219898224, -0.3642301559448242, -0.17843399941921234, -0.02684967964887619, -0.6082350015640259, 0.07513167709112167, 0.10924048721790314, -0.43043985962867737, -0.007891583256423473, 0.34654349088668823, 0.17585688829421997, 0.20593732595443726, 0.15814493596553802, 0.05786943435668945, 0.07358590513467789, 0.19194087386131287, 0.30137088894844055, 0.013224572874605656, -1.0793941020965576, 0.24787120521068573, -0.10118603706359863, 0.18945060670375824, -0.042559970170259476, 0.003857928328216076, -0.007036890368908644, 0.1352902203798294, -0.3259793221950531, 0.1679995208978653, 0.2945740818977356, 0.40347570180892944, 0.32392024993896484, 0.29058173298835754, 0.119998499751091, -0.1880522519350052, -0.04212505742907524, -0.024792632088065147, -0.1104523316025734, -0.01913217082619667, -0.046250734478235245, -0.3020276427268982, -0.22124145925045013, 0.14383965730667114, -0.0034027118235826492, -0.09350494295358658, -0.07239390909671783, -0.1806456744670868, -0.09917449951171875, -0.11203999817371368, -0.19266949594020844, 0.4853630065917969, -0.07760908454656601, 0.004118452314287424, 0.3081197738647461, 0.16056044399738312, -0.31584522128105164, 0.6189024448394775, -0.21940574049949646, -0.004165886901319027, -0.0394858792424202, -0.05072513222694397, 0.3516824543476105, -0.049213167279958725, -0.11760921776294708, -0.3199025094509125, -0.008536971174180508, -0.09553834050893784, -0.0104717668145895, 0.04555266350507736, -0.18968357145786285, -0.1362568438053131, 0.1231558546423912, -0.08285746723413467, 0.3949943780899048, 0.008138172328472137, 0.11609715968370438, -0.07727544009685516, -0.23378562927246094, 0.26128149032592773, 0.08438526093959808, -0.3634278476238251, 0.07892724126577377, -0.057513266801834106, 0.2959238588809967, 0.496308296918869, 0.11463752388954163, 0.100225068628788, 0.0594138503074646, 0.0041066431440413, -0.4696659743785858, 0.08479820936918259, -0.10997387766838074, -0.21206755936145782, 0.14909373223781586, -0.05627840384840965, 0.21995572745800018, 0.21936552226543427, 0.07928885519504547, 0.14278359711170197, 0.09043442457914352, -0.18213750422000885, -0.2607790529727936, 0.8815639019012451, -0.07145519554615021, 0.04391447454690933, -0.3638973832130432, -0.016035165637731552, 0.1660117655992508, 0.32981443405151367, -0.19503025710582733, -0.262949138879776, 0.22067752480506897, 0.1451585292816162, 0.05543160066008568, 0.08713158965110779, -0.3770183324813843, 0.00031097730970941484, 0.029809823259711266, -0.15041635930538177, -0.2608472406864166, 0.6026160717010498, -0.20859910547733307, -0.26650741696357727, -0.21501478552818298, -0.007810396142303944, 0.19195392727851868, -0.05840487405657768, 0.08947402983903885, 0.017059193924069405, -0.24173717200756073, 0.09875430911779404, -0.08503136038780212, 0.009247003123164177, -0.7424082159996033, -0.10403797775506973, -0.31496661901474, 0.1281510442495346, 0.11881837993860245, -0.03296807035803795, -0.1200045719742775, 0.08073010295629501, -0.07505909353494644, -0.16741958260536194, 0.013865286484360695, -0.07810432463884354, 0.19818730652332306, 0.029789909720420837, -0.2482472062110901, 0.32269105315208435, -0.1302211433649063, 0.16381922364234924, -0.0790610983967781, -0.27778929471969604, -0.12109552323818207, -0.07582240551710129, 0.10257943719625473, -0.0603942796587944, 0.07989285886287689, 0.16843590140342712, 0.0844302773475647, -0.17597277462482452, 0.12082433700561523, 0.16115379333496094, -0.19325482845306396, -0.007513953372836113, 0.3220897316932678, 0.3006194531917572, -0.4046420156955719, 0.05426684394478798, 0.2613098919391632, -0.15954981744289398, -0.033483389765024185, -0.07773464173078537, 0.39297160506248474, 0.2106206715106964, -0.070354163646698, 0.33335453271865845, 0.04953857883810997, 0.3060782253742218, -0.09668300300836563, -1.2768528461456299, -0.0022879932075738907, 0.22267957031726837, 0.08882991224527359, 0.09508329629898071, -0.17457683384418488, -0.1589263677597046, 0.10220807045698166, 0.07042010128498077, 0.47756820917129517, 0.3855552077293396, -0.06275369971990585, -0.23720762133598328, 0.19498713314533234, 0.020594405010342598, -0.023539181798696518, -0.4120393991470337, 0.08420698344707489, -0.45343735814094543, 0.23582938313484192, -0.08957444876432419, 0.11422718316316605, 0.12674938142299652, -0.5486255884170532, 0.21083451807498932, -0.16757771372795105, 0.7563004493713379, -0.2522583603858948, -0.19730862975120544, -0.233993262052536, 0.20236411690711975, 0.16236092150211334, -0.17774678766727448, -0.5213578343391418, 0.1419307440519333, 0.1653398722410202, -6.508737897092942e-06, -0.06975360214710236, 0.22781623899936676, 0.03562057018280029, -0.21372917294502258, -0.08464348316192627, 0.1708126813173294, -0.6020753383636475, -0.3273530900478363, 0.13177090883255005, 0.06404417753219604, -0.13620054721832275, -0.45513972640037537, 0.15545853972434998, 0.14088009297847748, 0.027541643008589745, 0.24620211124420166, 0.04487180337309837, -0.3456677794456482, -0.21218858659267426, -0.7861723899841309, 0.21428948640823364, -0.11386507004499435, 0.1523558646440506, 0.12883540987968445, -0.13874849677085876, -0.004824185743927956, -0.17584234476089478, 0.1905025690793991, -0.3896452486515045, -0.13618481159210205, 0.19754239916801453, 0.04617097228765488, -0.23719561100006104, -0.15808425843715668, 0.4403230845928192, -0.14918704330921173, 0.1714629828929901, 0.23330311477184296, -0.011302651837468147, -0.0891561210155487, -0.1390416920185089, -0.08498267084360123, -0.059220097959041595, 0.4013979434967041, 0.06450999528169632, 0.0051224795170128345, 0.09321915358304977, 0.14919818937778473, 0.12930114567279816, 0.31014764308929443, 0.22497259080410004, 0.05255318060517311, 0.01375207956880331, -0.06893523037433624, -0.175764799118042, -0.27764692902565, 0.01006396021693945, -0.1245778426527977, 0.07720618695020676, -1.0036170482635498, 0.04278486967086792, -0.16225650906562805, 0.4146103858947754, -0.0726810097694397, -0.07692909985780716, 0.15645594894886017, 0.0008568120538257062, 0.201568141579628, 0.2445196956396103, 0.17421644926071167, 0.07867666333913803, 0.1527259647846222, -0.10368742793798447, 0.1507302075624466, -0.2029343545436859, 0.17602954804897308, 0.031550534069538116, 0.29219546914100647, -0.308489054441452, 0.04024842754006386, 0.11622918397188187, 1.0649735927581787, -0.07058537006378174, 0.05341332033276558, 0.4338732361793518, -0.04023650661110878, -0.05775976926088333, -0.012870424427092075, 0.21321779489517212, -0.18619686365127563, 0.1341918706893921, 0.8220836520195007, 0.054743584245443344, -0.13459515571594238, 0.46951401233673096, -0.09204009920358658, 0.03014516644179821, 0.2554331123828888, 0.2170669436454773, 0.3424161672592163, -0.11108853667974472, 0.1293780356645584, -0.12862323224544525, 0.743192732334137, -0.0004976287018507719, 0.2073706090450287, -0.526185929775238, 0.11436230689287186, -0.04964021220803261, -0.11355531215667725, -0.10075680166482925, -0.18813668191432953, 0.05989699065685272, 0.34824877977371216, 0.26138409972190857, -0.0884999930858612, -0.10172818601131439, -0.13194389641284943, -0.11272002011537552, -0.004408496432006359, 0.09896069765090942, 0.1053076907992363, -0.3113710880279541, -0.29432716965675354], "sparse_embedding": null}, {"id": "a70fcfada9bb004b4006decfa221c19975eedd91dd6b43a28f2a0c762b704a09", "content": "Biomedical language understanding benchmarks are the driving forces for\nartificial intelligence applications with large language model (LLM) back-ends.\nHowever, most current benchmarks: (a) are limited to English which makes it\nchallenging to replicate many of the successes in English for other languages,\nor (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs\napply these knowledge to perform on a wide range of bio-medical tasks, or (c)\nhave become a publicly available corpus and are leaked to LLMs during\npre-training. To facilitate the research in medical LLMs, we re-build the\nChinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a\nlarge scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable\ntest-bed and an online platform for evaluating Chinese LLMs' multi-task\ncapabilities on a wide range bio-medical tasks including medical entity\nrecognition, medical text classification, medical natural language inference,\nmedical dialogue understanding and medical content/dialogue generation. To\nestablish evaluation on these tasks, we have experimented and report the\nresults with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning\ntechniques.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2310.14151v1", "title": "PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain", "content": "http://arxiv.org/pdf/2310.14151v1", "datetime": "2023-10-22 02:20:38", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting News in the World of AI and NLP! \ud83c\udf1f\n\nBiomedical language understanding benchmarks play a crucial role in advancing artificial intelligence applications with large language model (LLM) back-ends. However, existing benchmarks face limitations such as being restricted to English, focusing solely on knowledge probing of LLMs, or being leaked to LLMs during pre-training.\n\nTo address these challenges and drive innovation in medical LLMs, we introduce PromptCBLUE - a revamped version of the Chinese Biomedical Language Understanding Evaluation benchmark. PromptCBLUE serves as a comprehensive platform for evaluating Chinese LLMs' multi-task capabilities across various bio-medical tasks.\n\nCurious about the results of our experiments with 9 Chinese LLMs fine-tuned using different techniques? Dive into our research paper to learn more: http://arxiv.org/abs/2310.14151v1\n\nLet's continue pushing the boundaries of AI and NLP together! \ud83c\udf10\ud83d\udca1\n#AI #NLP #LLM #MedicalAI #Research #Innovation", "x": "\ud83d\ude80 Exciting news in the world of AI and NLP! Explore how the new PromptCBLUE benchmark is revolutionizing Chinese biomedical language understanding evaluation for LLMs. Dive into the research and results here: http://arxiv.org/abs/2310.14151v1 #AI #NLP #LLMs #MedTech #Research #TechInnovation \ud83e\uddec\ud83d\udd0d\ud83e\udd16", "source_id": "50b6b674058bfdae80113c640d9ccaa774e29ee94346537304566de2ad7daf0b", "page_number": 1}, "score": null, "embedding": [-0.4477560818195343, -0.037751488387584686, 0.09953823685646057, -0.041626762598752975, 0.010234475135803223, 0.023337511345744133, 0.07562966644763947, 0.08886322379112244, 0.3965144157409668, -0.32975661754608154, -0.07976682484149933, -0.4982413053512573, 0.02988051250576973, 0.16661004722118378, 0.11268576234579086, 0.025490550324320793, 0.01538020372390747, -0.009047308005392551, -0.21428485214710236, 0.22132129967212677, 0.18235674500465393, -0.1098732128739357, 0.11444418877363205, 0.061127640306949615, -0.08217380940914154, -0.23156240582466125, -0.15149950981140137, -0.06417064368724823, -0.21353183686733246, -1.1277389526367188, 0.08077392727136612, -0.17865535616874695, 0.3114665448665619, 0.05648241192102432, -0.032091282308101654, 0.12701113522052765, -0.14862972497940063, 0.13495773077011108, -0.01228281483054161, 0.11706706881523132, 0.2721768915653229, 0.1840549260377884, 0.16232506930828094, 0.10161493718624115, 0.19535508751869202, -0.3377131223678589, -0.11251627653837204, -0.13054823875427246, -0.45171698927879333, -0.0545077808201313, -0.09180569648742676, -0.22137562930583954, 0.2625235915184021, 0.10654442012310028, 0.018042122945189476, 0.14742907881736755, -0.014762278646230698, 0.1726880520582199, 0.06542644649744034, 0.10906009376049042, 0.39800798892974854, 0.2965908646583557, -0.9580208659172058, 0.4380192160606384, 0.07256961613893509, -0.07520623505115509, -0.16899894177913666, -0.26157575845718384, 0.2713340222835541, 0.21263186633586884, -0.15274696052074432, 0.011983396485447884, 0.4148452877998352, 0.40890446305274963, 0.1302996426820755, 0.19711318612098694, 0.3342784643173218, -0.20199717581272125, -0.008587807416915894, -0.13403233885765076, 0.0711500346660614, -0.029407750815153122, -0.0025255680084228516, -0.4928436279296875, -0.09465766698122025, 0.23758269846439362, -0.1514294594526291, -0.01078442670404911, 0.06555641442537308, 0.1832187920808792, 0.029130810871720314, -0.03803855553269386, -0.43801459670066833, 0.5066311359405518, -0.3649148941040039, -0.15104593336582184, 0.28729408979415894, -0.026421064510941505, -0.20303744077682495, 0.6749584078788757, -0.3087882399559021, -0.110501728951931, -0.11787060648202896, -0.28229984641075134, 0.5089528560638428, -0.22327212989330292, -0.16775527596473694, -0.041019830852746964, -0.10817404091358185, -0.047907982021570206, -0.051170967519283295, -0.011944752186536789, 0.12684248387813568, 0.044586893171072006, 0.03330343961715698, 0.14224877953529358, 0.5714318752288818, 0.01968935877084732, 0.014578153379261494, 0.1098228171467781, -0.32529035210609436, 0.14901165664196014, -0.0872240886092186, -0.41907814145088196, 0.008194818161427975, 0.020292527973651886, 0.22057875990867615, 0.4816363751888275, 0.09675200283527374, -0.09429750591516495, 0.40104371309280396, -0.0970059409737587, -0.36121881008148193, -0.08105591684579849, 0.07457653433084488, -0.08815131336450577, -0.19299793243408203, -0.17045466601848602, 0.2668164074420929, -0.06225209683179855, -0.0424533486366272, -0.022498343139886856, 0.012430636212229729, -0.15778502821922302, -0.6067913174629211, 0.629980206489563, -0.1080673485994339, 0.09158969670534134, -0.22584635019302368, -0.1766405999660492, 0.0342160202562809, 0.09175440669059753, -0.062039054930210114, -0.06081585958600044, 0.19081993401050568, 0.2583180069923401, 0.29268738627433777, 0.09978877753019333, -0.21507005393505096, -0.1576889008283615, 0.04379177838563919, -0.11282116919755936, -0.283096045255661, 0.8906931281089783, 0.16336631774902344, -0.17034628987312317, -0.3272968828678131, -0.0876147523522377, 0.1953936517238617, -0.1436542123556137, 0.25487881898880005, -0.04079027846455574, -0.19878016412258148, -0.023046595975756645, -0.17548806965351105, -0.16169674694538116, -0.4385647475719452, -0.1489880234003067, -0.07897992432117462, 0.0025994936004281044, 0.2421785295009613, -0.29257461428642273, -0.07232972979545593, 0.17580363154411316, -0.08425989747047424, -0.3771946132183075, -0.13862663507461548, -0.2986297011375427, 0.21765485405921936, 0.09311245381832123, -0.3105149567127228, 0.2560139000415802, 0.04304345324635506, 0.20937694609165192, -0.08945604413747787, -0.2525947690010071, -0.13814827799797058, -0.07267189770936966, 0.14410753548145294, -0.11629878729581833, 0.0021583973430097103, 0.046556200832128525, -0.03468644246459007, 0.11171089857816696, 0.011268731206655502, -0.24981807172298431, 0.00799721572548151, 0.09809045493602753, 0.4079833924770355, 0.7115700244903564, -0.20675821602344513, 0.2701190412044525, 0.18194103240966797, -0.28685373067855835, -0.019191842526197433, -0.17697474360466003, 0.04715793952345848, 0.0314454548060894, -0.17843322455883026, 0.2520543336868286, 0.14767909049987793, 0.10972175747156143, -0.16386792063713074, -1.1988756656646729, -0.10162565112113953, 0.19379882514476776, -0.1619304120540619, 0.3034524917602539, -0.15496549010276794, -0.03527363762259483, 0.2181910127401352, 0.2311033308506012, 0.2780785858631134, 0.04811844602227211, 0.07434214651584625, -0.22715403139591217, 0.02023777924478054, -0.22293683886528015, 0.07460569590330124, 0.14623121917247772, 0.09052799642086029, -0.2627893090248108, 0.27020153403282166, 0.387324720621109, 0.04001101478934288, -0.01253957487642765, -0.4406571686267853, 0.11846893280744553, -0.3160834014415741, 0.6058235168457031, -0.2309388667345047, 0.18916542828083038, 0.015337321907281876, 0.1587364822626114, 0.18793855607509613, 0.06064774468541145, -0.5578030943870544, 0.2697795331478119, 0.03117525763809681, -0.24696487188339233, -0.1099582239985466, 0.00799911841750145, 0.1167270839214325, -0.23131245374679565, -0.1455807089805603, 0.12461020052433014, -0.42245909571647644, -0.4101991057395935, 0.06205206736922264, 0.14816144108772278, -0.32179662585258484, -0.4098052978515625, 0.05059931054711342, 0.1649109572172165, -0.00504970271140337, -0.11583833396434784, 0.2677583694458008, -0.22952869534492493, -0.1526515781879425, -0.548860490322113, 0.10243725776672363, -0.3929920196533203, -0.03603614866733551, 0.13602276146411896, -0.10449986159801483, -0.05147170275449753, 0.030642319470643997, 0.18881972134113312, -0.2870264947414398, -0.06628362089395523, 0.05777141824364662, 0.15918929874897003, -0.10184672474861145, -0.1076013371348381, 0.6659310460090637, 0.14604419469833374, 0.11616988480091095, 0.2119615525007248, 0.14638014137744904, 0.16287048161029816, -0.2697488069534302, -0.13525846600532532, -0.11781919747591019, 0.5899928212165833, 0.33050909638404846, 0.0007484210073016584, -0.07354331016540527, 0.1478511244058609, -0.037507541477680206, 0.5052765011787415, -0.02747885324060917, 0.043477583676576614, 0.20284804701805115, -0.07073643803596497, 0.0850665494799614, -0.354756623506546, -0.25854456424713135, -0.06686857342720032, 0.2047460824251175, -1.0340900421142578, 0.2904297411441803, 0.0422571562230587, 0.4782926142215729, -0.12499884516000748, -0.00024163350462913513, 0.03008153848350048, -0.1100417822599411, 0.026595862582325935, 0.43260547518730164, -0.02723350003361702, 0.04688549414277077, 0.0752779021859169, -0.16713665425777435, -0.05084783956408501, -0.1588229238986969, 0.5470018982887268, -0.1822805106639862, 0.17417052388191223, -0.06860774010419846, -0.12061209976673126, -0.1428043395280838, 1.0973756313323975, -0.13635466992855072, -0.032720696181058884, 0.14562174677848816, -0.26250720024108887, -0.2527202367782593, 0.07014267891645432, 0.01782170869410038, 0.07046429067850113, 0.029382696375250816, 0.6834908723831177, -0.13907454907894135, 0.06462976336479187, 0.566064178943634, -0.1753757894039154, 0.08830167353153229, 0.2501234710216522, 0.22408229112625122, 0.49104151129722595, -0.013401935808360577, -0.26553860306739807, -0.18809470534324646, 0.4200236201286316, 0.17273345589637756, -0.12492197006940842, -0.3082304000854492, 0.03310162201523781, -0.08474423736333847, -0.17864638566970825, 0.05180950090289116, -0.045850206166505814, -0.02137019671499729, 0.2553870975971222, 0.28091615438461304, 0.34448865056037903, -0.33979377150535583, 0.0016066444804891944, -0.20871755480766296, 0.12318730354309082, -0.09369761496782303, -0.22367216646671295, 0.3343042731285095, -0.3771805465221405], "sparse_embedding": null}, {"id": "e7ac09c228c26db37659b7317ef439f859f7da03abcd73bcc1465d121172e618", "content": "Recent advances in large language models (LLMs) have led to the development\nof various evaluation benchmarks. These benchmarks typically rely on a single\ninstruction template for evaluating all LLMs on a specific task. In this paper,\nwe comprehensively analyze the brittleness of results obtained via\nsingle-prompt evaluations across 6.5M instances, involving 20 different LLMs\nand 39 tasks from 3 benchmarks. To improve robustness of the analysis, we\npropose to evaluate LLMs with a set of diverse prompts instead. We discuss\ntailored evaluation metrics for specific use cases (e.g., LLM developers vs.\ndevelopers interested in a specific downstream task), ensuring a more reliable\nand meaningful assessment of LLM capabilities. We then implement these criteria\nand conduct evaluations of multiple models, providing insights into the true\nstrengths and limitations of current LLMs.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2401.00595v3", "title": "State of What Art? A Call for Multi-Prompt LLM Evaluation", "content": "http://arxiv.org/pdf/2401.00595v3", "datetime": "2024-05-06 10:20:26", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting developments in the field of large language models (LLMs)! A recent study delves into the brittleness of single-prompt evaluations and proposes a more robust approach using diverse prompts. With analysis across 6.5M instances and 20 LLMs, this research offers tailored evaluation metrics for different user cases. Check out the full paper for insights into enhancing the assessment of LLM capabilities: http://arxiv.org/abs/2401.00595v3 #AI #NLP #LLMs #TechResearch \ud83d\udcca\ud83d\udd0d", "x": "\ud83d\ude80 Exciting new research on large language models (LLMs) evaluation benchmarks! This study analyzes the brittleness of single-prompt evaluations across 6.5M instances with 20 LLMs and 39 tasks. Explore how diverse prompts can lead to more robust assessments of LLM capabilities. Check out the full paper here: http://arxiv.org/abs/2401.00595v3 #AI #NLP #LLMs #Research #Tech \ud83d\udd0d\ud83d\udcca", "source_id": "8862db5175e42699addffd369de128d731baeff9b779de29ad65bf4a0ca7da8b", "page_number": 1}, "score": null, "embedding": [-0.4604327380657196, -0.13876068592071533, 0.08056215941905975, -0.1274825781583786, -0.14356617629528046, 0.12115824967622757, -0.313810259103775, 0.06648431718349457, 0.2554818391799927, -0.45540618896484375, -0.057606495916843414, -0.18052642047405243, 0.13881346583366394, 0.25496360659599304, 0.13883259892463684, 0.18019498884677887, -0.08726434409618378, 0.19645892083644867, -0.027782168239355087, -0.25635507702827454, 0.35676309466362, -0.20644645392894745, 0.07576972246170044, -0.034386634826660156, 0.11981452256441116, -0.14979493618011475, -0.055271487683057785, -0.10370868444442749, -0.2694757878780365, -1.5067191123962402, 0.1183590292930603, -0.12761469185352325, 0.41094785928726196, -0.001068337238393724, -0.19609762728214264, 0.1842876672744751, -0.27561283111572266, 0.035272154957056046, -0.05577688291668892, 0.02941952645778656, 0.11956699192523956, 0.32358139753341675, -0.04798056557774544, -0.23073463141918182, -0.20942968130111694, -0.517108142375946, -0.24773555994033813, -0.20375806093215942, -0.5129420161247253, 0.0007297663833014667, -0.03633520007133484, -0.4056829810142517, 0.26436659693717957, 0.0765867605805397, 0.04417016729712486, 0.09734859317541122, 0.0028997508343309164, 0.2383236438035965, -0.09051952511072159, 0.16317883133888245, 0.336044043302536, 0.12851381301879883, -1.0652410984039307, 0.21088308095932007, 0.1356387585401535, 0.2676597535610199, -0.03782984986901283, 0.03419950231909752, -0.04572049528360367, 0.35526829957962036, -0.1135491132736206, 0.03833182528614998, 0.3555208444595337, 0.5341017842292786, 0.2555190324783325, 0.17401987314224243, 0.15450848639011383, -0.13129861652851105, 0.05829857289791107, -0.03417706862092018, -0.033310666680336, -0.3026784360408783, 0.016521362587809563, -0.2763863503932953, -0.10258890688419342, 0.17037539184093475, -0.2011328935623169, -0.013494515791535378, 0.14016224443912506, -0.0812106803059578, 0.13491950929164886, 0.006866179872304201, -0.25035181641578674, 0.4639629125595093, -0.18730010092258453, 0.09740712493658066, 0.3684229850769043, -0.14400440454483032, -0.3240169286727905, 0.6493035554885864, -0.23127636313438416, 0.07178439944982529, 0.0858125239610672, -0.1409270465373993, 0.17145375907421112, 0.07847916334867477, -0.07297258079051971, -0.3977462649345398, 0.015270648524165154, -0.131492480635643, 0.06491183489561081, -0.009348453022539616, -0.19199217855930328, -0.22041912376880646, -0.12871143221855164, -0.02612989768385887, 0.3373382091522217, -0.09221058338880539, 0.11818746477365494, 0.18196861445903778, -0.3960151672363281, 0.1943667232990265, -0.04715254157781601, -0.31684166193008423, 0.16923457384109497, -0.15153932571411133, 0.2019626945257187, 0.4422769248485565, 0.19940614700317383, -0.04215329885482788, 0.12369495630264282, -0.08484597504138947, -0.44268447160720825, -0.03157767280936241, 0.129275381565094, -0.04751203954219818, -0.005852831993252039, -0.10605412721633911, 0.23475250601768494, 0.24585029482841492, -0.16686508059501648, 0.039450887590646744, 0.2664131224155426, -0.19762597978115082, -0.23526500165462494, 0.9734976887702942, -0.14813493192195892, 0.016002295538783073, -0.2882380485534668, -0.15331369638442993, 0.023718882352113724, 0.17896056175231934, -0.1419309675693512, -0.2785091996192932, 0.3387371301651001, 0.19853653013706207, -0.000698012881912291, 0.034634802490472794, -0.22189797461032867, 0.03749007359147072, 0.05410285294055939, -0.1364595890045166, -0.2740289866924286, 0.8269491791725159, -0.18405099213123322, -0.2250271439552307, -0.2466985583305359, 0.09281406551599503, -0.042649004608392715, -0.1964215636253357, 0.2314322292804718, 0.2534205913543701, -0.2686924636363983, -0.07056193798780441, -0.22026067972183228, -0.012773328460752964, -0.407947838306427, -0.11585861444473267, -0.08232448995113373, 0.11212905496358871, 0.0334908626973629, -0.01380069088190794, -0.024415835738182068, 0.1539069414138794, 0.0002752669097390026, -0.08839991688728333, -0.01843852922320366, -0.2921963334083557, 0.052228670567274094, 0.07616647332906723, -0.5634874105453491, 0.1824430227279663, 0.16689682006835938, 0.09949488937854767, -0.19595959782600403, -0.2546369135379791, -0.02358953095972538, -0.17868353426456451, 0.18915292620658875, -0.12259325385093689, 0.12476208806037903, 0.1414538472890854, 0.01803564839065075, -0.09512905776500702, 0.21387216448783875, 0.10080449283123016, -0.10028480738401413, 0.17656880617141724, 0.2739473283290863, 0.6104212999343872, -0.36110228300094604, -0.006017913576215506, 0.43626880645751953, -0.221164271235466, 0.022691801190376282, -0.021787302568554878, 0.3112480044364929, 0.16900844871997833, 0.04383073374629021, 0.21276973187923431, 0.11861327290534973, 0.08477276563644409, -0.10561133176088333, -1.2476844787597656, -0.07607858628034592, 0.1934700459241867, 0.04082551598548889, 0.3436184227466583, -0.289253830909729, 0.13117973506450653, -0.10619516670703888, 0.15536600351333618, 0.2839384973049164, 0.1696675419807434, 0.03936612606048584, -0.19427205622196198, 0.055896222591400146, 0.0361887626349926, -0.019793273881077766, -0.2723102271556854, 0.12935781478881836, -0.4181651771068573, 0.339509516954422, -0.013026018626987934, 0.03611953929066658, 0.10930224508047104, -0.6296330690383911, 0.039034176617860794, -0.2221362292766571, 0.7319893836975098, -0.3400760591030121, 0.004544902127236128, -0.17739146947860718, 0.22397372126579285, 0.15018516778945923, 0.12391386181116104, -0.2933639883995056, 0.4844163954257965, 0.1973196417093277, -0.04930587857961655, 0.007003247272223234, 0.19244398176670074, 0.019363639876246452, -0.30356132984161377, -0.11790354549884796, 0.25896382331848145, -0.6478005051612854, -0.34536489844322205, 0.11335773020982742, 0.26382964849472046, -0.0823247879743576, -0.3183855414390564, 0.29210907220840454, 0.02612697333097458, -0.16887524724006653, 0.22410723567008972, -0.04368901252746582, -0.23402519524097443, -0.06290104985237122, -0.5855554342269897, 0.22845576703548431, -0.09223882108926773, 0.006693348288536072, 0.09596896916627884, -0.18884016573429108, -0.10805439203977585, -0.12422061711549759, 0.37490880489349365, -0.19878987967967987, -0.19638894498348236, -0.04819946736097336, 0.025809573009610176, -0.3901720643043518, -0.18170608580112457, 0.2372320145368576, 0.04537210613489151, -0.09901644289493561, 0.11301838606595993, 0.10699926316738129, -0.06936489790678024, -0.07267148047685623, -0.21991151571273804, -0.2323116809129715, 0.47733214497566223, 0.14203280210494995, -0.21357955038547516, 0.01776234246790409, 0.03554407134652138, 0.1079639121890068, 0.3688441216945648, 0.2160091996192932, -0.09454787522554398, 0.0680035948753357, -0.015836212784051895, -0.14433273673057556, -0.25515061616897583, 0.18845556676387787, 0.061361707746982574, 0.17292548716068268, -0.9424514174461365, 0.08412975817918777, -0.03608265519142151, 0.36574774980545044, 0.007557331118732691, 0.12095174193382263, -0.056865643709897995, -0.1777743250131607, 0.12422268092632294, 0.10688985884189606, 0.021091070026159286, 0.32483744621276855, 0.17118793725967407, -0.17077507078647614, 0.08006203174591064, -0.2571028470993042, 0.41001182794570923, -0.2269962579011917, 0.26539403200149536, -0.4790463149547577, 0.08834885060787201, 0.4015943109989166, 1.1452178955078125, -0.16922983527183533, -0.09101255238056183, 0.4636754095554352, 0.12087703496217728, 0.03556431457400322, -0.014138852246105671, -0.024077681824564934, 0.05643441155552864, -0.11292769014835358, 0.7652256488800049, 0.037176307290792465, -0.06840135902166367, 0.6425544619560242, -0.2522610127925873, 0.06497695297002792, 0.2921518385410309, 0.25628986954689026, 0.48794957995414734, -0.024831388145685196, 0.008243361487984657, -0.23426949977874756, 0.7611701488494873, 0.07374127209186554, 0.1346408873796463, -0.5314002633094788, -0.1509113758802414, -0.037293557077646255, -0.11714444309473038, 0.0021449930500239134, -0.024277936667203903, 0.061224982142448425, 0.33017629384994507, 0.22006244957447052, 0.018972253426909447, 0.0053878054022789, -0.1958986520767212, -0.17425037920475006, 0.21003670990467072, 0.10491510480642319, 0.20810745656490326, -0.02395476959645748, -0.07832662016153336], "sparse_embedding": null}, {"id": "621f2ed5b623ffdbb85483c6bfe4b7d88320bad9abdb32aeffd9910562368257", "content": "With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.", "dataframe": null, "blob": null, "meta": {"type": "arxiv", "url": "http://arxiv.org/abs/2305.14540v1", "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond", "content": "http://arxiv.org/pdf/2305.14540v1", "datetime": "2023-05-23 21:50:06", "query": "LLM benchmarking and evaluation", "linkedin": "\ud83d\ude80 Exciting advancements in the field of Language Models! \ud83e\udd16\ud83d\udcda\n\nDetecting factual inconsistencies in AI-generated content is vital for combatting misinformation and enhancing trust in AI models. Recent research has shown that while some Large Language Models (LLMs) excel in identifying factual inconsistencies in standard benchmarks, they struggle with more complex tasks, showcasing the need for improved evaluation methods.\n\nDiscover more about the cutting-edge research on factual inconsistency detection and the proposed SummEdits benchmark in the full article here: [Check out the research paper!](http://arxiv.org/abs/2305.14540v1)\n\n#AI #NLP #LLMs #Research #FactualInconsistencyDetection #TechInnovation \u2728", "x": "\ud83d\ude80 Just in: New research on detecting factual inconsistencies in LLMs! While some LLMs perform well on existing benchmarks, a new 10-domain benchmark called SummEdits reveals their limitations. Explore the study at: http://arxiv.org/abs/2305.14540v1 #AI #NLP #LLMs #TechResearch \ud83e\udd16\ud83d\udd0d", "source_id": "53a977bbdfddb7702d0c6562509350b5a4823eba122be4968b83c671564d0147", "page_number": 1}, "score": null, "embedding": [-0.07549437880516052, -0.14497895538806915, 0.08184748888015747, -0.10499462485313416, 0.16764788329601288, 0.17457355558872223, -0.08391417562961578, -0.007685722783207893, 0.291056752204895, -0.3347164988517761, 0.1800953447818756, -0.18750189244747162, -0.0806042030453682, 0.34797143936157227, 0.04640822112560272, 0.16051822900772095, 0.08793692290782928, 0.16651548445224762, 0.0423973985016346, -0.012738739140331745, 0.35460081696510315, -0.1587117463350296, 0.06547434628009796, -0.024295682087540627, 0.1643548309803009, 0.035701118409633636, -0.21589332818984985, -0.15653882920742035, -0.32660406827926636, -1.6263233423233032, -0.04449852555990219, -0.43248653411865234, -0.005143270827829838, -0.1380060464143753, 0.0385093167424202, -0.0787850171327591, -0.14818838238716125, 0.05808202177286148, 0.11488956958055496, -0.016733111813664436, 0.017879962921142578, 0.17902113497257233, -0.0007769247749820352, -0.3689058721065521, -0.0037762108258903027, -0.376261442899704, 0.04771485924720764, 0.06517661362886429, -0.611672580242157, -0.10286413878202438, -0.10034677386283875, -0.28583407402038574, 0.2507348954677582, 0.17648057639598846, -0.10425644367933273, 0.4099702835083008, 0.14294618368148804, 0.35756245255470276, 0.3275143504142761, 0.024359967559576035, 0.3308999240398407, 0.5900931358337402, -0.9236866235733032, 0.20685571432113647, 0.4071342647075653, -0.026387471705675125, 0.03977129980921745, -0.3871425688266754, 0.042897600680589676, 0.1224561333656311, 0.052409544587135315, 0.1804046332836151, 0.2967679798603058, 0.29663506150245667, 0.14958107471466064, 0.17139223217964172, 0.1636091023683548, -0.0529225654900074, 0.06372468918561935, 0.08054852485656738, -0.051162779331207275, -0.04130353406071663, -0.19318558275699615, -0.1572325974702835, -0.31370192766189575, -0.09604350477457047, -0.11387073248624802, -0.04983748868107796, 0.30744630098342896, -0.3025570809841156, 0.0892268717288971, 0.15142807364463806, -0.2070167064666748, 0.27222174406051636, -0.033405087888240814, 0.2512814700603485, 0.13203461468219757, 0.06394658237695694, -0.06824349611997604, 0.581790566444397, -0.08881943672895432, 0.1405314803123474, -0.3203984797000885, 0.07663639634847641, 0.25774651765823364, -0.1126202642917633, -0.2687422037124634, -0.34851425886154175, -0.0588708333671093, 0.09321015328168869, 0.11437501013278961, 0.07564792037010193, 0.14097705483436584, -0.2962450385093689, -0.07926556468009949, 0.029492566362023354, 0.5203404426574707, 0.05735231935977936, -0.0658915787935257, 0.11836222559213638, -0.38243404030799866, 0.1297599822282791, 0.08179622143507004, -0.23620568215847015, 0.11532671004533768, -0.060858968645334244, 0.02992893196642399, 0.34248819947242737, 0.12332708388566971, -0.0011508201714605093, 0.03318850323557854, -0.15676714479923248, -0.3587285578250885, -0.09705181419849396, 0.034404207020998, 0.170135959982872, 0.11600334197282791, 0.1590278595685959, 0.30950015783309937, 0.2304047793149948, -0.17269273102283478, -0.08223335444927216, 0.033296920359134674, -0.044558197259902954, -0.38842830061912537, 0.8822315335273743, -0.3436301648616791, 0.18682873249053955, -0.0031739443074911833, -0.12840203940868378, -0.0373479463160038, -0.020850704982876778, -0.3279488682746887, -0.33054104447364807, 0.21649956703186035, -0.00800098292529583, 0.12293384224176407, 0.09054862707853317, -0.3290645480155945, -0.0063132839277386665, 0.05391354858875275, -0.11649159342050552, -0.07637675106525421, 0.7186470627784729, -0.1360897421836853, -0.1644853949546814, -0.13155846297740936, 0.007287371903657913, 0.23915131390094757, -0.20619387924671173, 0.27527064085006714, 0.12911711633205414, -0.0492909736931324, -0.1681687831878662, -0.23173485696315765, -0.1176791787147522, -0.6404844522476196, 0.033222854137420654, -0.0003166943206451833, 0.0023431507870554924, 0.1696711778640747, -0.14808809757232666, -0.35278406739234924, 0.2732013165950775, 0.14865687489509583, -0.26208874583244324, -0.05282701551914215, -0.21303920447826385, 0.3301183879375458, 0.2218407392501831, -0.32012560963630676, 0.03273755684494972, -0.09608110785484314, 0.2789653539657593, -0.05097799748182297, -0.3111830949783325, -0.07399972528219223, 0.007991751655936241, 0.2101510912179947, -0.13494446873664856, -0.11503187566995621, 0.05453638359904289, 0.16714251041412354, -0.050557512789964676, -0.06349360942840576, 0.0807843804359436, -0.15504339337348938, -0.046181995421648026, 0.41208842396736145, 0.3978528678417206, -0.27722400426864624, -0.09908375889062881, 0.016443070024251938, -0.10818953812122345, -0.23533976078033447, -0.005844308529049158, 0.3563545048236847, 0.36708730459213257, -0.25849449634552, 0.26571202278137207, -0.015742843970656395, 0.3079459071159363, -0.11517827212810516, -1.2391443252563477, -0.3588946759700775, 0.3226001560688019, 0.20783129334449768, 0.48965343832969666, -0.3919958174228668, -0.03480003401637077, 0.30769556760787964, 0.1495511531829834, 0.4494363069534302, -0.06255494803190231, 0.17051240801811218, -0.2852509617805481, -0.02078450471162796, -0.06609699875116348, 0.06361918896436691, -0.14302819967269897, 0.1430390179157257, -0.2575016915798187, 0.2804504632949829, -0.3957558870315552, 0.25365954637527466, -0.13517335057258606, -0.6379835605621338, 0.3846382200717926, -0.2489422708749771, 0.6266502737998962, -0.31738072633743286, 0.09017395973205566, -0.1325945258140564, -0.2668111324310303, 0.0810077041387558, -0.12485778331756592, -0.3604658544063568, 0.3536081612110138, 0.27575919032096863, -0.13773217797279358, -0.0207394789904356, -0.10074227303266525, -0.07175610959529877, -0.08477695286273956, 0.022616731002926826, 0.16930079460144043, -0.6513494253158569, -0.4149084985256195, 0.4129531681537628, -0.07336423546075821, -0.05016862228512764, -0.36287611722946167, 0.2521575391292572, 0.05616851896047592, -0.09802751988172531, 0.34331271052360535, 0.07565542310476303, -0.20349374413490295, -0.13621510565280914, -0.4696800410747528, 0.12853838503360748, -0.06053636968135834, 0.010025485418736935, 0.10149121284484863, 0.08967922627925873, -0.09313306212425232, -0.03511529043316841, 0.6554499864578247, -0.3095768690109253, -0.13621538877487183, -0.15751619637012482, 0.35064876079559326, -0.06951110810041428, 0.028348110616207123, 0.6503700613975525, -0.0787382423877716, -0.1638527661561966, 0.22480574250221252, 0.10932260751724243, 0.19671189785003662, -0.42567601799964905, -0.3171578049659729, -0.0630803108215332, 0.48483940958976746, 0.13785117864608765, -0.0033251415006816387, -0.009325472638010979, -0.030229192227125168, 0.004342505242675543, 0.19594170153141022, 0.2160491943359375, 0.1999548226594925, 0.18482324481010437, 0.03526831045746803, 0.2688797116279602, -0.2788613438606262, -0.01470146980136633, -0.0620003268122673, 0.07493922114372253, -1.057155966758728, -0.019635049626231194, -0.2798578441143036, 0.37441638112068176, -0.25906166434288025, -0.08278455585241318, 0.058322228491306305, -0.17547769844532013, 0.04013187065720558, 0.15473052859306335, -0.251706063747406, 0.1350845843553543, 0.2231169193983078, -0.12312638759613037, -0.20019316673278809, 0.1463177502155304, 0.17134149372577667, -0.35275039076805115, 0.12617447972297668, -0.2306506186723709, 0.016121454536914825, 0.20385289192199707, 1.1133391857147217, -0.06113646924495697, -0.12486480921506882, 0.4139779806137085, 0.2430906593799591, 0.08618509024381638, 0.10380065441131592, 0.11454939842224121, -0.25861918926239014, -0.16699601709842682, 0.4975084662437439, 0.19108246266841888, -0.12623552978038788, 0.6105000972747803, -0.28826892375946045, 0.017746571451425552, 0.042206328362226486, -0.013959400355815887, 0.5241668820381165, -0.10516802221536636, 0.08019871264696121, -0.20413756370544434, 0.7518986463546753, -0.08111012727022171, 0.07383929193019867, -0.2740863859653473, 0.00036861858097836375, 0.03458147123456001, -0.15428279340267181, -0.14318226277828217, 0.06081775203347206, -0.0132569195702672, 0.29950079321861267, 0.1953773945569992, 0.03525044023990631, -0.10352346301078796, -0.10590961575508118, -0.19894452393054962, 0.10799416899681091, -0.06989213079214096, 0.12360856682062149, 0.1558253914117813, -0.3110201358795166], "sparse_embedding": null}]}